{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-mcfarland/2022Spring/blob/main/old_main_FinAcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOsri5m-5JAC"
      },
      "source": [
        "#pip install --will exit --must restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUg5jEfEuBVm"
      },
      "outputs": [],
      "source": [
        "#### Since a restart runtime is required (via, exit()... you will need to \"Run After\" on the next code block)\n",
        "import sys\n",
        "#!pip install --pre pycaret\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2gxtT_g4FbD"
      },
      "source": [
        "#pip install --after restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Throw an error to remind me to restart the runtime\n",
        "#MUST RESTART the runtime\n"
      ],
      "metadata": {
        "id": "V2mntSngDADm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3baynY_LBhQ",
        "outputId": "b85e3d23-df10-46a4-ea15-516f656f43a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: markupsafe==2.0.1 in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install pyyaml\n",
        "!pip install markupsafe==2.0.1\n",
        "\n",
        "!pip install graphviz \n",
        "!pip install -q pydot\n",
        "!pip install country_converter \n",
        "\n",
        "!pip install jenkspy\n",
        "!pip install bioinfokit\n",
        "!pip install git+https://github.com/jamesdj/tobit/blob/ba91c92471eb41e63ea27bdb56b99f72fcc5c0cc/tobit.py\n",
        "\n",
        "!pip install Pingouin\n",
        "!pip install Jinja2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJB1i2X-jOJF"
      },
      "outputs": [],
      "source": [
        "#!pip install twine \n",
        "#!pip install git+https://github.com/d-mcfarland/tobit\n",
        "\n",
        "#import tobit *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocpTZxDj5RlZ"
      },
      "source": [
        "#imports and mount gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODBGa5yu_0OP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import jenkspy\n",
        "import pingouin as pg\n",
        "import copy\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import f_oneway\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.stats.api as sms\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from itertools import permutations\n",
        "import itertools \n",
        "import traceback\n",
        "\n",
        "'''#using pycaret in Google Colab requires the following two statements\n",
        "from pycaret.utils import enable_colab\n",
        "enable_colab()\n",
        "from pycaret.classification import *\n",
        "'''\n",
        "# show all rows & columns (wrap, truncate, see all, view)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option(\"expand_frame_repr\", False)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0RwqngW2MOw"
      },
      "source": [
        "#Read dataframes from .csv generated from LoadRawData.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj48gt51DW7Y"
      },
      "outputs": [],
      "source": [
        "# If RAW data changed, run LoadRawData.ipynb to create the following .csv files:\n",
        "#    df_ifdindex.csv --this file has the Financial Development Indexes from the IMF\n",
        "#    df_demograph.csv\n",
        "#    df_incgrp.csv\n",
        "#    df_iset.csv\n",
        "\n",
        "###df_relig = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/data/df_relig.csv')\n",
        "###df_country = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/data/df_country.csv')\n",
        "###df_iset = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/data/df_iset.csv')\n",
        "df_merged = pd.read_excel('/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/data/df_merged.xlsx',index_col='FIPS')\n",
        "###df_all_religions = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/data/df_all_religions.csv')\n",
        "###df_iset_subset = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/data/df_iset_subset.csv')\n",
        "\n",
        "#df_iset['year']=pd.to_numeric(df_iset['year'], errors='coerce')\n",
        "### to end\n",
        "'''\n",
        "df_iset.reset_index(inplace=True)\n",
        "df_iset.drop(columns=['Unnamed: 0','index'],inplace=True)\n",
        "df_country.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "df_relig.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZwAHFuoTyiv"
      },
      "outputs": [],
      "source": [
        "#drop unused columns\n",
        "df_merged = df_merged[df_merged.columns.drop(list(df_merged.filter(regex='(?:_y|_x_x|Unnamed|AMERICA|ASIA|AFRICA|AUSTR|EUROP|COUNTRY)')))]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xAjDeTu8zaI"
      },
      "source": [
        "##not used... Inspect & Summarize data (appending df in a loop) & Debug (extra columns)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tholJ1x48x7Y"
      },
      "outputs": [],
      "source": [
        "# Upadate a dataframe in a loop\n",
        "# create a summary showing the columns in each df\n",
        "#   and showing the unique columns pulled from each df\n",
        "'''\n",
        "#first define the dataframe with column names\n",
        "df_all_cols = ['df_name','col_desc']\n",
        "df_all = pd.DataFrame(columns = df_all_cols)\n",
        "\n",
        "#in loop append rows using \"dictionary\" input \n",
        "\n",
        "for i in df_lst:\n",
        "  currdf = get_df_name(i) #retrieve the name of the current dataframe\n",
        "  print(currdf, i.columns, i.shape,'\\n')\n",
        "  print(len(i['country'].unique()))\n",
        "  #save column names into a new dataframe\n",
        "  try:\n",
        "    my_unique = i['desc'].unique()\n",
        "  except:\n",
        "    my_unique = i.columns\n",
        "  for unq_col in my_unique:\n",
        "    df_all=df_all.append({'df_name':currdf, 'col_desc': unq_col},ignore_index=True)\n",
        "\n",
        "print ('\\n',df_all)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRVyJ-aLtWE-"
      },
      "source": [
        "##NOT USED: Manual Updates based on:\n",
        "###Anomaly analysis (remove outliers)\n",
        "###Review OIC members\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxyNBtv-pqCE"
      },
      "outputs": [],
      "source": [
        "# After a manual review, update the following countries to OIC = 'Muslim'\n",
        "'''mylist=['South Sudan', 'Brunei', 'Egypt', 'French Guinana', 'Gambia', 'Iran', 'Kyrgyzstan', 'Syria', 'Yemen']\n",
        "for my_col in mylist:\n",
        "  df_demograph.loc[df_demograph['country']==my_col,'OIC']=1\n",
        "\n",
        "'''\n",
        "'''# Results of an anomaly analysis, the following countries will be excluded:\n",
        "mylist=['Singapore','Luxembourg','San Marino','Hong Kong SAR, China','Vietnam','United Arab Emirates','Malaysia'] \n",
        "df_demograph=df_demograph.loc[~df_demograph['country'].isin(mylist)]\n",
        "\n",
        "'''\n",
        "# to slice a dataframe based on values in a string\n",
        "'''\n",
        "search_values = ['Iran']\n",
        "try: \n",
        "  df0=df_demograph[df_demograph['country'].str.contains('|'.join(search_values ))]\n",
        "  print(df0.transpose())\n",
        "except:\n",
        "  True\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc2Zen7bZRlo"
      },
      "source": [
        "#DEFINE functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3##Jitter funct to add a small amount of noise"
      ],
      "metadata": {
        "id": "A3CCHNiSJ78U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jitter(a_series, noise_reduction=1000000):\n",
        "    return (np.random.random(len(a_series))*a_series.std()/noise_reduction)-(a_series.std()/(2*noise_reduction))\n"
      ],
      "metadata": {
        "id": "yUztBrdnKAaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## regress_table(df_a, predictors, iv_list): Format Regression Table \n"
      ],
      "metadata": {
        "id": "_JilCTiOtDau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regress_table(df_a, predictors, iv_list):\n",
        "    temp=[]\n",
        "    cnt=0\n",
        "    for curr_fd in predictors:\n",
        "        # define DV\n",
        "        Y = df_a[curr_fd]  \n",
        "        print('\\n', Y.head())\n",
        "        if(len(Y)>0):\n",
        "            for curr_iv in iv_list:\n",
        "                #try:\n",
        "                X = df_a[curr_iv]\n",
        "                n=X['FIPS'].nunique()\n",
        "                print('\\nn=',n,'\\n',X.head(),'\\n',Y.head())\n",
        "\n",
        "                #except:\n",
        "                #    X = []\n",
        "                if(len(X) > 0): \n",
        "                    X = sm.add_constant(X)  \n",
        "                    pglm = pg.linear_regression(X, Y)\n",
        "                    pglm=pglm[['names', 'coef','pval','r2']]\n",
        "                    pglm=pglm.loc[pglm['names']!='Intercept']\n",
        "                    \n",
        "                    #assign pval designator\n",
        "                    select_conditions = [\n",
        "                        (pglm['pval'] <=.01),\n",
        "                        ((pglm['pval'] <=.05)&(pglm['pval'] >.01)),\n",
        "                        ((pglm['pval'] <=.1)&(pglm['pval'] >.05)),\n",
        "                        (pglm['pval'] >.1)\n",
        "                        ]\n",
        "                    select_values = ['a', 'b', 'c','']\n",
        "                    pglm['my_level'] = np.select(select_conditions, select_values)\n",
        "\n",
        "                    pglm.drop(columns=['pval'],inplace=True)\n",
        "                    pglm['run_nbr']=cnt\n",
        "                    pglm['n']=n\n",
        "                    pglm['dv']=curr_fd\n",
        "\n",
        "                    #transpose data for output table \n",
        "                    #### No longer done for the Excel format!!!!\n",
        "                    ### pglm=pglm.transpose()\n",
        "                    temp.append(pglm.round(3))\n",
        "                    cnt+=1\n",
        "\n",
        "    df_out = pd.concat(temp)\n",
        "\n",
        "    df_out['ind']= range(1, len(df_out) + 1)\n",
        "    df_out.set_index('ind')\n",
        "\n",
        "    #print(X.rcorr())\n",
        "\n",
        "    #concat the coef beta-value and sig-level \n",
        "    df_out['outcol'] = [str(x) + ' ' + y for x, y in zip(df_out['coef'], df_out['my_level'])]\n",
        "\n",
        "    piv=df_out.pivot(index=['run_nbr','dv','r2','n'],columns=['names'],values=['outcol'])\n",
        "    return(piv)\n",
        "\n"
      ],
      "metadata": {
        "id": "wMZENO4C-GSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def regress_table(df_a, predictors, iv_list):\n",
        "    temp=[]\n",
        "    cnt=0\n",
        "    for curr_fd in predictors:\n",
        "        if(curr_fd=='Financial Development Index'):\n",
        "            short_dv='FD'\n",
        "        elif(curr_fd=='Financial Institutions Index'):\n",
        "            short_dv='FI'\n",
        "        elif(curr_fd=='Financial Markets Index'):\n",
        "            short_dv='FM'\n",
        "\n",
        "        # Only look at current FD value\n",
        "        df_loop=df_a.loc[df_a['desc']==curr_fd].copy(deep=True)\n",
        "        # define DV\n",
        "        Y = df_loop['value']  \n",
        "        for curr_iv in iv_list:\n",
        "            try:\n",
        "                X = df_loop[curr_iv]\n",
        "                n=df_loop['FIPS'].nunique()\n",
        "            except:\n",
        "                X = []\n",
        "            if(len(X) > 0): \n",
        "                X = sm.add_constant(X)  \n",
        "                pglm = pg.linear_regression(X, Y)\n",
        "                pglm=pglm[['names', 'coef','pval','r2']]\n",
        "                pglm=pglm.loc[pglm['names']!='Intercept']\n",
        "                \n",
        "                #assign pval designator\n",
        "                select_conditions = [\n",
        "                    (pglm['pval'] <=.01),\n",
        "                    ((pglm['pval'] <=.05)&(pglm['pval'] >.01)),\n",
        "                    ((pglm['pval'] <=.1)&(pglm['pval'] >.05)),\n",
        "                    (pglm['pval'] >.1)\n",
        "                    ]\n",
        "                select_values = ['a', 'b', 'c','']\n",
        "                pglm['my_level'] = np.select(select_conditions, select_values)\n",
        "\n",
        "                pglm.drop(columns=['pval'],inplace=True)\n",
        "                pglm['run_nbr']=cnt\n",
        "                pglm['n']=n\n",
        "                pglm['dv']=short_dv\n",
        "\n",
        "                #transpose data for output table \n",
        "                #### No longer done for the Excel format!!!!\n",
        "                ### pglm=pglm.transpose()\n",
        "                temp.append(pglm.round(3))\n",
        "                cnt+=1\n",
        "\n",
        "    df_out = pd.concat(temp)\n",
        "\n",
        "    df_out['ind']= range(1, len(df_out) + 1)\n",
        "    df_out.set_index('ind')\n",
        "\n",
        "    #print(X.rcorr())\n",
        "\n",
        "    #concat the coef beta-value and sig-level \n",
        "    df_out['outcol'] = [str(x) + ' ' + y for x, y in zip(df_out['coef'], df_out['my_level'])]\n",
        "\n",
        "    piv=df_out.pivot(index=['run_nbr','dv','r2','n'],columns=['names'],values=['outcol'])\n",
        "    return(piv)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "NtkbMQjxtJH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sub-total pivot table"
      ],
      "metadata": {
        "id": "WqzJc1lFSqQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pivot_table_w_subtotals(df, values, indices, columns, aggfunc, fill_value):\n",
        "    '''\n",
        "    Adds tabulated subtotals to pandas pivot tables with multiple hierarchical indices.\n",
        "    \n",
        "    Args:\n",
        "    - df - dataframe used in pivot table\n",
        "    - values - values used to aggregrate\n",
        "    - indices - ordered list of indices to aggregrate by\n",
        "    - columns - columns to aggregrate by\n",
        "    - aggfunc - function used to aggregrate (np.max, np.mean, np.sum, etc)\n",
        "    - fill_value - value used to in place of empty cells\n",
        "    \n",
        "    Returns:\n",
        "    -flat table with data aggregrated and tabulated\n",
        "    \n",
        "    '''\n",
        "    listOfTable = []\n",
        "    for indexNumber in range(len(indices)):\n",
        "        n = indexNumber+1\n",
        "        if n == 1:\n",
        "            table = pd.pivot_table(df, values=values, index=indices[:n], columns=columns, aggfunc=aggfunc, fill_value=fill_value, margins=True)\n",
        "        else:\n",
        "            table = pd.pivot_table(df,values=values,index=indices[:n],columns=columns,aggfunc=aggfunc,fill_value=fill_value)\n",
        "        table = table.reset_index()\n",
        "        for column in indices[n:]:\n",
        "            table[column] = ''\n",
        "        listOfTable.append(table)\n",
        "    concatTable = pd.concat(listOfTable).sort_index()\n",
        "    concatTable = concatTable.set_index(keys=indices)\n",
        "    return concatTable.sort_index(axis=0,ascending=True)\n",
        "#pivot_table_w_subtotals(df=df,values='Value',indices=['Store','Department','Type'],columns=[],aggfunc='sum',fill_value='')"
      ],
      "metadata": {
        "id": "yBDT-gbDStkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa-h6YPWxG1j"
      },
      "source": [
        "##T-Test / Comparison of means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQUN-nBtxNJk"
      },
      "outputs": [],
      "source": [
        "#### loops thru row_order\n",
        "### need to modify this to work for:\n",
        "###     ['desc'] = passed_dv (long format)\n",
        "###     OR   [passed_dv] (wide format)\n",
        "'''\n",
        "############   timeframe defines DV for the ttests\n",
        "        #comp_means=comp_means[['country_relig','desc','overall','1980_89','1990_99','2000_09','2010_20']]\n",
        "        time_frame=['overall','1980_89','1990_99','2000_09','2010_20']\n",
        "'''   \n",
        "\n",
        "def compare_means_long(passed_df, passed_dv):\n",
        "    comp_means =passed_df.copy(deep=True)\n",
        "    comp_means.reset_index(inplace=True)\n",
        "    #comp_means=comp_means[['country_relig','desc',passed_dv]]\n",
        "    comp_means=comp_means.dropna()\n",
        "    try:\n",
        "        comp_means.drop(columns='level_0',inplace=True)\n",
        "    except:\n",
        "        pass    \n",
        "\n",
        "    for curr_col in row_order:         \n",
        "        b=comp_means.loc[(['desc']==curr_col)].copy(deep=True)\n",
        "        ### Games Howell\n",
        "        try:\n",
        "            print(\"\\n\\nGames Howell multiple comparison of means for unequal variances and/or unequal sample sizes\\n\")\n",
        "            res=pg.pairwise_gameshowell(dv=passed_dv, between='country_relig', data=b).round(3)\n",
        "            signif=res.loc[res['pval']<.05]\n",
        "            insignif = pd.concat([res,signif]).drop_duplicates(keep=False)\n",
        "            print ('Signif Different Religions (Games Howell):', passed_dv, '/', curr_col,':\\n', signif[['A','B','pval']])\n",
        "            print ('Insignif Religions (Games Howell):', passed_dv, '/', curr_col,':\\n', insignif[['A','B','pval']])\n",
        "            \n",
        "        except Exception:\n",
        "            print('Games Howell failed\\n')\n",
        "            traceback.print_exc()\n",
        "            pass\n",
        "\n",
        "        ### Penguin pairwise ttest\n",
        "        try:\n",
        "            print(\"\\n\\nttest of means (pairwise): adjusts heteroscedasticity \\n\")\n",
        "\n",
        "            # only compare country_relig countries if count > 1\n",
        "            #gbtmp=b.groupby(['country_relig']).filter(lambda x: len(x)>1).reset_index()\n",
        "            res=pg.pairwise_ttests(dv=passed_dv, between='country_relig', data=b)\n",
        "            signif=res.loc[res['p-unc']<.05]\n",
        "            insignif = pd.concat([res,signif]).drop_duplicates(keep=False)\n",
        "            print ('Signif Different Religions (ttest of means --pairwise):', passed_dv,'\\n', signif[['A','B','pval']])\n",
        "            print ('Insignif Religions (ttest of means --pairwise)):', passed_dv,'\\n', insignif[['A','B','pval']])\n",
        "\n",
        "        except Exception:\n",
        "            print('ttest of means (pairwise) failed\\n')\n",
        "            traceback.print_exc()\n",
        "            pass\n",
        "\n",
        "        ### Statsmodel ttest\n",
        "        print(\"\\n\\nttest of means (baseline is CIANoRelAll): \\n\")\n",
        "        base=b.loc[b['country_relig']=='CIANoRelAll'].copy(deep=True)\n",
        "        for curr_relig in relig_list:\n",
        "            if curr_relig != 'CIANoRelAll':\n",
        "                try:\n",
        "                    c=b.loc[b['country_relig']==curr_relig].copy(deep=True)\n",
        "                    t_stat, p_val = stats.ttest_ind(c[curr_dv], base[curr_dv], equal_var=True)\n",
        "                    print('ttest of means (baseline is CIANoRelAll), p_val=',p_val,'\\n')                            \n",
        "                    if p_val < .05:\n",
        "                        print('\\nSignif:', passed_dv, '/', curr_dv, '/', curr_relig, 'CIANoRelAll',p_val)\n",
        "                    else:\n",
        "                        print('\\nNOT Signif:',passed_dv, '/', curr_dv, '/', curr_relig, 'CIANoRelAll',p_val)\n",
        "                except:\n",
        "                    print('ttest of means (baseline is CIANoRelAll) failed')\n",
        "                    traceback.print_exc()\n",
        "                    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_iyH99uI5Qb"
      },
      "source": [
        "##Tobit regression functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLChKmpQIxKE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import warnings\n",
        "from scipy.optimize import minimize\n",
        "import scipy.stats\n",
        "from scipy.special import log_ndtr\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import r2_score, explained_variance_score\n",
        "def split_left_right_censored(x, y, cens):\n",
        "    counts = cens.value_counts()\n",
        "    if -1 not in counts and 1 not in counts:\n",
        "        warnings.warn(\"No censored observations; use regression methods for uncensored data\")\n",
        "    xs = []\n",
        "    ys = []\n",
        "\n",
        "    for value in [-1, 0, 1]:\n",
        "        if value in counts:\n",
        "            split = cens == value\n",
        "            y_split = np.squeeze(y[split].values)\n",
        "            x_split = x[split].values\n",
        "\n",
        "        else:\n",
        "            y_split, x_split = None, None\n",
        "        xs.append(x_split)\n",
        "        ys.append(y_split)\n",
        "    return xs, ys\n",
        "\n",
        "\n",
        "def tobit_neg_log_likelihood(xs, ys, params):\n",
        "    x_left, x_mid, x_right = xs\n",
        "    y_left, y_mid, y_right = ys\n",
        "\n",
        "    b = params[:-1]\n",
        "    # s = math.exp(params[-1])\n",
        "    s = params[-1]\n",
        "\n",
        "    to_cat = []\n",
        "\n",
        "    cens = False\n",
        "    if y_left is not None:\n",
        "        cens = True\n",
        "        left = (y_left - np.dot(x_left, b))\n",
        "        to_cat.append(left)\n",
        "    if y_right is not None:\n",
        "        cens = True\n",
        "        right = (np.dot(x_right, b) - y_right)\n",
        "        to_cat.append(right)\n",
        "    if cens:\n",
        "        concat_stats = np.concatenate(to_cat, axis=0) / s\n",
        "        log_cum_norm = scipy.stats.norm.logcdf(concat_stats)  # log_ndtr(concat_stats)\n",
        "        cens_sum = log_cum_norm.sum()\n",
        "    else:\n",
        "        cens_sum = 0\n",
        "\n",
        "    if y_mid is not None:\n",
        "        mid_stats = (y_mid - np.dot(x_mid, b)) / s\n",
        "        mid = scipy.stats.norm.logpdf(mid_stats) - math.log(max(np.finfo('float').resolution, s))\n",
        "        mid_sum = mid.sum()\n",
        "    else:\n",
        "        mid_sum = 0\n",
        "\n",
        "    loglik = cens_sum + mid_sum\n",
        "\n",
        "    return - loglik\n",
        "\n",
        "\n",
        "def tobit_neg_log_likelihood_der(xs, ys, params):\n",
        "    x_left, x_mid, x_right = xs\n",
        "    y_left, y_mid, y_right = ys\n",
        "\n",
        "    b = params[:-1]\n",
        "    # s = math.exp(params[-1]) # in censReg, not using chain rule as below; they optimize in terms of log(s)\n",
        "    s = params[-1]\n",
        "\n",
        "    beta_jac = np.zeros(len(b))\n",
        "    sigma_jac = 0\n",
        "\n",
        "    if y_left is not None:\n",
        "        left_stats = (y_left - np.dot(x_left, b)) / s\n",
        "        l_pdf = scipy.stats.norm.logpdf(left_stats)\n",
        "        l_cdf = log_ndtr(left_stats)\n",
        "        left_frac = np.exp(l_pdf - l_cdf)\n",
        "        beta_left = np.dot(left_frac, x_left / s)\n",
        "        beta_jac -= beta_left\n",
        "\n",
        "        left_sigma = np.dot(left_frac, left_stats)\n",
        "        sigma_jac -= left_sigma\n",
        "\n",
        "    if y_right is not None:\n",
        "        right_stats = (np.dot(x_right, b) - y_right) / s\n",
        "        r_pdf = scipy.stats.norm.logpdf(right_stats)\n",
        "        r_cdf = log_ndtr(right_stats)\n",
        "        right_frac = np.exp(r_pdf - r_cdf)\n",
        "        beta_right = np.dot(right_frac, x_right / s)\n",
        "        beta_jac += beta_right\n",
        "\n",
        "        right_sigma = np.dot(right_frac, right_stats)\n",
        "        sigma_jac -= right_sigma\n",
        "\n",
        "    if y_mid is not None:\n",
        "        mid_stats = (y_mid - np.dot(x_mid, b)) / s\n",
        "        beta_mid = np.dot(mid_stats, x_mid / s)\n",
        "        beta_jac += beta_mid\n",
        "\n",
        "        mid_sigma = (np.square(mid_stats) - 1).sum()\n",
        "        sigma_jac += mid_sigma\n",
        "\n",
        "    combo_jac = np.append(beta_jac, sigma_jac / s)  # by chain rule, since the expression above is dloglik/dlogsigma\n",
        "\n",
        "    return -combo_jac\n",
        "\n",
        "\n",
        "class TobitModel:\n",
        "    def __init__(self, fit_intercept=True):\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.ols_coef_ = None\n",
        "        self.ols_intercept = None\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.sigma_ = None\n",
        "\n",
        "    def fit(self, x, y, cens, verbose=False):\n",
        "        \"\"\"\n",
        "        Fit a maximum-likelihood Tobit regression\n",
        "        :param x: Pandas DataFrame (n_samples, n_features): Data\n",
        "        :param y: Pandas Series (n_samples,): Target\n",
        "        :param cens: Pandas Series (n_samples,): -1 indicates left-censored samples, 0 for uncensored, 1 for right-censored\n",
        "        :param verbose: boolean, show info from minimization\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        x_copy = x.copy()\n",
        "        if self.fit_intercept:\n",
        "            x_copy.insert(0, 'intercept', 1.0)\n",
        "        else:\n",
        "            x_copy.scale(with_mean=True, with_std=False, copy=False)\n",
        "        init_reg = LinearRegression(fit_intercept=False).fit(x_copy, y)\n",
        "        b0 = init_reg.coef_\n",
        "        y_pred = init_reg.predict(x_copy)\n",
        "        resid = y - y_pred\n",
        "        resid_var = np.var(resid)\n",
        "        s0 = np.sqrt(resid_var)\n",
        "        params0 = np.append(b0, s0)\n",
        "        xs, ys = split_left_right_censored(x_copy, y, cens)\n",
        "\n",
        "        result = minimize(lambda params: tobit_neg_log_likelihood(xs, ys, params), params0, method='BFGS',\n",
        "                          jac=lambda params: tobit_neg_log_likelihood_der(xs, ys, params), options={'disp': verbose})\n",
        "        if verbose:\n",
        "            print(result)\n",
        "        self.ols_coef_ = b0[1:]\n",
        "        self.ols_intercept = b0[0]\n",
        "        if self.fit_intercept:\n",
        "            self.intercept_ = result.x[1]\n",
        "            self.coef_ = result.x[1:-1]\n",
        "        else:\n",
        "            self.coef_ = result.x[:-1]\n",
        "            self.intercept_ = 0\n",
        "        self.sigma_ = result.x[-1]\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.intercept_ + np.dot(x, self.coef_)\n",
        "\n",
        "    def score(self, x, y, scoring_function=mean_absolute_error):\n",
        "        y_pred = np.dot(x, self.coef_)\n",
        "        return scoring_function(y, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X7vcHC_sb_v"
      },
      "source": [
        "##Define function to split records into bins: make_bins()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w3s2Eyjsh0H"
      },
      "outputs": [],
      "source": [
        "def make_bins(df2split, split_into, val_col, col_name, labels):\n",
        "    #debugging, if bin size issue, set with_label = False\n",
        "    with_label = True\n",
        "\n",
        "    split_into=int(split_into)\n",
        "\n",
        "    '''\n",
        "    #binning based on equal distance (essentially, one divides the range by nbr of desired segments)\n",
        "    my_min=float(df2split[val_col].min())\n",
        "    my_max=float(df2split[val_col].max())\n",
        "    equ_dist_bins = np.linspace(my_min,my_max,split_into+1)\n",
        "    if with_label:\n",
        "        df2split['bin_'+col_name+'_dist'] = pd.cut(df2split[col_name], bins= equ_dist_bins, labels=labels, duplicates='drop', include_lowest=True)\n",
        "    else:\n",
        "        df2split['bin_'+col_name+'_dist'] = pd.cut(df2split[col_name], bins= equ_dist_bins, duplicates='drop', include_lowest=True)\n",
        "    #binning based on natural breaks in data (using Jenkspy algorithm/library) \n",
        "    jenkspy_breaks = jenkspy.jenks_breaks(df2split[col_name], nb_class=split_into)\n",
        "    if with_label:\n",
        "        df2split['bin_'+col_name+'_jenkspy'] = pd.cut(df2split[col_name], bins=jenkspy_breaks, labels=labels, duplicates='drop',  include_lowest=True)\n",
        "    else:\n",
        "        df2split['bin_'+col_name+'_jenkspy'] = pd.cut(df2split[col_name], bins=jenkspy_breaks, duplicates='drop',  include_lowest=True)\n",
        "    '''    \n",
        "    #binning based on equal number of rows in each bin (using qcut)\n",
        "    if with_label:\n",
        "        df2split['bin_'+col_name+'_freq'] = pd.qcut(df2split[col_name].rank(method = 'first'), q=split_into, precision=1, duplicates='drop', labels=labels)\n",
        "    else:\n",
        "        df2split['bin_'+col_name+'_freq'] = pd.qcut(df2split[col_name].rank(method = 'first'), q=split_into, precision=1, duplicates='drop')\n",
        "\n",
        "    #print('equ_dist_bins:',equ_dist_bins, 'jenkspy_breaks:',jenkspy_breaks)\n",
        "\n",
        "#pd.qcut(df2split[col_name].rank(method = 'first')\n",
        "#df2split[col_name] + jitter(df2split[col_name]) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9NTIKr5NYYY"
      },
      "source": [
        "##Define function to test Linear Regression Assumptions:\n",
        "###1. Linearity: observed & predicted\n",
        "###2. Normally distributed residuals\n",
        "###3. Multicolinearity of IV\n",
        "###4. Autocorrelation of residuals\n",
        "###5. Heteroscedasticity of residuals\n",
        "\n",
        "\n",
        "Sources: \n",
        "https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/\n",
        "\n",
        "https://people.duke.edu/~rnau/testing.htm#linearity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVrXkqJ8NgjS"
      },
      "outputs": [],
      "source": [
        "def linear_regression_assumptions(features, label, feature_names=None):\n",
        "    \"\"\"\n",
        "    Tests a linear regression on the model to see if assumptions are being met\n",
        "    \"\"\"\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from statsmodels.graphics.gofplots import qqplot\n",
        "    \n",
        "    # Setting feature names to x1, x2, x3, etc. if they are not defined\n",
        "    if feature_names is None:\n",
        "        feature_names = ['X'+str(feature+1) for feature in range(features.shape[1])]\n",
        "    \n",
        "    print('Fitting linear regression')\n",
        "    # Multi-threading if the dataset is a size where doing so is beneficial\n",
        "    if features.shape[0] < 100000:\n",
        "        model = LinearRegression(n_jobs=-1)\n",
        "    else:\n",
        "        model = LinearRegression()\n",
        "        \n",
        "    model.fit(features, label)\n",
        "    \n",
        "    # Returning linear regression R^2 and coefficients before performing diagnostics\n",
        "    r2 = model.score(features, label)\n",
        "    print()\n",
        "    print('R^2:', r2, '\\n')\n",
        "    print('Coefficients')\n",
        "    print('-------------------------------------')\n",
        "    print('Intercept:', model.intercept_)\n",
        "    \n",
        "    for feature in range(len(model.coef_)):\n",
        "        print('{0}: {1}'.format(feature_names[feature], model.coef_[feature]))\n",
        "\n",
        "    print('\\nPerforming linear regression assumption testing')\n",
        "    \n",
        "    # Creating predictions and calculating residuals for assumption tests\n",
        "    predictions = model.predict(features)\n",
        "    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n",
        "    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
        "\n",
        "    ########################################################\n",
        "    #def linear_assumption():\n",
        "    \"\"\"\n",
        " \n",
        "    Linearity: Extremely serious. Assumes there is a linear relationship between the predictors and\n",
        "                the response variable. The points should be symmetrically distributed \n",
        "                around a diagonal line with a roughly constant variance.  \n",
        "                Look carefully for evidence of a \"bowed\" pattern, indicating that the model \n",
        "                makes systematic errors whenever it is making unusually large or small predictions.  \n",
        "                In multiple regression models, nonlinearity or nonadditivity may also be \n",
        "                revealed by systematic patterns in plots of the \n",
        "                residuals versus individual independent variables.\n",
        "\n",
        "    How to fix: \n",
        "            Consider applying a nonlinear transformation to the DV and/or IV \n",
        "            if you can think of a transformation seems appropriate. \n",
        "            (Don’t just make something up!) \n",
        "            \n",
        "            For example, if the data are strictly positive, the log transformation is an option.  \n",
        "            (The logarithm base does not matter--all log functions are same up to linear scaling\n",
        "            --although the NL is usually preferred because small changes in the  \n",
        "            NL are equivalent to percentage changes.\n",
        "            If a log transformation is applied to the DV only, this is equivalent \n",
        "            to assuming that it grows (or decays) exponentially as a function of the IV.  \n",
        "            If a log transformation is applied to both the DV and the IV, this is equivalent \n",
        "            to assuming that the effects of the IV are multiplicative rather than additive in \n",
        "            their original units. This means that, on the margin, a small percentage change in \n",
        "            one of the IV induces a proportional percentage change in the expected value of the DV, \n",
        "            other things being equal. Models of this kind are commonly used in modeling \n",
        "            price-demand relationships. \n",
        "\n",
        "            Another possibility to consider is adding another regressor that is a nonlinear function \n",
        "            of one of the other variables. For example, if you have regressed Y on X, and the graph \n",
        "            of residuals versus predicted values suggests a parabolic curve, then it may make sense to \n",
        "            regress Y on both X and X^2 (i.e., X-squared). The latter transformation is possible even \n",
        "            when X and/or Y have negative values, whereas logging is not.  Higher-order terms of this \n",
        "            kind (cubic, etc.) might also be considered in some cases.  But don’t get carried away!\n",
        "            This sort of \"polynomial curve fitting\" can be a nice way to draw a smooth curve through \n",
        "            a wavy pattern of points (in fact, it is a trend-line option on scatterplots on Excel), \n",
        "            but it is usually a terrible way to extrapolate outside the range of the sample data. \n",
        "\n",
        "            Finally, it may be that you have overlooked some entirely different IV that explains or\n",
        "            corrects for the nonlinear pattern or interactions among variables that you are seeing \n",
        "            in your residual plots. In that case the shape of the pattern, together with economic \n",
        "            or physical reasoning, may suggest some likely suspects.  For example, if the strength \n",
        "            of the linear relationship between Y and X1 depends on the level of some other variable \n",
        "            X2, this could perhaps be addressed by creating a new IV that is the product of X1 and X2.\n",
        "            In the case of time series data, if the trend in Y is believed to have changed at a \n",
        "            particular point in time, then the addition of a piecewise linear trend variable \n",
        "            (one whose string of values looks like 0, 0, …, 0, 1, 2, 3, … ) could be used to fit \n",
        "            the kink in the data.  Such a variable can be considered as the product of a trend \n",
        "            variable and a dummy variable.  Again, though, you need to beware of overfitting the \n",
        "            sample data by throwing in artificially constructed variables that are poorly motivated.  \n",
        "            At the end of the day you need to be able to interpret the model and explain (or sell) \n",
        "            it to others. \n",
        "    \"\"\"\n",
        "    print('\\n=======================================================================================')\n",
        "    print('Assumption 1: Linear Relationship between the Target and the Features')\n",
        "    \n",
        "    print('Checking with a scatter plot of actual vs. predicted. Predictions should follow the diagonal line.')\n",
        "    \n",
        "    # Expected mean of the residual term should be close to zero\n",
        "    print('Expected mean of the residual term should be close to zero:', df_results['Residuals'].mean())\n",
        "\n",
        "    # Plotting the actual vs predicted values\n",
        "    sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, size=7)\n",
        "    # Plotting the diagonal line\n",
        "    line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
        "    plt.plot(line_coords, line_coords,  # X and y points\n",
        "                color='darkorange', linestyle='--')\n",
        "    plt.title('Actual vs. Predicted')\n",
        "    plt.show()\n",
        "    print('If non-linearity is apparent, consider adding a polynomial term')\n",
        "    \n",
        "    sns.lmplot(x='Residuals', y='Predicted', data=df_results, fit_reg=False, size=7)\n",
        "    # Plotting the diagonal line\n",
        "    line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
        "    plt.plot(line_coords, line_coords,  # X and y points\n",
        "                color='darkorange', linestyle='--')\n",
        "    plt.title('Residuals vs. Predicted')\n",
        "    plt.show()\n",
        "    print('If non-linearity is apparent, consider adding a polynomial term')\n",
        "\n",
        "\n",
        "    ########################################################\n",
        "    #def normal_errors_assumption(p_value_thresh=0.05):\n",
        "    p_value_thresh=0.05\n",
        "    \"\"\"\n",
        "    Normality: Assumes that the error terms are normally distributed. If they are not,\n",
        "    nonlinear transformations of variables may solve this.\n",
        "            \n",
        "    This assumption being violated primarily causes issues with the confidence intervals\n",
        "\n",
        "    Violations of normality create problems for determining whether model coefficients are significantly\n",
        "    different from zero and for calculating confidence intervals for forecasts. Sometimes the error \n",
        "    distribution is \"skewed\" by the presence of a few large outliers. Since parameter estimation is \n",
        "    based on the minimization of squared error, a few extreme observations can exert a disproportionate \n",
        "    influence on parameter estimates. Calculation of confidence intervals and various significance \n",
        "    tests for coefficients are all based on the assumptions of normally distributed errors. If the error \n",
        "    distribution is significantly non-normal, confidence intervals may be too wide or too narrow.\n",
        "\n",
        "    Technically, the normal distribution assumption is not necessary if you are willing to assume \n",
        "    the model equation is correct and your only goal is to estimate its coefficients and generate \n",
        "    predictions in such a way as to minimize mean squared error.  The formulas for estimating \n",
        "    coefficients require no more than that, and some references on regression analysis do not list \n",
        "    normally distributed errors among the key assumptions.  But generally we are interested in making \n",
        "    inferences about the model and/or estimating the probability that a given forecast error will \n",
        "    exceed some threshold in a particular direction, in which case distributional assumptions are \n",
        "    important.  Also, a significant violation of the normal distribution assumption is often a \n",
        "    \"red flag\" indicating that there is some other problem with the model assumptions and/or that \n",
        "    there are a few unusual data points that should be studied closely and/or that a better model \n",
        "    is still waiting out there somewhere.\n",
        "\n",
        "    How to diagnose: \n",
        "        The best test for normally distributed errors is a normal probability plot \n",
        "        or normal quantile plot of the residuals. These are plots of the fractiles of error distribution \n",
        "        versus the fractiles of a normal distribution having the same mean and variance. If the distribution \n",
        "        is normal, the points on such a plot should fall close to the diagonal reference line. A \n",
        "        bow-shaped pattern of deviations from the diagonal indicates that the residuals have excessive \n",
        "        skewness (i.e., they are not symmetrically distributed, with too many large errors in one \n",
        "        direction). An S-shaped pattern of deviations indicates that the residuals have excessive \n",
        "        kurtosis--i.e., there are either too many or two few large errors in both directions. \n",
        "        Sometimes the problem is revealed to be that there are a few data points on one or both ends \n",
        "        that deviate significantly from the reference line (\"outliers\"), in which case they should get \n",
        "        close attention. \n",
        "\n",
        "        There are also a variety of statistical tests for normality, including the Kolmogorov-Smirnov \n",
        "        test, the Shapiro-Wilk test, the Jarque-Bera test, and the Anderson-Darling test.  The \n",
        "        Anderson-Darling test is generally considered to be the best, because it is specific to the \n",
        "        normal distribution (unlike the K-S test) and it looks at the whole distribution rather than \n",
        "        just the skewness and kurtosis (like the J-B test).  But all of these tests are excessively \n",
        "        \"picky\" in this author’s opinion.  Real data rarely has errors that are perfectly normally \n",
        "        distributed, and it may not be possible to fit your data with a model whose errors do not \n",
        "        violate the normality assumption at the 0.05 level of significance.  It is usually better \n",
        "        to focus more on violations of the other assumptions and/or the influence of a few outliers \n",
        "        (which may be mainly responsible for violations of normality anyway) and to look at a normal \n",
        "        probability plot or normal quantile plot and draw your own conclusions about whether the \n",
        "        problem is serious and whether it is systematic.\n",
        "\n",
        "    How to fix: \n",
        "        Violations of normality often arise either because (a) the distributions of the dependent\n",
        "         and/or independent variables are themselves significantly non-normal, and/or \n",
        "         (b) the linearity assumption is violated. In such cases, a nonlinear transformation of \n",
        "         variables might cure both problems. In the case of the two normal quantile plots above, \n",
        "         the second model was obtained applying a natural log transformation to the variables in \n",
        "         the first one. \n",
        "\n",
        "        The dependent and independent variables in a regression model do not need to be normally \n",
        "        distributed by themselves--only the prediction errors need to be normally distributed.  \n",
        "        (In fact, independent variables do not even need to be random, as in the case of trend \n",
        "        or dummy or treatment or pricing variables.)  But if the distributions of some of the \n",
        "        variables that are random are extremely asymmetric or long-tailed, it may be hard to fit \n",
        "        them into a linear model whose errors will be normally distributed, and explaining the \n",
        "        shape of their distributions may be an interesting topic all by itself.  Keep in mind \n",
        "        that the normal error assumption is usually justified by appeal to the central limit \n",
        "        theorem, which holds in the case where many random variations are added together.  If \n",
        "        the underlying sources of randomness are not interacting additively, this argument fails \n",
        "        to hold.\n",
        "\n",
        "        Another possibility is that there are two or more subsets of the data having different \n",
        "        statistical properties, in which case separate models should be built, or else some data \n",
        "        should merely be excluded, provided that there is some a priori criterion that can be \n",
        "        applied to make this determination.\n",
        "\n",
        "        In some cases, the problem with the error distribution is mainly due to one or two very \n",
        "        large errors. Such values should be scrutinized closely: are they genuine (i.e., not the \n",
        "        result of data entry errors), are they explainable, are similar events likely to occur \n",
        "        again in the future, and how influential are they in your model-fitting results? If they \n",
        "        are merely errors or if they can be explained as unique events not likely to be repeated, \n",
        "        then you may have cause to remove them. In some cases, however, it may be that the extreme \n",
        "        values in the data provide the most useful information about values of some of the \n",
        "        coefficients and/or provide the most realistic guide to the magnitudes of forecast errors.  \n",
        "\n",
        "    \"\"\"\n",
        "    from statsmodels.stats.diagnostic import normal_ad\n",
        "    print('\\n=======================================================================================')\n",
        "    print('Assumption 2: The error terms are normally distributed')\n",
        "    print()\n",
        "\n",
        "    print('Using the Anderson-Darling test for normal distribution')\n",
        "\n",
        "    # Performing the test on the residuals\n",
        "    p_value = normal_ad(df_results['Residuals'])[1]\n",
        "    print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n",
        "\n",
        "    # Reporting the normality of the residuals\n",
        "    if p_value < p_value_thresh:\n",
        "        print('Residuals are not normally distributed')\n",
        "    else:\n",
        "        print('Residuals are normally distributed')\n",
        "\n",
        "    # Plotting the residuals distribution\n",
        "    plt.subplots(figsize=(12, 6))\n",
        "    plt.title('Distribution of Residuals')\n",
        "    sns.distplot(df_results['Residuals'])\n",
        "    plt.show()\n",
        "\n",
        "    print()\n",
        "    if p_value > p_value_thresh:\n",
        "        print('Assumption satisfied')\n",
        "    else:\n",
        "        print('Assumption not satisfied')\n",
        "        print()\n",
        "        print('Confidence intervals will likely be affected')\n",
        "        print('Try performing nonlinear transformations on variables')\n",
        "\n",
        "    # qqplot\n",
        "    print('\\nQQ Plot... normally distrib data will appears as a straight line along the diagonal.')\n",
        "    qqplot(label, line='s')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "    \n",
        "    ########################################\n",
        "    #def multicollinearity_assumption():\n",
        "    \"\"\"\n",
        "    Multicollinearity: Assumes that predictors are not correlated with each other. If there is\n",
        "                        correlation among the predictors, then either remove prepdictors with high\n",
        "                        Variance Inflation Factor (VIF) values or perform dimensionality reduction\n",
        "                        \n",
        "                        This assumption being violated causes issues with interpretability of the \n",
        "                        coefficients and the standard errors of the coefficients.\n",
        "    \"\"\"\n",
        "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "    print('\\n=======================================================================================')\n",
        "    print('Assumption 3: Little to no multicollinearity among predictors')\n",
        "    \n",
        "    # Plotting the heatmap\n",
        "    plt.figure(figsize = (10,8))\n",
        "    sns.heatmap(pd.DataFrame(features, columns=feature_names).corr(), annot=True)\n",
        "    plt.title('Correlation of Variables')\n",
        "    plt.show()\n",
        "    \n",
        "    print('Variance Inflation Factors (VIF)')\n",
        "    print('> 10: An indication that multicollinearity may be present')\n",
        "    print('> 100: Certain multicollinearity among the variables')\n",
        "    print('-------------------------------------')\n",
        "    \n",
        "    # Gathering the VIF for each variable\n",
        "    VIF = [variance_inflation_factor(features, i) for i in range(features.shape[1])]\n",
        "    for idx, vif in enumerate(VIF):\n",
        "        print('{0}: {1}'.format(feature_names[idx], vif))\n",
        "    \n",
        "    # Gathering and printing total cases of possible or definite multicollinearity\n",
        "    possible_multicollinearity = sum([1 for vif in VIF if vif > 10])\n",
        "    definite_multicollinearity = sum([1 for vif in VIF if vif > 100])\n",
        "    print()\n",
        "    print('{0} cases of possible multicollinearity'.format(possible_multicollinearity))\n",
        "    print('{0} cases of definite multicollinearity'.format(definite_multicollinearity))\n",
        "    print()\n",
        "\n",
        "    if definite_multicollinearity == 0:\n",
        "        if possible_multicollinearity == 0:\n",
        "            print('Assumption satisfied')\n",
        "        else:\n",
        "            print('Assumption possibly satisfied')\n",
        "            print()\n",
        "            print('Coefficient interpretability may be problematic')\n",
        "            print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
        "    else:\n",
        "        print('Assumption not satisfied')\n",
        "        print()\n",
        "        print('Coefficient interpretability will be problematic')\n",
        "        print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
        "    \n",
        "    \n",
        "    ###########################################\n",
        "    #def autocorrelation_assumption():\n",
        "    \"\"\"\n",
        "    ## autocorrelation of residuals indicates a voilation of independence\n",
        "\n",
        "    Autocorrelation: \n",
        "            Assumes that there is no autocorrelation in the residuals. If there is\n",
        "            autocorrelation, then there is a pattern that is not explained due to\n",
        "            the current value being dependent on the previous value.\n",
        "            This may be resolved by adding a lag variable of either the dependent\n",
        "            variable or some of the predictors.\n",
        "\n",
        "            To test for non-time-series violations of independence, you can look at plots of the \n",
        "            residuals versus independent variables or plots of residuals versus row number in \n",
        "            situations where the rows have been sorted or grouped in some way that depends (only) \n",
        "            on the values of the independent variables.  The residuals should be randomly and \n",
        "            symmetrically distributed around zero under all conditions, and in particular there \n",
        "            should be no correlation between consecutive errors no matter how the rows are sorted, \n",
        "            as long as it is on some criterion that does not involve the dependent variable.  If \n",
        "            this is not true, it could be due to a violation of the linearity assumption or due to \n",
        "            bias that is explainable by omitted variables (say, interaction terms or dummies for \n",
        "            identifiable conditions).\n",
        "\n",
        "        Interpret: \n",
        "            Ideally, most of the residual autocorrelations should fall within the 95% \n",
        "            confidence bands around zero, which are located at roughly plus-or-minus \n",
        "            2-over-the-square-root-of-n, where n is the sample size. Thus, if the sample size is \n",
        "            50, the autocorrelations should be between +/- 0.3. If the sample size is 100, they \n",
        "            should be between +/- 0.2. Pay especially close attention to significant correlations \n",
        "            at the first couple of lags and in the vicinity of the seasonal period, because these \n",
        "            are probably not due to mere chance and are also fixable. The Durbin-Watson statistic \n",
        "            provides a test for significant residual autocorrelation at lag 1: the DW stat is \n",
        "            approximately equal to 2(1-a) where a is the lag-1 residual autocorrelation, so ideally \n",
        "            it should be close to 2.0--say, between 1.4 and 2.6 for a sample size of 50.\n",
        "\n",
        "    How to fix: Minor cases of positive serial correlation (say, lag-1 residual autocorrelation \n",
        "            in the range 0.2 to 0.4, or a Durbin-Watson statistic between 1.2 and 1.6) indicate \n",
        "            that there is some room for fine-tuning in the model. Consider adding lags of the \n",
        "            DV and/or lags of some of the IVs. Or, find an ARIMA+regressor procedure to add an\n",
        "            AR(1) or MA(1) term to the regression model.  An AR(1) term adds a lag of the DV to\n",
        "            the forecasting equation, whereas an MA(1) term adds a lag of the forecast error. \n",
        "            IF there is significant correlation at lag 2, then a 2nd-order lag may be appropriate.\n",
        "\n",
        "            If there is significant negative correlation in the residuals (lag-1 autocorrelation\n",
        "            more negative than -0.3 or DW stat greater than 2.6), watch out for the possibility \n",
        "            that you may have overdifferenced some of your variables. Differencing tends to \n",
        "            drive autocorrelations in the negative direction, and too much differencing may lead\n",
        "            to artificial patterns of negative correlation that lagged variables cannot correct \n",
        "            for.\n",
        "\n",
        "            If there is significant correlation at the seasonal period (e.g. at lag 4 for quarterly \n",
        "            data or lag 12 for monthly data), this indicates that seasonality has not been properly \n",
        "            accounted for in the model. Seasonality can be handled in a regression model in one of \n",
        "            the following ways: (i) seasonally adjust the variables (if they are not already \n",
        "            seasonally adjusted), or (ii) use seasonal lags and/or seasonally differenced variables \n",
        "            (caution: be careful not to overdifference!), or (iii) add seasonal dummy variables to \n",
        "            the model (i.e., indicator variables for different seasons of the year, such as MONTH=1 \n",
        "            or QUARTER=2, etc.) The dummy-variable approach enables additive seasonal adjustment to \n",
        "            be performed as part of the regression model: a different additive constant can be \n",
        "            estimated for each season of the year. If the dependent variable has been logged, the \n",
        "            seasonal adjustment is multiplicative. (Something else to watch out for: it is possible \n",
        "            that although your dependent variable is already seasonally adjusted, some of your IVs \n",
        "            may not be, causing their seasonal patterns to leak into the forecasts.)\n",
        "\n",
        "            Major cases of serial correlation (a Durbin-Watson statistic well below 1.0, \n",
        "            autocorrelations well above 0.5) usually indicate a fundamental structural problem in \n",
        "            the model. You may wish to reconsider the transformations (if any) that have been \n",
        "            applied to the dependent and independent variables. It may help to stationarize all \n",
        "            variables through appropriate combinations of differencing, logging, and/or deflating.\n",
        "\n",
        "    \"\"\"\n",
        "    from statsmodels.stats.stattools import durbin_watson\n",
        "    print('\\n=======================================================================================')\n",
        "    print('Assumption 4: No Autocorrelation')\n",
        "    print('\\nPerforming Durbin-Watson Test')\n",
        "    print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n",
        "    print('0 to 2< is positive autocorrelation')\n",
        "    print('>2 to 4 is negative autocorrelation')\n",
        "    print('-------------------------------------')\n",
        "    durbinWatson = durbin_watson(df_results['Residuals'])\n",
        "    print('Durbin-Watson:', durbinWatson)\n",
        "    if durbinWatson < 1.5:\n",
        "        print('Signs of positive autocorrelation', '\\n')\n",
        "        print('Assumption not satisfied', '\\n')\n",
        "        print('Consider adding lag variables')\n",
        "    elif durbinWatson > 2.5:\n",
        "        print('Signs of negative autocorrelation', '\\n')\n",
        "        print('Assumption not satisfied', '\\n')\n",
        "        print('Consider adding lag variables')\n",
        "    else:\n",
        "        print('Little to no autocorrelation', '\\n')\n",
        "        print('Assumption satisfied')\n",
        "\n",
        "        \n",
        "    ######################################\n",
        "    #def homoscedasticity_assumption():\n",
        "    \"\"\"\n",
        "    Homoscedasticity: Assumes that the errors exhibit constant variance\n",
        "\n",
        "    How to diagnose: (same plots as linearity)\n",
        "        Look at a plot of residuals versus predicted values and, in the case of time \n",
        "        series data, a plot of residuals versus time.  Be alert for evidence of residuals that grow \n",
        "        larger either as a function of time or as a function of the predicted value. To be really thorough, \n",
        "        you should also generate plots of residuals versus independent variables to look for consistency \n",
        "        there as well.  Because of imprecision in the coefficient estimates, the errors may tend to be \n",
        "        slightly larger for forecasts associated with predictions or values of independent variables that \n",
        "        are extreme in both directions, although the effect should not be too dramatic.  What you hope not \n",
        "        to see are errors that systematically get larger in one direction by a significant amount.\n",
        "\n",
        "    How to fix:  \n",
        "        1. Transform the dependent variable. One way to fix heteroscedasticity is to transform the\n",
        "        dependent variable in some way. One common transformation is to simply take the log of the \n",
        "        dependent variable.\n",
        "\n",
        "        2. Redefine the dependent variable. Another way to fix heteroscedasticity is to redefine the \n",
        "        dependent variable. One common way to do so is to use a rate for the dependent variable, \n",
        "        rather than the raw value.\n",
        "\n",
        "        3. Use weighted regression. Another way to fix heteroscedasticity is to use weighted regression.\n",
        "        This type of regression assigns a weight to each data point based on the variance of its \n",
        "        fitted value. When the proper weights are used, this can eliminate the problem of \n",
        "        heteroscedasticity.\n",
        "\n",
        "        If the dependent variable is strictly positive and if the residual-versus-predicted plot\n",
        "        shows that the size of the errors is proportional to the size of the predictions (i.e., if the \n",
        "        errors seem consistent in percentage rather than absolute terms), a log transformation applied to \n",
        "        the dependent variable may be appropriate.  In time series models, heteroscedasticity often arises \n",
        "        due to the effects of inflation and/or real compound growth. Some combination of logging and/or \n",
        "        deflating will often stabilize the variance in this case. Stock market data may show periods of \n",
        "        increased or decreased volatility over time. This is normal and is often modeled with so-called \n",
        "        ARCH (auto-regressive conditional heteroscedasticity) models in which the error variance is fitted \n",
        "        by an autoregressive model. Such models are beyond the scope of this discussion, but a simple fix \n",
        "        would be to work with shorter intervals of data in which volatility is more nearly constant. \n",
        "        Heteroscedasticity can also be a byproduct of a significant violation of the linearity and/or \n",
        "        independence assumptions, in which case it may also be fixed as a byproduct of fixing those problem. \n",
        "\n",
        "        Seasonal patterns in the data are a common source of heteroscedasticity in the errors:  \n",
        "        unexplained variations in the dependent variable throughout the course of a season may be \n",
        "        consistent in percentage rather than absolute terms, in which case larger errors will be made \n",
        "        in seasons where activity is greater, which will show up as a seasonal pattern of changing variance \n",
        "        on the residual-vs-time plot.  A log transformation is often used to address this problem.  For \n",
        "        example, if the seasonal pattern is being modeled through the use of dummy variables for months \n",
        "        or quarters of the year, a log transformation applied to the dependent variable will convert the \n",
        "        coefficients of the dummy variables to multiplicative adjustment factors rather than additive \n",
        "        adjustment factors, and the errors in predicting the logged variable will be (roughly) \n",
        "        interpretable as percentage errors in predicting the original variable.  Seasonal adjustment of \n",
        "        all the data prior to fitting the regression model might be another option. \n",
        "\n",
        "        If a log transformation has already been applied to a variable, then (as noted above) additive \n",
        "        rather than multiplicative seasonal adjustment should be used, if it is an option that your \n",
        "        software offers.  Additive seasonal adjustment is similar in principle to including dummy \n",
        "        variables for seasons of the year.  Whether-or-not you should perform the adjustment outside the \n",
        "        model rather than with dummies depends on whether you want to be able to study the seasonally \n",
        "        adjusted data all by itself and on whether there are unadjusted seasonal patterns in some of the\n",
        "        independent variables.  (The dummy-variable approach would address the latter problem.)  \n",
        "\n",
        "    \"\"\"\n",
        "    print('\\n=======================================================================================')\n",
        "    print('Assumption 5: Homoscedasticity of Error Terms')\n",
        "    print('Residuals should have relative constant variance')\n",
        "    \n",
        "    # Plotting the residuals\n",
        "    plt.subplots(figsize=(12, 6))\n",
        "    ax = plt.subplot(111)  # To remove spines\n",
        "    plt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\n",
        "    plt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\n",
        "    ax.spines['right'].set_visible(False)  # Removing the right spine\n",
        "    ax.spines['top'].set_visible(False)  # Removing the top spine\n",
        "    plt.title('Residuals')\n",
        "    plt.show() \n",
        "    print('If heteroscedasticity is apparent, confidence intervals and predictions will be affected')\n",
        "    \n",
        "    \n",
        "    bp_test = pd.DataFrame(sms.het_breuschpagan(df_results.Residuals, X), \n",
        "                           columns=['value'],\n",
        "                           index=['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value'])\n",
        "\n",
        "    gq_test = pd.DataFrame(sms.het_goldfeldquandt(df_results.Residuals, X)[:-1],\n",
        "                           columns=['value'],\n",
        "                           index=['F statistic', 'p-value'])\n",
        "\n",
        "    print('\\n Breusch-Pagan test ----')\n",
        "    print(bp_test)\n",
        "    print('\\n Goldfeld-Quandt test ----')\n",
        "    print(gq_test)\n",
        "\n",
        "    ###############################\n",
        "    # check the absolute value of the standardized residuals for influencial outliers\n",
        "    ###############################\n",
        "\n",
        "    '''ATTEMPT #1:\n",
        "    #create instance of influence\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    #obtain standardized residuals\n",
        "    standardized_residuals = influence.resid_studentized_internal\n",
        "\n",
        "    #display standardized residuals\n",
        "\n",
        "    for x in standardized_residuals:\n",
        "        if x >= 3:\n",
        "            print (x)\n",
        "\n",
        "    ATTEMPT #2:\n",
        "\n",
        "    N = len(X)\n",
        "    p = len(X.columns) + 1  # plus one because LinearRegression adds an intercept term\n",
        "\n",
        "    X_with_intercept = np.empty(shape=(N, p), dtype=np.float)\n",
        "    X_with_intercept[:, 0] = 1\n",
        "    X_with_intercept[:, 1:p] = X.values\n",
        "\n",
        "    beta_hat = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y.values\n",
        "    print(beta_hat)\n",
        "\n",
        "    y_hat = model.predict(X)\n",
        "    residuals = y.values - y_hat\n",
        "    residual_sum_of_squares = residuals.T @ residuals\n",
        "    sigma_squared_hat = residual_sum_of_squares[0, 0] / (N - p)\n",
        "    var_beta_hat = np.linalg.inv(X_with_intercept.T @ X_with_intercept) * sigma_squared_hat\n",
        "    for p_ in range(p):\n",
        "        standard_error = var_beta_hat[p_, p_] ** 0.5\n",
        "        print(f\"SE(beta_hat[{p_}]): {standard_error}\")\n",
        "    '''\n",
        "    print('Attempt to show standard errors of predictors')\n",
        "    N = len(X)\n",
        "    p = len(X.columns) + 1  # plus one because LinearRegression adds an intercept term\n",
        "\n",
        "    X_with_intercept = np.empty(shape=(N, p), dtype=np.float)\n",
        "    X_with_intercept[:, 0] = 1\n",
        "    X_with_intercept[:, 1:p] = X.values\n",
        "\n",
        "    ols = sm.OLS(Y.values, X_with_intercept)\n",
        "    ols_result = ols.fit()\n",
        "    print(ols_result.summary())\n",
        "\n",
        "'''     \n",
        "linear_assumption()\n",
        "normal_errors_assumption()\n",
        "multicollinearity_assumption()\n",
        "autocorrelation_assumption()\n",
        "homoscedasticity_assumption()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Univariate descriptive statistics -->univariate_metrics(df)"
      ],
      "metadata": {
        "id": "pS9OEuVmoZEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def univariate_metrics (df):\n",
        "    #create blank lists for univariate statistics\n",
        "    my_col_dtype = []\n",
        "    my_col_num_distinct = []\n",
        "    my_col_perc_missing = []\n",
        "    my_col_cnt_null = []\n",
        "    my_col_min = []\n",
        "    my_col_max = []\n",
        "    my_col_range = []\n",
        "    my_col_mean = []\n",
        "    my_col_std = []\n",
        "    my_col_median = []\n",
        "    my_col_Q25 = []\n",
        "    my_col_Q75 = []\n",
        "    my_col_CV = []\n",
        "    my_col_skew = []\n",
        "    my_col_kurtosis = []\n",
        "\n",
        "    my_col_mode = []\n",
        "    my_col_unique_vals = []\n",
        "\n",
        "    df_loop=[]\n",
        "    for mycol in df.columns:  #add values to the newly created lists (use a dict to save col_name for each val)\n",
        "    \n",
        "        df_loop.append({'Col_Name' : mycol, 'desc':'DType', 'value':df.dtypes[mycol]})\n",
        "        df_loop.append({'Col_Name' : mycol, 'desc':'NumUnique', 'value':df[mycol].nunique()})\n",
        "        df_loop.append({'Col_Name' : mycol, 'desc':'PercNull', 'value':df[mycol].isnull().mean()})\n",
        "        df_loop.append({'Col_Name' : mycol, 'desc':'CntNull', 'value':df[mycol].isnull().sum()})\n",
        "        \n",
        "        #if df[mycol].dtype.kind in 'iuf': # these metrics return a single value per feature\n",
        "        if is_numeric_dtype(df[mycol]): # these metrics return a single value per feature\n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Min', 'value':df[mycol].min()})\n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Max', 'value':df[mycol].max()})\n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Range', 'value': df[mycol].max() - df[mycol].min()})\n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Mean', 'value': df[mycol].mean()})\n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Std', 'value': df[mycol].std()})\n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Median', 'value': df[mycol].median()})\n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Q25', 'value': df[mycol].quantile(q=.25)})\n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Q75', 'value': df[mycol].quantile(q=.75)}) \n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'CoefVariance', 'value': df[mycol].std() / df[mycol].mean()})\n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Skew', 'value': df[mycol].skew()})            \n",
        "            df_loop.append({'Col_Name' : mycol, 'desc':'Kurtosis', 'value': df[mycol].kurtosis()})           \n",
        "\n",
        "    df1 = pd.DataFrame.from_dict(df_loop)\n",
        "    return df1\n"
      ],
      "metadata": {
        "id": "DZGKqcVWodEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTravTc0pUK0"
      },
      "source": [
        "#Print available variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkH8AVKz84Fu"
      },
      "outputs": [],
      "source": [
        "print('df_merged:\\n',df_merged.columns.to_list(),'\\nhead():\\n\\n',df_merged.head(100),'\\n\\ndesc values:\\n',df_merged['desc'].unique())\n",
        "#print('df_iset_subset:\\n',df_iset_subset.columns.to_list(),df_iset_subset.head(200))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JUtJwQZkqVW"
      },
      "outputs": [],
      "source": [
        "print(df_merged.info(verbose=True,show_counts=True),'\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lls8VS1xaoRN"
      },
      "source": [
        "#ANOVA: MUST Update to pingouin & to test assumptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVkqorpkaqeR"
      },
      "outputs": [],
      "source": [
        "#dataframe \"a\" is the merged df of df_merged  \n",
        "\n",
        "\n",
        "#Unconstrained oneway, uncontrained ANOVA\n",
        "fvalue, pvalue = stats.f_oneway(df_merged['CIABudAll'], df_merged['CIABudHindAll'], df_merged['CIACathAll'], \n",
        "                                df_merged['CIAChrAll'],df_merged['CIAHindAll'], df_merged['CIAJewAll'], \n",
        "                                df_merged['CIAMusAll'], df_merged['CIANoRelAll'], df_merged['CIAOrthAll'],\n",
        "                                df_merged['CIAOthNoJew'], df_merged['CIAOthWithJew'], df_merged['CIAProtAll'], df_merged['CIABelieverAll'] )\n",
        "print('statsmodels unconstrained ANOVA: H0: no differences when comparing religions\\n',fvalue, pvalue)\n",
        "\n",
        "alpha=.05\n",
        "if (pvalue < alpha):\n",
        "    print('Reject H0, religions have different values')\n",
        "\n",
        "# get ANOVA table as R like output\n",
        "\n",
        "b=df_merged.loc[(df_merged['desc']=='Financial Development Index') & df_merged['overall']!= np.nan]\n",
        "b.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Ordinary Least Squares (OLS) model\n",
        "model=ols('value ~ country_relig', data=b).fit() #added 'overall' column\n",
        "#model = ols('value ~ C(treatments)', data=df_melt).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print('\\nstatsmodels ANOVA: H0=equal means\\n',anova_table)\n",
        "\n",
        "\n",
        "# ANOVA table using bioinfokit v1.0.3 or later (it uses wrapper script for anova_lm)\n",
        "from bioinfokit.analys import stat\n",
        "res = stat()\n",
        "res.anova_stat(df=b, res_var='value', anova_model='value ~ country_relig')\n",
        "print('\\nbioinfokit ANOVA: H0=equal means\\n',res.anova_summary)\n",
        "\n",
        "# note: if the data is balanced (equal sample size for each group), Type 1, 2, and 3 sums of squares\n",
        "# (typ parameter) will produce similar results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSx1FRK6l89Y"
      },
      "source": [
        "#Print TABLES"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rename columns/rows for sorting purposes"
      ],
      "metadata": {
        "id": "xwAzI7_gl1-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df_merged.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "df_merged=df_merged.reset_index(drop=False)\n",
        "try:\n",
        "    df_merged.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "fd_list=pd.Categorical(['Financial Development Index', \n",
        "         'Financial Institutions Index', 'Financial Institutions Depth Index',\n",
        "         'Financial Institutions Access Index', 'Financial Institutions Efficiency Index',\n",
        "         'Financial Markets Index','Financial Markets Depth Index',\n",
        "         'Financial Markets Access Index','Financial Markets Efficiency Index'], ordered=True)\n",
        "\n",
        "fd_list=['FD','FI','FID','FIA','FM','FMD','FMA']\n",
        "\n",
        "df_merged['country_relig'].replace(\n",
        "    {'CIAProtAll':'aCIAProtAll', \n",
        "    'CIACathAll':'bCIACathAll', \n",
        "    'CIAOrthAll':'cCIAOrthAll', \n",
        "    'CIAChrAll':'dCIAChrAll',\n",
        "    'CIAMusAll':'eCIAMusAll', \n",
        "    'CIABudAll':'fCIABudAll', \n",
        "    'CIAHindAll':'gCIAHindAll', \n",
        "    'CIABudHindAll':'hCIABudHindAll',\n",
        "    'CIAJewAll':'iCIAJewAll',\n",
        "    'CIAOthNoJew':'jCIAOthNoJew', \n",
        "    'CIAOthWithJew':'kCIAOthWithJew',\n",
        "    'CIABelieverAll':'lCIABelieverAll',\n",
        "    'CIANoRelAll':'mCIANoRelAll'},inplace=True)\n",
        "\n",
        "df_merged.rename(columns=\n",
        "    {'CIAProtAll':'aCIAProtAll', \n",
        "    'CIACathAll':'bCIACathAll', \n",
        "    'CIAOrthAll':'cCIAOrthAll', \n",
        "    'CIAChrAll':'dCIAChrAll',\n",
        "    'CIAMusAll':'eCIAMusAll', \n",
        "    'CIABudAll':'fCIABudAll', \n",
        "    'CIAHindAll':'gCIAHindAll', \n",
        "    'CIABudHindAll':'hCIABudHindAll',\n",
        "    'CIAJewAll':'iCIAJewAll',\n",
        "    'CIAOthNoJew':'jCIAOthNoJew', \n",
        "    'CIAOthWithJew':'kCIAOthWithJew',\n",
        "    'CIABelieverAll':'lCIABelieverAll',\n",
        "    'CIANoRelAll':'mCIANoRelAll',    \n",
        "    'IsikFrench':'za_IsikFrench',\n",
        "    'BeckSettlerMortality':'zb_BeckSettlerMortality',\n",
        "    'wgi_avg':'zc_wgi_avg',\n",
        "    'sfi':'zd_sfi',\n",
        "    'Population (millions)':'ze_Population (millions)'},inplace=True)\n",
        "\n",
        "''' Run this code right before report runs/saves:\n",
        "#Rename columns for tables:\n",
        "piv.rename(columns={'aCIAProtAll':'Prot', \n",
        "                    'bCIACathAll':'Cath', \n",
        "                    'cCIAOrthAll':'Orth', \n",
        "                    'dCIAChrAll':'Chr',\n",
        "                    'eCIAMusAll':'Mus', \n",
        "                    'fCIABudAll':'Bud', \n",
        "                    'gCIAHindAll':'Hind', \n",
        "                    'hCIABudHindAll':'BudHind',\n",
        "                    'iCIAJewAll':'Jew', \n",
        "                    'jCIAOthNoJew':'Oth-NoJew', \n",
        "                    'kCIAOthWithJew':'Oth-Jew', \n",
        "                    'lCIABelieverAll':'Believers',\n",
        "                    'mCIANoRelAll':'No-Rel'},inplace=True)\n",
        "'''\n",
        "relig_list=['aCIAProtAll', 'bCIACathAll', 'cCIAOrthAll', 'dCIAChrAll', \n",
        "            'eCIAMusAll', 'fCIABudAll', 'gCIAHindAll', 'hCIABudHindAll',\n",
        "            'iCIAJewAll', 'jCIAOthNoJew', 'kCIAOthWithJew', 'lCIABelieverAll', 'mCIANoRelAll' ] \n"
      ],
      "metadata": {
        "id": "vXW6Igsxl0Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2t5q2tJYdv8"
      },
      "source": [
        "##Table 1: Economic, political, legal, and physical health across religions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDPShy9aYp7z"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "# lists of columns for this report\n",
        "##########\n",
        "my_col_list=['FIPS','country_relig','desc','overall','1980_89','1990_99','2000_09','2010_20']\n",
        "\n",
        "my_decades_list=['overall','1980_89','1990_99','2000_09','2010_20']\n",
        "\n",
        "row_order =['HDI', \n",
        "            'GNI per capita', \n",
        "            'Gini 2010-18', \n",
        "            'ze_Population (millions)', \n",
        "            'Inequality in income %', \n",
        "            'Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)',\n",
        "            'Poverty gap at $3.20 a day (2011 PPP) (%)',\n",
        "            'Poverty headcount ratio at $5.50 a day (2011 PPP) (% of population)',\n",
        "            'Multidimensional Poverty Index',\n",
        "            'Life expectancy at birth', \n",
        "            'Expected years of schooling', \n",
        "            'Prison population (per 100K) 2013-2018', \n",
        "            'zd_sfi',\n",
        "            'actotal', \n",
        "            'autoc']\n",
        "\n",
        "desc_lst = ['Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)',\n",
        "            'Poverty gap at $3.20 a day (2011 PPP) (%)',\n",
        "            'Poverty headcount ratio at $5.50 a day (2011 PPP) (% of population)',\n",
        "            'zd_sfi']\n",
        "\n",
        "# Read data\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "# Save values for table into a new table\n",
        "temp=[]\n",
        "for curr_relig in relig_list:\n",
        "    df_a=df_tmp.loc[(df_tmp['country_relig']==curr_relig)]\n",
        "    cnt=len(df_a)\n",
        "    for curr_col in row_order:\n",
        "        if curr_col in (desc_lst):\n",
        "            df_a=df_a.loc[df_a['desc']==curr_col]\n",
        "            df_a['overall']=pd.to_numeric(df_a['overall'], errors ='coerce')\n",
        "            df_a['overall'] = df_a['overall'].astype(float, errors = 'raise')\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                        'desc':curr_col,\n",
        "                        'value':df_a['overall'].mean()})\n",
        "            #reset df_a for all columns in curr_relig\n",
        "            df_a=df_tmp.loc[(df_tmp['country_relig']==curr_relig)]\n",
        "        else:\n",
        "            \n",
        "            df_a[curr_col]=pd.to_numeric(df_a[curr_col], errors ='coerce')\n",
        "            df_a[curr_col] = df_a[curr_col].astype(float, errors = 'raise')\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                        'desc':curr_col,\n",
        "                        'value':df_a[curr_col].mean()})\n",
        "            \n",
        "for curr_relig in relig_list:\n",
        "    t=df_tmp.loc[df_tmp['country_relig']==curr_relig]\n",
        "\n",
        "    print(curr_relig,t['FIPS'].nunique())\n",
        "\n",
        "#calc world averages\n",
        "for curr_col in row_order:\n",
        "    if curr_col in (desc_lst):\n",
        "        df_a=df_tmp.loc[df_tmp['desc']==curr_col]\n",
        "        df_a['overall']=pd.to_numeric(df_a['overall'], errors ='coerce')\n",
        "        df_a['overall'] = df_a['overall'].astype(float, errors = 'raise')\n",
        "        temp.append({'country_relig':'world',\n",
        "                    'desc':curr_col,\n",
        "                    'value':df_a['overall'].mean()})\n",
        "    else:\n",
        "        df_a[curr_col]=pd.to_numeric(df_tmp[curr_col], errors ='coerce')\n",
        "        df_a[curr_col] = df_a[curr_col].astype(float, errors = 'raise')\n",
        "        temp.append({'country_relig':'World',\n",
        "                    'desc':curr_col,\n",
        "                    'value':df_a[curr_col].mean()})\n",
        "\n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['desc'], columns=['country_relig'], \n",
        "    values=['value'], sort=False),3)\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "\n",
        "\n",
        "#$$$piv = piv.reindex(row_order, axis=0)\n",
        "#Sort Col order\n",
        "#print(piv.columns.to_list)\n",
        "\n",
        "piv.sort_index(axis='columns', level='country_relig')\n",
        "#$$$Rename columns for tables:\n",
        "piv.rename(columns={'aCIAProtAll':'Prot', \n",
        "                    'bCIACathAll':'Cath', \n",
        "                    'cCIAOrthAll':'Orth', \n",
        "                    'dCIAChrAll':'Chr',\n",
        "                    'eCIAMusAll':'Mus', \n",
        "                    'fCIABudAll':'Bud', \n",
        "                    'gCIAHindAll':'Hind', \n",
        "                    'hCIABudHindAll':'BudHind',\n",
        "                    'iCIAJewAll':'Jew', \n",
        "                    'jCIAOthNoJew':'Oth-NoJew', \n",
        "                    'kCIAOthWithJew':'Oth-Jew', \n",
        "                    'lCIABelieverAll':'Believers',\n",
        "                    'mCIANoRelAll':'No-Rel'},inplace=True)\n",
        "\n",
        "print ('piv:\\n\\n',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_1_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "#################   Multiple Comparison of means   ##################\n",
        "\n",
        "# must define the following lists:\n",
        "### 'row_order' = columns to be compared\n",
        "### desc_lst = values in desc field to be compared\n",
        "# must pass these values:\n",
        "### 'passed_df' \n",
        "### 'passed_col'\n",
        "### 'passed_col_name'\n",
        "\n",
        "for curr_dv in row_order:\n",
        "    # compare desc/overall value rather than column value\n",
        "    if curr_dv in desc_lst:\n",
        "        df_a=df_tmp.copy(deep=True)\n",
        "        df_a=df_a.loc[df_a['desc']==curr_dv]\n",
        "        df_a['overall']=pd.to_numeric(df_a['overall'], errors ='coerce')\n",
        "        df_a['overall'] = df_a['overall'].astype(float, errors = 'raise')\n",
        "        if len(df_a)>0:\n",
        "            #compare_means_long(df_a, 'overall')\n",
        "            res=pg.pairwise_gameshowell(dv='overall', between='country_relig', data=df_a).round(3)\n",
        "            print('\\nGames Howell:',curr_dv,'\\n')\n",
        "            signif=res.loc[res['pval']<.05]\n",
        "            insignif = pd.concat([res,signif]).drop_duplicates(keep=False)\n",
        "            print ('Signif Different Religions (Games Howell):', curr_dv,'\\n', signif[['A','B','pval']])\n",
        "            print ('Insignif Religions (Games Howell):', curr_dv,'\\n', insignif[['A','B','pval']])\n",
        "\n",
        "            try:\n",
        "                print('\\nttest of means --pairwise')\n",
        "                #gbtmp=df_a.groupby(['country_relig']).filter(lambda x: len(x)>1).reset_index()\n",
        "                #changed df to df_a\n",
        "                res=pg.pairwise_tests(dv='overall', between='country_relig', data=df_a).round(3)\n",
        "                signif=res.loc[res['p-unc']<.05]\n",
        "                insignif = pd.concat([res,signif]).drop_duplicates(keep=False)\n",
        "                print ('Signif Different Religions (ttest of means --pairwise):', curr_dv,'\\n', signif[['A','B','p-unc']])\n",
        "                print ('Insignif Religions (ttest of means --pairwise)):', curr_dv,'\\n', insignif[['A','B','p-unc']])\n",
        "\n",
        "            except: \n",
        "                print(curr_dv ,'has a NULL value.  Cannot perform all pairwise ttests.')\n",
        "                pass\n",
        "\n",
        "            ''' \n",
        "            base=df_a.loc[b['country_relig']=='CIANoRelAll'].copy(deep=True)\n",
        "            for curr_relig in relig_list:\n",
        "                if curr_relig != 'CIANoRelAll':\n",
        "                    try:\n",
        "                        print(\"\\n\\nttest of means (baseline is CIANoRelAll --assumes equal variance and sample size): \\n\")\n",
        "                        c=df_a.loc[(b['country_relig']==curr_relig)&(b['desc']==curr_dv)].copy(deep=True)\n",
        "                        t_stat, p_val = stats.ttest_ind(c['overall'], base['overall'])\n",
        "                        if p_val < .05:\n",
        "                            print ('Signif Different Religions (baseline is CIANoRelAll). p-value=:',p_val, curr_dv, curr_relig,'\\n')\n",
        "                        else:\n",
        "                            print ('Insignif Religions (baseline is CIANoRelAll). p-value=:',p_val, curr_dv,curr_relig,'\\n')\n",
        "                    except: \n",
        "                        print(curr_dv ,'has a NULL value. Cannot perform all pairwise ttests.')\n",
        "                        pass\n",
        "            '''\n",
        "    else:  #compare column (rather than desc/overall value)\n",
        "        df_tmp[curr_dv]=pd.to_numeric(df_tmp[curr_dv], errors ='coerce')\n",
        "        df_tmp[curr_dv] = df_tmp[curr_dv].astype(float, errors = 'raise')    \n",
        "\n",
        "        res=pg.pairwise_gameshowell(dv=curr_dv, between='country_relig', data=df_tmp).round(3)\n",
        "        print('\\nGames Howell:',curr_dv,'\\n')\n",
        "        signif=res.loc[res['pval']<.05]\n",
        "        insignif = pd.concat([res,signif]).drop_duplicates(keep=False)\n",
        "        print ('Signif Different Religions (Games Howell):', curr_dv,'\\n', signif[['A','B','pval']])\n",
        "        print ('Insignif Religions (Games Howell):', curr_dv,'\\n', insignif[['A','B','pval']])\n",
        "    \n",
        "        print('\\nttest of means --pairwise; uses Welsh for unequal variance or unequal sample size')\n",
        "        #gbtmp=df_tmp.groupby(['country_relig']).filter(lambda x: len(x)>1).reset_index()\n",
        "        try:\n",
        "            res=pg.pairwise_tests(dv=curr_dv, between='country_relig', data=df_tmp, nan_policy='listwise').round(3)\n",
        "            signif=res.loc[res['p-unc']<.05]\n",
        "            insignif = pd.concat([res,signif]).drop_duplicates(keep=False)\n",
        "            print ('Signif Different Religions (ttest of means --pairwise):', curr_dv,'\\n', signif[['A','B','p-unc']])\n",
        "            print ('Insignif Religions (ttest of means --pairwise)):', curr_dv,'\\n', insignif[['A','B','p-unc']])\n",
        "    \n",
        "        except: \n",
        "            print(curr_dv ,'has a NULL value. Cannot perform all pairwise ttests.')\n",
        "            pass\n",
        "\n",
        "        '''\n",
        "        base=df_tmp.loc[df_tmp['country_relig']=='CIANoRelAll'].copy(deep=True)\n",
        "        for curr_relig in relig_list:\n",
        "            if curr_relig != 'CIANoRelAll':\n",
        "                try:\n",
        "                    print(\"\\n\\nttest of means (baseline is CIANoRelAll -assumes equal variance and sample size): \\n\")\n",
        "                    c=df_tmp.loc[df_tmp['country_relig']==curr_relig].copy(deep=True)\n",
        "                    t_stat, p_val = stats.ttest_ind(c[curr_dv], base[curr_dv])\n",
        "                    if p_val < .05:\n",
        "                        print ('Signif Different Religions (baseline is CIANoRelAll). p-value=:',p_val, curr_dv, curr_relig,'\\n')\n",
        "                    else:\n",
        "                        print ('Insignif Religions (baseline is CIANoRelAll). p-value=:',p_val, curr_dv,curr_relig,'\\n')\n",
        "                except: \n",
        "                    print(curr_dv ,'has a NULL value. Cannot perform all pairwise ttests.')\n",
        "                    pass\n",
        "        '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8a1D0y1a_yF"
      },
      "outputs": [],
      "source": [
        "a=df_tmp.copy(deep=True)\n",
        "a.reset_index(drop=False, inplace=True)\n",
        "print(a.index,a.columns.to_list)\n",
        "\n",
        "cols=['index', 'country_relig', 'desc', 'my_level', 'value']\n",
        "\n",
        "'''\n",
        "    fvalue, pvalue = stats.f_oneway(b[['country_relig']])\n",
        "\n",
        "            a[('value',     'CIAChrAll')],\n",
        "            a[('value',     'CIAMusAll')],\n",
        "            a[('value',     'CIABudAll')],\n",
        "            a[('value',    'CIAHindAll')],\n",
        "            a[('value', 'CIABudHindAll')],\n",
        "            a[('value',   'CIANoRelAll')],\n",
        "            a[('value',     'CIAJewAll')],\n",
        "            a[('value',   'CIAOthNoJew')],\n",
        "            a[('value', 'CIAOthWithJew')])\n",
        "\n",
        "print('statsmodels unconstrained ANOVA: H0: no differences when comparing religions\\n',fvalue, pvalue)\n",
        "\n",
        "alpha=.05\n",
        "if (pvalue < alpha):\n",
        "    print('Reject H0, religions have different values')\n",
        "'''\n",
        "'''    # get ANOVA table as R like output\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "############later I should include actotal & autoc##############\n",
        "for curr_dv in desc_col_lst: \n",
        "    b=a.loc[a['desc']==curr_dv]\n",
        "    #b['value']=np.abs(b['value'])\n",
        "    print(a.head(50))\n",
        "\n",
        "    # Ordinary Least Squares (OLS) model\n",
        "    model=ols('value ~ country_relig', data=b).fit() #added 'overall' column\n",
        "    #model = ols('value ~ C(treatments)', data=df_melt).fit()\n",
        "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "    print('\\n ANOVA: H0=equal means:\\n',curr_col, '\\n', anova_table)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phn6tMLwznow"
      },
      "source": [
        "##Table 2 Longitudinal religions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpcxyTW2zsot"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#load hist_country_relig data (by FIPS)\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/data'\n",
        "myfile = 'df_all_religions.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "df_all_religions=pd.read_excel(joined_path, sheet_name='Sheet1')\n",
        "print('df_all_religions data:\\n',df_all_religions.info(verbose=True),df_all_religions.head())\n",
        "\n",
        "#load Population data (by FIPS, by year)\n",
        "myfile = 'df_population.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "df_population=pd.read_excel(joined_path, sheet_name='Sheet1')\n",
        "\n",
        "#df_populations FIPS ='world' for each year\n",
        "\n",
        "#drop unnamed columns\n",
        "df_population = df_population[df_population.columns.drop(list(df_population.filter(regex='Unnamed')))]\n",
        "df_population = df_population[['FIPS', 1900, 1945, 1960, 1980, 2000, 2021]]\n",
        "#df_population.set_index('FIPS')\n",
        "df_all_religions=df_all_religions[['CIAProt%',\n",
        "                                    'CIACath%',\n",
        "                                    'CIAOrth%',\n",
        "                                    'CIAChr%',\n",
        "                                    'CIAMus%',\n",
        "                                    'CIABud%',\n",
        "                                    'CIAHindu%',\n",
        "                                    'CIABudHind%',\n",
        "                                    'CIANoRel%',\n",
        "                                    'CIAJew%',\n",
        "                                    'CIAOthNoJew%',\n",
        "                                    'CIAOthWithJew%',\n",
        "                                    'FIPS',\n",
        "                                    'Prot2000%',\n",
        "                                    'Cath2000%',\n",
        "                                    'Orth2000%',\n",
        "                                    'NonTradChr2000%',\n",
        "                                    'Chr2000%',\n",
        "                                    'Mus2000%',\n",
        "                                    'Bud2000%',\n",
        "                                    'Hindu2000%',\n",
        "                                    'BudHind2000%',\n",
        "                                    'NoRel2000%',\n",
        "                                    'Jew2000%',\n",
        "                                    'OthNoJew2000%',\n",
        "                                    'OthWithJew2000%',\n",
        "                                    'Prot1945%',\n",
        "                                    'Cath1945%',\n",
        "                                    'Orth1945%',\n",
        "                                    'NonTradChr1945%',\n",
        "                                    'Chr1945%',\n",
        "                                    'Mus1945%',\n",
        "                                    'Bud1945%',\n",
        "                                    'Hindu1945%',\n",
        "                                    'BudHind1945%',\n",
        "                                    'NoRel1945%',\n",
        "                                    'Jew1945%',\n",
        "                                    'OthNoJew1945%',\n",
        "                                    'OthWithJew1945%',\n",
        "                                    'Prot1960%',\n",
        "                                    'Cath1960%',\n",
        "                                    'Orth1960%',\n",
        "                                    'NonTradChr1960%',\n",
        "                                    'Chr1960%',\n",
        "                                    'Mus1960%',\n",
        "                                    'Bud1960%',\n",
        "                                    'Hindu1960%',\n",
        "                                    'BudHind1960%',\n",
        "                                    'NoRel1960%',\n",
        "                                    'Jew1960%',\n",
        "                                    'OthNoJew1960%',\n",
        "                                    'OthWithJew1960%',\n",
        "                                    'Prot1980%',\n",
        "                                    'Cath1980%',\n",
        "                                    'Orth1980%',\n",
        "                                    'NonTradChr1980%',\n",
        "                                    'Chr1980%',\n",
        "                                    'Mus1980%',\n",
        "                                    'Bud1980%',\n",
        "                                    'Hindu1980%',\n",
        "                                    'BudHind1980%',\n",
        "                                    'NoRel1980%',\n",
        "                                    'Jew1980%',\n",
        "                                    'OthNoJew1980%',\n",
        "                                    'OthWithJew1980%',\n",
        "                                    'Prot1900%',\n",
        "                                    'Cath1900%',\n",
        "                                    'Orth1900%',\n",
        "                                    'NonTradChr1900%',\n",
        "                                    'Chr1900%',\n",
        "                                    'Mus1900%',\n",
        "                                    'Bud1900%',\n",
        "                                    'Hindu1900%',\n",
        "                                    'BudHind1900%',\n",
        "                                    'NoRel1900%',\n",
        "                                    'Jew1900%',\n",
        "                                    'OthNoJew1900%',\n",
        "                                    'OthWithJew1900%'\n",
        "                                    ]]\n",
        "\n",
        "#df_all_religions.set_index('FIPS')\n",
        "df=pd.DataFrame(columns=['FIPS','Prot2021'])\n",
        "df[2021] =df_population['FIPS'].map(df_population.set_index('FIPS')[2021])\n",
        "df['Prot2021%'] = df['FIPS'].map(df_all_religions.set_index('FIPS')['CIAProt%'])\n",
        "df=df_population.merge(df_all_religions, how='right',on='FIPS')\n",
        "df['prot_2021']=(df['CIAProt%']*df[2021]).sum()\n",
        "\n",
        "print('sum-product?:\\n',df.head(75))\n",
        "\n",
        "#valid_columns = [col for col in df.columns if col.isdigit()]\n",
        "#df['sumproduct'] = (df[valid_columns] * [int(x) for x in valid_columns]).sum(axis=1)\n",
        "\n",
        "#print('df_population data:\\n',df_population.info(verbose=True),df_population.head())\n",
        "\n",
        "#manually load data into df\n",
        "#df = pd.DataFrame(columns=['FIPS','hist_country_relig%', 1900, 1945, 1960, 1980, 2000, 2021 ])\n",
        "\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "df_tmp=df_tmp.reset_index(drop=False)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                 'country',\n",
        "                 'country_relig',\n",
        "                 'CIAProt%', 'CIACath%', 'CIAOrth%', 'CIAChr%', 'CIAMus%', 'CIABud%', 'CIAHindu%', 'CIABudHind%', 'CIANoRel%', 'CIAJew%', 'CIAOthNoJew%', 'CIAOthWithJew%', \n",
        "                 'Prot2000', 'Cath2000', 'Orth2000', 'Chr2000', 'Mus2000', 'Bud2000', 'Hind2000', 'BudHind2000', 'NoRel2000', 'Jew2000', 'OthNoJew2000', 'OthWithJew2000', 'Believer2000', \n",
        "                 'Prot2000%', 'Cath2000%', 'Orth2000%', 'NonTradChr2000%', 'Chr2000%', 'Mus2000%', 'Bud2000%', 'Hindu2000%', 'BudHind2000%', 'NoRel2000%', 'Jew2000%', 'OthNoJew2000%', 'OthWithJew2000%', \n",
        "                 'Prot1945', 'Cath1945', 'Orth1945', 'Chr1945', 'Mus1945', 'Bud1945', 'Hind1945', 'BudHind1945', 'NoRel1945', 'Jew1945', 'OthNoJew1945', 'OthWithJew1945', 'Believer1945', \n",
        "                 'Prot1945%', 'Cath1945%', 'Orth1945%', 'NonTradChr1945%', 'Chr1945%', 'Mus1945%', 'Bud1945%', 'Hindu1945%', 'BudHind1945%', 'NoRel1945%', 'Jew1945%', 'OthNoJew1945%', 'OthWithJew1945%', \n",
        "                 'Prot1960', 'Cath1960', 'Orth1960', 'Chr1960', 'Mus1960', 'Bud1960', 'Hind1960', 'BudHind1960', 'NoRel1960', 'Jew1960', 'OthNoJew1960', 'OthWithJew1960', 'Believer1960', \n",
        "                 'Prot1960%', 'Cath1960%', 'Orth1960%', 'NonTradChr1960%', 'Chr1960%', 'Mus1960%', 'Bud1960%', 'Hindu1960%', 'BudHind1960%', 'NoRel1960%', 'Jew1960%', 'OthNoJew1960%', 'OthWithJew1960%', \n",
        "                 'Prot1980', 'Cath1980', 'Orth1980', 'Chr1980', 'Mus1980', 'Bud1980', 'Hind1980', 'BudHind1980', 'NoRel1980', 'Jew1980', 'OthNoJew1980', 'OthWithJew1980', 'Believer1980', \n",
        "                 'Prot1980%', 'Cath1980%', 'Orth1980%', 'NonTradChr1980%', 'Chr1980%', 'Mus1980%', 'Bud1980%', 'Hindu1980%', 'BudHind1980%', 'NoRel1980%', 'Jew1980%', 'OthNoJew1980%', 'OthWithJew1980%', \n",
        "                 'Prot1900', 'Cath1900', 'Orth1900', 'Chr1900', 'Mus1900', 'Bud1900', 'Hind1900', 'BudHind1900', 'NoRel1900', 'Jew1900', 'OthNoJew1900', 'OthWithJew1900', 'Believer1900', \n",
        "                 'Prot1900%', 'Cath1900%', 'Orth1900%', 'NonTradChr1900%', 'Chr1900%', 'Mus1900%', 'Bud1900%', 'Hindu1900%', 'BudHind1900%', 'NoRel1900%', 'Jew1900%', 'OthNoJew1900%', 'OthWithJew1900%', \n",
        "                 'desc', \n",
        "                 2020, 2021, \n",
        "                 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,\n",
        "                 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, \n",
        "                 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, \n",
        "                 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, \n",
        "                 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, \n",
        "                 1945, 1946, 1947, 1948, 1949, \n",
        "                 1900\n",
        "                ]\n",
        "\n",
        "'''\n",
        "'''\n",
        "df_tmp=df_tmp[[keep_col_lst]]\n",
        "df_tmp.drop_duplicates(inplace=True, ignore_index=True)\n",
        "'''\n",
        "'''\n",
        "# add additional columns\n",
        "\n",
        "\n",
        "# by relig by year\n",
        "temp=[]\n",
        "for curr_level in select_values:\n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in desc_col_lst:\n",
        "            df_col=df_r1.copy(deep=True)\n",
        "            df_col1=df_col.loc[(df_col['my_level']==curr_level)&(df_col['desc']==curr_col)&(df_col['country_relig']==curr_relig)]\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                        'desc':curr_col,\n",
        "                        'my_level':curr_level,\n",
        "                        'value':df_col1['overall'].mean()})\n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "print ('dfloop:::::::',dfloop.head(15))\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['my_level','country_relig'], columns=['desc'], \n",
        "    values=['value'], sort=False),3)\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "row_order =['HDI Score', \n",
        "         'UN Income Index',\n",
        "         'Gini index (World Bank estimate)', \n",
        "         'Multidimensional Poverty Index',\n",
        "         'UN Life Expectancy Index', \n",
        "         'Life expectancy at birth, total (years)',\n",
        "         'UIS: Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes',\n",
        "         'Prison per 100K', \n",
        "         'sfi',\n",
        "         'actotal', \n",
        "         'autoc']\n",
        "\n",
        "#piv = piv.reindex(row_order, axis=0)\n",
        "\n",
        "#Sort Col order\n",
        "print(piv.columns.to_list)\n",
        "\n",
        "order = [   ('value',    'CIAProtAll'),\n",
        "            ('value',    'CIACathAll'),\n",
        "            ('value',    'CIAOrthAll'),\n",
        "            ('value',     'CIAChrAll'),\n",
        "            ('value',     'CIAMusAll'),\n",
        "            ('value',     'CIABudAll'),\n",
        "            ('value',    'CIAHindAll'),\n",
        "            ('value', 'CIABudHindAll'),\n",
        "            ('value',     'CIAJewAll'),\n",
        "            ('value',   'CIAOthNoJew'),\n",
        "            ('value', 'CIAOthWithJew'),\n",
        "            ('value',   'CIABeliever'),  \n",
        "            ('value',   'CIANoRelAll')\n",
        "        ]        \n",
        "   \n",
        "col_order=[('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')\n",
        "          ]\n",
        "piv = piv.reindex(col_order, axis=1)\n",
        "\n",
        "print ('piv',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_colonizer_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_colonizer_counts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "df_level.to_excel(joined_path)\n",
        "\n",
        "time_frame=['overall']\n",
        "\n",
        "#compare_means(df_tmp, other)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUJQqbVFhmE8"
      },
      "source": [
        "##Table 3: relig by decade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mAhUZCd_A20"
      },
      "outputs": [],
      "source": [
        "# Clean load of data\n",
        "a=df_merged.copy(deep=True)\n",
        "\n",
        "#lists of columns for this report\n",
        "fd_list=pd.Categorical(['Financial Development Index', \n",
        "         'Financial Institutions Index', 'Financial Institutions Depth Index',\n",
        "         'Financial Institutions Access Index', 'Financial Institutions Efficiency Index',\n",
        "         'Financial Markets Index','Financial Markets Depth Index',\n",
        "         'Financial Markets Access Index','Financial Markets Efficiency Index'], ordered=True)\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','1980_89','1990_99','2000_09','2010_20']\n",
        "\n",
        "my_decades_list=['overall','1980_89','1990_99','2000_09','2010_20']\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list) # to see row index\n",
        "\n",
        "row_order =[    ('1980_89',             'Financial Development Index'),\n",
        "            ('1980_89',            'Financial Institutions Index'),\n",
        "            ('1980_89',      'Financial Institutions Depth Index'),\n",
        "            ('1980_89',     'Financial Institutions Access Index'),\n",
        "            ('1980_89', 'Financial Institutions Efficiency Index'),\n",
        "            ('1980_89',                 'Financial Markets Index'),\n",
        "            ('1980_89',           'Financial Markets Depth Index'),\n",
        "            ('1980_89',          'Financial Markets Access Index'),\n",
        "            ('1980_89',      'Financial Markets Efficiency Index'),\n",
        "            ('1990_99',             'Financial Development Index'),\n",
        "            ('1990_99',            'Financial Institutions Index'),\n",
        "            ('1990_99',      'Financial Institutions Depth Index'),\n",
        "            ('1990_99',     'Financial Institutions Access Index'),\n",
        "            ('1990_99', 'Financial Institutions Efficiency Index'),\n",
        "            ('1990_99',                 'Financial Markets Index'),\n",
        "            ('1990_99',           'Financial Markets Depth Index'),\n",
        "            ('1990_99',          'Financial Markets Access Index'),\n",
        "            ('1990_99',      'Financial Markets Efficiency Index'),\n",
        "            ('2000_09',             'Financial Development Index'),\n",
        "            ('2000_09',            'Financial Institutions Index'),\n",
        "            ('2000_09',      'Financial Institutions Depth Index'),\n",
        "            ('2000_09',     'Financial Institutions Access Index'),\n",
        "            ('2000_09', 'Financial Institutions Efficiency Index'),\n",
        "            ('2000_09',                 'Financial Markets Index'),\n",
        "            ('2000_09',           'Financial Markets Depth Index'),\n",
        "            ('2000_09',          'Financial Markets Access Index'),\n",
        "            ('2000_09',      'Financial Markets Efficiency Index'),\n",
        "            ('2010_20',             'Financial Development Index'),\n",
        "            ('2010_20',            'Financial Institutions Index'),\n",
        "            ('2010_20',      'Financial Institutions Depth Index'),\n",
        "            ('2010_20',     'Financial Institutions Access Index'),\n",
        "            ('2010_20', 'Financial Institutions Efficiency Index'),\n",
        "            ('2010_20',                 'Financial Markets Index'),\n",
        "            ('2010_20',           'Financial Markets Depth Index'),\n",
        "            ('2010_20',          'Financial Markets Access Index'),\n",
        "            ('2010_20',      'Financial Markets Efficiency Index'),\n",
        "            ('overall',             'Financial Development Index'),\n",
        "            ('overall',            'Financial Institutions Index'),\n",
        "            ('overall',      'Financial Institutions Depth Index'),\n",
        "            ('overall',     'Financial Institutions Access Index'),\n",
        "            ('overall', 'Financial Institutions Efficiency Index'),\n",
        "            ('overall',                 'Financial Markets Index'),\n",
        "            ('overall',           'Financial Markets Depth Index'),\n",
        "            ('overall',          'Financial Markets Access Index'),\n",
        "            ('overall',      'Financial Markets Efficiency Index')]\n",
        "\n",
        "#print(df_b_piv.columns.to_list) # to see column index\n",
        "\n",
        "col_order = [('mean',   'CIAProtAll'),\n",
        "            ('mean',    'CIACathAll'),\n",
        "            ('mean',    'CIAOrthAll'),\n",
        "            ('mean',     'CIAChrAll'),\n",
        "            ('mean',     'CIAMusAll'),\n",
        "            ('mean',     'CIABudAll'),\n",
        "            ('mean',    'CIAHindAll'),\n",
        "            ('mean', 'CIABudHindAll'),\n",
        "            ('mean',     'CIAJewAll'),\n",
        "            ('mean',   'CIAOthNoJew'),\n",
        "            ('mean', 'CIAOthWithJew'),\n",
        "            ('mean',  'CIABelieverAll'),\n",
        "            ('mean',   'CIANoRelAll')\n",
        "            ]  \n",
        "\n",
        "row_cnt_order=[('1980_89', 'CIAProtAll'),\n",
        "            ('1980_89',    'CIACathAll'),\n",
        "            ('1980_89',    'CIAOrthAll'),\n",
        "            ('1980_89',     'CIAChrAll'),\n",
        "            ('1980_89',     'CIAMusAll'),\n",
        "            ('1980_89',     'CIABudAll'),\n",
        "            ('1980_89',    'CIAHindAll'),\n",
        "            ('1980_89', 'CIABudHindAll'),\n",
        "            ('1980_89',     'CIAJewAll'),\n",
        "            ('1980_89',   'CIAOthNoJew'),\n",
        "            ('1980_89', 'CIAOthWithJew'),\n",
        "            ('1980_89',  'CIABelieverAll'),\n",
        "            ('1980_89',   'CIANoRelAll'),\n",
        "            ('1990_99',    'CIAProtAll'),\n",
        "            ('1990_99',    'CIACathAll'),\n",
        "            ('1990_99',    'CIAOrthAll'),\n",
        "            ('1990_99',     'CIAChrAll'),\n",
        "            ('1990_99',     'CIAMusAll'),\n",
        "            ('1990_99',     'CIABudAll'),\n",
        "            ('1990_99',    'CIAHindAll'),\n",
        "            ('1990_99', 'CIABudHindAll'),\n",
        "            ('1990_99',     'CIAJewAll'),\n",
        "            ('1990_99',   'CIAOthNoJew'),\n",
        "            ('1990_99', 'CIAOthWithJew'),\n",
        "            ('1990_99',  'CIABelieverAll'),\n",
        "            ('1990_99',   'CIANoRelAll'),\n",
        "\n",
        "            ('2000_09',    'CIAProtAll'),\n",
        "            ('2000_09',    'CIACathAll'),\n",
        "            ('2000_09',    'CIAOrthAll'),\n",
        "            ('2000_09',     'CIAChrAll'),\n",
        "            ('2000_09',     'CIAMusAll'),\n",
        "            ('2000_09',     'CIABudAll'),\n",
        "            ('2000_09',    'CIAHindAll'),\n",
        "            ('2000_09', 'CIABudHindAll'),\n",
        "            ('2000_09',     'CIAJewAll'),\n",
        "            ('2000_09',   'CIAOthNoJew'),\n",
        "            ('2000_09', 'CIAOthWithJew'),\n",
        "            ('2000_09',  'CIABelieverAll'),\n",
        "            ('2000_09',   'CIANoRelAll'),\n",
        "            \n",
        "            ('2010_20',    'CIAProtAll'),\n",
        "            ('2010_20',    'CIACathAll'),\n",
        "            ('2010_20',    'CIAOrthAll'),\n",
        "            ('2010_20',     'CIAChrAll'),\n",
        "            ('2010_20',     'CIAMusAll'),\n",
        "            ('2010_20',     'CIABudAll'),\n",
        "            ('2010_20',    'CIAHindAll'),\n",
        "            ('2010_20', 'CIABudHindAll'),\n",
        "            ('2010_20',     'CIAJewAll'),\n",
        "            ('2010_20',   'CIAOthNoJew'),\n",
        "            ('2010_20', 'CIAOthWithJew'),\n",
        "            ('2010_20',  'CIABelieverAll'),\n",
        "            ('2010_20',   'CIANoRelAll'),\n",
        "\n",
        "            ('overall',    'CIAProtAll'),\n",
        "            ('overall',    'CIACathAll'),\n",
        "            ('overall',    'CIAOrthAll'),\n",
        "            ('overall',     'CIAChrAll'),\n",
        "            ('overall',     'CIAMusAll'),\n",
        "            ('overall',     'CIABudAll'),\n",
        "            ('overall',    'CIAHindAll'),\n",
        "            ('overall', 'CIABudHindAll'),\n",
        "            ('overall',     'CIAJewAll'),\n",
        "            ('overall',   'CIAOthNoJew'),\n",
        "            ('overall', 'CIAOthWithJew'),\n",
        "            ('overall',  'CIABelieverAll'),\n",
        "            ('overall',   'CIANoRelAll')\n",
        "            ] \n",
        "\n",
        "#reduce columns of dataframe\n",
        "df_a=a.loc[a['desc'].isin(fd_list)]\n",
        "df_a.drop(df_a.columns.difference(my_col_list),axis=1, inplace=True )\n",
        "\n",
        "###########\n",
        "# PRINTS the world avg by decade\n",
        "############\n",
        "w=df_a.loc[df_a['FIPS']=='wld']\n",
        "print('FD order is NOT correct!!!!\\n',w)\n",
        "\n",
        "#move decade values from columns to rows (make long)\n",
        "b=pd.melt(df_a,id_vars=['FIPS','country_relig','desc'])\n",
        "b.rename(columns={'variable':'year'},inplace=True)\n",
        "b=b.loc[(b['year']!=0)]\n",
        "wcnt=b.loc[b['FIPS']=='wld']\n",
        "df_wcnt=b.groupby(['year'])['FIPS'].nunique()\n",
        "print('Unique FIPS per decade for world:\\n',df_wcnt)\n",
        "\n",
        "#generate pivot_table with mean values\n",
        "df_b_piv = np.round(b.pivot_table(index=['year','desc'], columns='country_relig', values='value', aggfunc=['mean'],margins=True),3)\n",
        "\n",
        "#update row, then column order\n",
        "piv = df_b_piv.reindex(row_order, axis=0)\n",
        "piv = piv.reindex(col_order, axis=1)\n",
        "\n",
        "print (df_b_piv)\n",
        "\n",
        "#Calc counts\n",
        "relig_cnt = b.groupby(['year','country_relig'])['FIPS'].nunique()\n",
        "\n",
        "#print(relig_cnt.index.to_list) # to see row index\n",
        "print(relig_cnt)\n",
        "piv_cnt = relig_cnt.reindex(row_cnt_order, axis=0)\n",
        "print ('piv_cnt:',piv_cnt)\n",
        "\n",
        "piv.rename(columns={'CIAProtAll':'Prot', 'CIACathAll':'Cath', 'CIAOrthAll':'Orth', 'CIAChrAll':'Chr', \n",
        "            'CIAMusAll':'Mus', 'CIABudAll':'Bud', 'CIAHindAll':'Hind', 'CIABudHindAll':'BudHind',\n",
        "            'CIAJewAll':'Jew', 'CIAOthNoJew':'Oth-NoJew', 'CIAOthWithJew':'Oth-Jew', \n",
        "            'CIABelieverAll':'Believers','CIANoRelAll':'No-Rel'},inplace=True)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_3_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_3_cnt.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv_cnt.to_excel(joined_path)\n",
        "\n",
        "myfile = 'nTable_3_World.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "w.to_excel(joined_path)\n",
        "\n",
        "time_frame=['overall','1980_89','1990_99','2000_09','2010_20']\n",
        "\n",
        "#####################  \n",
        "# compare_means does not work  \n",
        "################################\n",
        "#compare_means(df_a, 'decades')\n",
        "\n",
        "'''\n",
        "#######OLD method\n",
        "temp=[]\n",
        "for decade in my_decades_list: \n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in fd_list:\n",
        "            df_a=a.loc[(a['desc']==curr_col) & (a['country_relig']==curr_relig)]\n",
        "            temp.append({'country_relig':curr_relig,'year':decade,'desc':curr_col,'value':df_a[decade].mean()})\n",
        "dftemp = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "piv=np.round(dftemp.pivot_table(\n",
        "    index=['year','desc'], columns=['country_relig'], \n",
        "    values=['value'], aggfunc=['mean','count'], sort=False),3)\n",
        "#world values df\n",
        "dftemp = pd.DataFrame.from_dict(temp)\n",
        "dftemp[\"desc\"] = dftemp[\"desc\"].astype('category')\n",
        "\n",
        "wrld=np.round(dftemp.groupby(['desc','year'])['value'].mean(),3)\n",
        "\n",
        "#REFRESH the data for the counts\n",
        "\n",
        "a=pd.melt(df_iset_subset,id_vars=['country','FIPS','desc'])\n",
        "a.rename(columns={'variable':'year'},inplace=True)\n",
        "\n",
        "#Merge with religion data\n",
        "a=a.merge(df_all_religions,on='FIPS',how='outer')\n",
        "a.rename(columns={'country_x':'country'},inplace=True)\n",
        "a=a.loc[~a['value'].isnull()]\n",
        "a=a.loc[a['year'].isin(my_decades_list)]\n",
        "relig_cnt=a.groupby(['year','country_relig'])['FIPS'].nunique()\n",
        "wrld_cnt=a.groupby(['year'])['FIPS'].nunique()\n",
        "print ('WO|RLD:',wrld_cnt,'BY RELIG:', relig_cnt)\n",
        "'''\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyXNgSUnwQzy"
      },
      "source": [
        "###NOT USED For Table 3... Unequal variance/sample size: Welsh's ANOVA & Games Howell multiple comparison of means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPgLqlWPnbYA"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "#perform Welch's ANOVA (for unequal variances)\n",
        "\n",
        "print(\"Welch's ANOVA for unequal variances\\n\",)\n",
        "print(pg.welch_anova(dv='value', between='country_relig', data=a))\n",
        "\n",
        "my_decades_list=['overall','1980_89','1990_99','2000_09','2010_20']\n",
        "b=copy.deepcopy(b.loc[b['year']=='overall'])\n",
        "a=b\n",
        "print(\"\\n\\nGames Howell multiple comparison of means for unequal variances and/or unequal sample sizes\\n\")\n",
        "\n",
        "for curr_decade in my_decades_list:\n",
        "    for curr_dv in b['desc'].unique():\n",
        "        c=a.loc[(a['year']==curr_decade) & (a['desc']==curr_dv)]\n",
        "        try:\n",
        "            res=pg.pairwise_gameshowell(dv='value', between='country_relig', data=c)\n",
        "            print('\\n',curr_decade, curr_dv,':\\n')\n",
        "            signif=res.loc[res['pval']<.05]\n",
        "            print (signif[['A','B','pval']])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "a=a.loc[a['year'].isin(my_decades_list)]\n",
        "\n",
        "for curr_relig in relig_list:\n",
        "    for curr_dv in b['desc'].unique():\n",
        "        c=a.loc[(a['country_relig']==curr_relig) & (a['desc']==curr_dv)]\n",
        "        try:\n",
        "            res=pg.pairwise_gameshowell(dv='value', between='year', data=c)\n",
        "            print('\\n',curr_relig, curr_dv,':\\n')\n",
        "            signif=res.loc[res['pval']<.05]\n",
        "            print (signif[['A','B','pval']])\n",
        "        except:\n",
        "            pass\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzqWS8kYWdVi"
      },
      "source": [
        "###NOT USED: Original Table 3 code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EdK7Go3goZ9"
      },
      "outputs": [],
      "source": [
        "#Print mean values for table 3\n",
        "'''\n",
        "my_decades_list=['overall','1980_89','1990_99','2000_09','2010_20']\n",
        "\n",
        "temp=[]\n",
        "for decade in my_decades_list: \n",
        "    for curr_relig in relig_list:\n",
        "        collst=a['desc'].unique()\n",
        "        for curr_col in collst:\n",
        "            df_a=a.loc[(a['desc']==curr_col) & (a['country_relig']==curr_relig) & (a['year']==decade)].copy(deep=True)\n",
        "            temp.append({'country_relig':curr_relig,'year':decade,'desc':curr_col,'value':df_a['value'].mean()})\n",
        "dftemp = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "fd_list=pd.Categorical(['Financial Development Index', \n",
        "         'Financial Institutions Index', 'Financial Institutions Depth Index',\n",
        "         'Financial Institutions Access Index', 'Financial Institutions Efficiency Index',\n",
        "         'Financial Markets Index','Financial Markets Depth Index',\n",
        "         'Financial Markets Access Index','Financial Markets Efficiency Index'], ordered=True)\n",
        "\n",
        "for decade in my_decades_list:\n",
        "    dftemp = pd.DataFrame.from_dict(temp)\n",
        "    dftemp = dftemp.loc[dftemp['year']==decade].copy(deep=True) \n",
        "    dftemp[\"country_relig\"] = dftemp[\"country_relig\"].astype('category')\n",
        "    dftemp[\"desc\"] = dftemp[\"desc\"].astype('category')\n",
        "\n",
        "    #sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "    piv=np.round(dftemp.pivot_table(index='desc', columns=['country_relig'], values='value', sort=False),3)\n",
        "\n",
        "    #sort rows in pivot_table\n",
        "    piv=piv.reindex(fd_list,axis=0)\n",
        "    print ('piv for:',decade,'\\n',piv)\n",
        "\n",
        "    mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "    myfile = 'nTable_3_'+decade+'.xlsx'\n",
        "    joined_path = os.path.join(mypath, myfile)\n",
        "    piv.to_excel(joined_path)\n",
        "\n",
        "dftemp = pd.DataFrame.from_dict(temp)\n",
        "dftemp[\"desc\"] = dftemp[\"desc\"].astype('category')\n",
        "\n",
        "wrld=np.round(dftemp.groupby(['desc','year'])['value'].mean(),3)\n",
        "\n",
        "#REFRESH the data for the counts\n",
        "a=pd.melt(df_iset_subset,id_vars=['country','FIPS','desc'])\n",
        "a.rename(columns={'variable':'year'},inplace=True)\n",
        "\n",
        "#Merge with religion data\n",
        "a=a.merge(df_all_religions,on='FIPS',how='outer')\n",
        "a.rename(columns={'country_x':'country'},inplace=True)\n",
        "a=a.loc[~a['value'].isnull()]\n",
        "a=a.loc[a['year'].isin(my_decades_list)]\n",
        "r=a.groupby(['year','country_relig'])['FIPS'].nunique()\n",
        "w=a.groupby(['year'])['FIPS'].nunique()\n",
        "#print(a.groupby(['year','country_relig'])['FIPS'].nunique())\n",
        "print ('WORLD:',w,'BY RELIG:', r)\n",
        "\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_3_wrld_values.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "wrld.to_excel(joined_path)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_3_wrld_cnts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "w.to_excel(joined_path)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_3_relig_cnts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "r.to_excel(joined_path)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ISQgbRFmQt4"
      },
      "source": [
        "##Table x. Religion and settler mortality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fLfyKqAmMgw"
      },
      "outputs": [],
      "source": [
        "#REFRESH the data for the counts\n",
        "'''\n",
        "c=pd.melt(df_iset_subset,id_vars=['country','FIPS','desc'])\n",
        "c.rename(columns={'variable':'year'},inplace=True)\n",
        "\n",
        "#Merge with religion data\n",
        "b=c.merge(df_all_religions,on='FIPS',how='outer')\n",
        "a=b.merge(df_country,on='FIPS',how='outer')\n",
        "a.rename(columns={'country_x':'country'},inplace=True)\n",
        "'''\n",
        "a=df_merged.copy(deep=True)\n",
        "\n",
        "#filter dataframe\n",
        "#dftemp=a.loc[a['year']=='overall']\n",
        "dftemp=a.loc[~a['overall'].isnull()]\n",
        "dftemp=dftemp.loc[~dftemp['zb_BeckSettlerMortality'].isnull()]\n",
        "\n",
        "\n",
        "#make bins\n",
        "labels=['lo_25','lomed_25', 'himed_25', 'hi_25']\n",
        "make_bins(df2split=dftemp,\n",
        "          split_into=4,\n",
        "          val_col=['zb_BeckSettlerMortality'],\n",
        "          col_name='zb_BeckSettlerMortality',\n",
        "          labels=labels)\n",
        "\n",
        "\n",
        "'''bins=['bin_zb_BeckSettlerMortality_dist', 'bin_zb_BeckSettlerMortality_jenkspy','bin_zb_BeckSettlerMortality_freq']\n",
        "for curr_bin in bins:\n",
        "    df_sort = dftemp.reindex(dftemp['zb_BeckSettlerMortality'].sort_values(ascending=True).index)\n",
        "    fig = plt.figure(figsize=(20,5))\n",
        "    ax = sns.barplot(x='FIPS', y=\"zb_BeckSettlerMortality\", hue=curr_bin, data=df_sort)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
        "    plt.show()\n",
        "'''\n",
        "use_bin = 'bin_zb_BeckSettlerMortality_freq'\n",
        "\n",
        "#calculate counts \n",
        "temp=[]\n",
        "for level_desc in labels:\n",
        "    for curr_relig in relig_list:\n",
        "        df_a=dftemp.loc[(dftemp[use_bin]==level_desc) & (dftemp['country_relig']==curr_relig)].copy(deep=True)\n",
        "        #df_a=df_a['FIPS'].unique()\n",
        "        if len(df_a)>0:\n",
        "            temp.append({use_bin:level_desc, 'country_relig':curr_relig, 'cnt':df_a['FIPS'].nunique()})\n",
        "\n",
        "dftemp2 = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "fd_list=pd.Categorical(['Financial Development Index', \n",
        "         'Financial Institutions Index', 'Financial Institutions Depth Index',\n",
        "         'Financial Institutions Access Index', 'Financial Institutions Efficiency Index',\n",
        "         'Financial Markets Index','Financial Markets Depth Index',\n",
        "         'Financial Markets Access Index','Financial Markets Efficiency Index'], ordered=True)\n",
        "\n",
        "dftemp.reset_index(drop=True)\n",
        "\n",
        "dftemp=dftemp.merge(dftemp2,on=[use_bin,'country_relig'],how='outer')\n",
        "dftemp=dftemp.loc[dftemp['desc'].isin(fd_list)]\n",
        "dftemp.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "piv=np.round(\n",
        "             pd.pivot_table(data=dftemp, index=[use_bin,'country_relig','cnt'], \n",
        "                            columns=['desc'], values=['value'], margins=True, sort=False\n",
        "                            ), 3\n",
        "             )\n",
        "\n",
        "print('here are the ALL values:\\n',piv)\n",
        "#piv.plot(kind='bar')\n",
        "#plt.show()\n",
        "\n",
        "#sort columns in pivot_table\n",
        "#     source: https://towardsdatascience.com/how-to-use-multiindex-in-pandas-to-level-up-your-analysis-aeac7f451fce\n",
        "#print('indexes\\n',piv.index.to_list)\n",
        "#print('cols\\n',piv.columns)\n",
        "\n",
        "\n",
        "order = [('value', 'Financial Development Index'), \n",
        "         ('value', 'Financial Institutions Index'), \n",
        "         ('value', 'Financial Institutions Depth Index'), \n",
        "         ('value', 'Financial Institutions Access Index'), \n",
        "         ('value', 'Financial Institutions Efficiency Index'),\n",
        "         ('value', 'Financial Markets Index'),\n",
        "         ('value', 'Financial Markets Depth Index'), \n",
        "         ('value', 'Financial Markets Access Index'), \n",
        "         ('value', 'Financial Markets Efficiency Index')\n",
        "        ]\n",
        "\n",
        "\n",
        "piv = piv.reindex(order, axis=1)\n",
        "\n",
        "\n",
        "print ('\\nTable_x_relig_mortality:\\n')\n",
        "print(piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_x_relig_mortality.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "#compare_means(dftemp, 'other')\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa_dWl5YVo0K"
      },
      "source": [
        "##Table wgi_avg (average 1980-2020)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdlYx8qUkaUz"
      },
      "outputs": [],
      "source": [
        "#calculate the average for 6 wgi measures\n",
        "\n",
        "##########\n",
        "# lists of columns for this report\n",
        "##########\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'wgi_avg'},inplace=True)\n",
        "\n",
        "'''\n",
        "#print(df_t2['wgi_avg'].head(20))\n",
        "try:\n",
        "    df_merged.drop(columns={'wgi_avg'},inplace=True)\n",
        "except: \n",
        "    pass\n",
        "df_merged=df_merged.join(df_t2['wgi_avg'],on='FIPS', how='outer')\n",
        "\n",
        "#print ('@@@@@\\n',df_merged['religiosity'].describe(),c['religiosity'].describe())\n",
        "'''\n",
        "#make bins\n",
        "labels=['a_lo_25','b_lomed_25', 'c_himed_25', 'd_hi_25']\n",
        "make_bins(df2split=df_t2,\n",
        "          split_into=4,\n",
        "          val_col=['wgi_avg'],\n",
        "          col_name='wgi_avg',\n",
        "          labels=labels)\n",
        "\n",
        "bins=['bin_wgi_avg_jenkspy','bin_wgi_avg_freq', 'bin_wgi_avg_dist']\n",
        "use_bin='bin_wgi_avg_freq'\n",
        "\n",
        "df_relig_wgi=df_t2.groupby([use_bin,'country_relig']).agg({'wgi_avg':'mean', 'FIPS':pd.Series.nunique},margins=True)\n",
        "\n",
        "'''\n",
        "print('\\nSeparation into WGI_avg groups: (df_relig_wgi).',\n",
        "      '\\nShowing wgi_avg (which is not reported). ',\n",
        "      '\\nMight lose some count when FD values are N/A\\n', df_relig_wgi)\n",
        "'''\n",
        "\n",
        "# add additional columns, limit the desc values to only the FD list\n",
        "df_r0 = df_tmp.loc[df_tmp['desc'].isin(fd_list)].copy(deep=True)\n",
        "df_r1=df_t2.merge(df_r0,on='FIPS',how='outer')\n",
        "\n",
        "#rename desc values (to allow pivot_table to correctly sort). Will search/replace \"a_\", \"b_\" with nothing in Excel\n",
        "df_r1.rename(columns={'country_relig_x':'country_relig'},inplace=True)\n",
        "relig_replace_lst={'CIAProtAll':'a_Prot', 'CIACathAll':'b_Cath', 'CIAOrthAll':'c_Orth', \n",
        "                      'CIAChrAll':'d_Chr', 'CIAMusAll':'e_Mus', 'CIABudAll':'f_Bud', \n",
        "                      'CIAHindAll':'g_Hind', 'CIABudHindAll':'h_BudHind',\n",
        "                      'CIAJewAll':'i_Jew', 'CIAOthNoJew':'j_Oth-NoJew', \n",
        "                      'CIAOthWithJew':'k_Oth-Jew', 'CIABelieverAll':'l_Believers',\n",
        "                      'CIANoRelAll':'m_No-Rel'}\n",
        "df_r1.replace({'country_relig':relig_replace_lst},inplace=True)\n",
        "\n",
        "fd_replace_lst= {'Financial Development Index':'a_FD',\n",
        "               'Financial Institutions Index':'b_FI',\n",
        "               'Financial Institutions Depth Index':'c_FID',\n",
        "               'Financial Institutions Access Index':'d_FIA',\n",
        "               'Financial Institutions Efficiency Index':'e_FIE',\n",
        "               'Financial Markets Index':'f_FM',\n",
        "               'Financial Markets Depth Index':'g_FMD',\n",
        "               'Financial Markets Access Index':'h_FMA',\n",
        "               'Financial Markets Efficiency Index':'i_FME'}\n",
        "df_r1.replace({'desc':fd_replace_lst},inplace=True)\n",
        "\n",
        "print('\\ndf_r1.head\\n',df_r1.head(3))\n",
        "#df_t2 has level, relig, fips\n",
        "#df_tmp has fips, fd\n",
        "\n",
        "df_r1.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "#create one pivot table for 'overall' for each FD; another for 'FIPS' nunique() for bin:country_relig index\n",
        "wgi_cnt=pivot_table_w_subtotals(df_r1, indices=['bin_wgi_avg_freq','country_relig'], columns=['desc'], values=['FIPS'], aggfunc=[pd.Series.nunique], fill_value='')\n",
        "print('\\nwgi_cnt\\n',wgi_cnt)\n",
        "\n",
        "wgi_avg=pivot_table_w_subtotals(df_r1, indices=['bin_wgi_avg_freq','country_relig'], columns=['desc'], values=['value'], aggfunc=[np.mean], fill_value='')\n",
        "\n",
        "print('\\nwgi_avg: FD avg\\n',wgi_avg)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_wgi_avg_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "wgi_avg.to_excel(joined_path)\n",
        "\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_wgi_avg_counts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "wgi_cnt.to_excel(joined_path)\n",
        "\n",
        "#compare_means(df_r1, 'other')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Religiosity as NoRel%"
      ],
      "metadata": {
        "id": "RAW75QOl_EPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#'CIANoRel%'\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "col_order=[('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')\n",
        "          ]\n",
        "\n",
        "desc_col_lst =['Financial Development Index',\n",
        "               'Financial Institutions Index',\n",
        "               'Financial Institutions Depth Index',\n",
        "               'Financial Institutions Access Index',\n",
        "               'Financial Institutions Efficiency Index',\n",
        "               'Financial Markets Index',\n",
        "               'Financial Markets Depth Index',\n",
        "               'Financial Markets Access Index',\n",
        "               'Financial Markets Efficiency Index']\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                'country',\n",
        "                'country_relig'\n",
        "                ]\n",
        "\n",
        "#Assign the level of latitude\n",
        "\n",
        "df_t1=df_tmp.copy(deep=True)\n",
        "df_t2=df_t1[['FIPS','country_relig', 'desc','overall','CIANoRel%']]\n",
        "df_t2=df_t2.loc[df_t2['desc'].isin(fd_list)]\n",
        "df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "\n",
        "\n",
        "df_t2.rename(columns={'CIANoRel%':'CIANoRel_per'},inplace=True)\n",
        "df_t2['CIANoRel_per'].fillna(0)\n",
        "df_t2['CIANoRel_per']=pd.to_numeric(df_t2['CIANoRel_per'], errors='coerce')\n",
        "#make bins\n",
        "labels=['lo_25','lomed_25', 'himed_25', 'hi_25']\n",
        "\n",
        "make_bins(df2split=df_t2,\n",
        "          split_into=4,\n",
        "          val_col=['CIANoRel_per'],\n",
        "          col_name='CIANoRel_per',\n",
        "          labels=labels)\n",
        "\n",
        "bins=['bin_CIANoRel_per_dist','bin_CIANoRel_per_jenkspy','bin_CIANoRel_per_freq']\n",
        "\n",
        "df_NoRel_avg=df_t2.copy(deep=True)\n",
        "#counts\n",
        "use_bin='bin_CIANoRel_per_freq'\n",
        "\n",
        "print(len(df_t2.loc[df_t2['bin_CIANoRel_per_freq']=='lo_25']),len(df_t2.loc[df_t2['bin_CIANoRel_per_freq']=='hi_25']))\n",
        "\n",
        "#create one pivot table for 'overall' for each FD; another for 'FIPS' nunique() for bin:country_relig index\n",
        "CIANoRel_per_cnt=pivot_table_w_subtotals(df_t2, indices=['bin_CIANoRel_per_freq','country_relig'], columns=['desc'], values=['FIPS'], aggfunc=[pd.Series.nunique], fill_value='')\n",
        "print('\\nCIANoRel%_cnt\\n',CIANoRel_per_cnt)\n",
        "\n",
        "CIANoRel_per_avg=pivot_table_w_subtotals(df_t2, indices=['bin_CIANoRel_per_freq','country_relig'], columns=['desc'], values=['overall'], aggfunc=[np.mean], fill_value='')\n",
        "\n",
        "print('\\nbin_CIANoRel%_freqavg: FD avg\\n',CIANoRel_per_avg)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_CIANoRel_percent_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "#compare_means(df_tmp, 'other')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GQoP8G1sUeug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GDP groups"
      ],
      "metadata": {
        "id": "C2bHXCmzUNdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## not currently working\n",
        "\n",
        "#calculate the average for NoRel% over time\n",
        "\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc']=='GDP')].copy(deep=True)\n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'GDP_avg'},inplace=True)\n",
        "\n",
        "#make bins\n",
        "labels=['a_lo_25','b_lomed_25', 'c_himed_25', 'd_hi_25']\n",
        "make_bins(df2split=df_t2,\n",
        "          split_into=4,\n",
        "          val_col=['GDP_avg'],\n",
        "          col_name='GDP_avg',\n",
        "          labels=labels)\n",
        "\n",
        "bins=['bin_GDP_avg_jenkspy','bin_GDP_avg_freq', 'bin_GDP_avg_dist']\n",
        "use_bin='bin_GDP_avg_freq'\n",
        "\n",
        "df_relig_gdp=df_t2.groupby([use_bin,'country_relig']).agg({'GDP_avg':'mean', 'FIPS':pd.Series.nunique},margins=True)\n",
        "print('df_t2\\n',df_t2.head(),'\\ndf_relig_gdp\\n',df_relig_gdp)\n",
        "\n",
        "# add additional columns, limit the desc values to only the FD list\n",
        "df_r0 = df_tmp.loc[df_tmp['desc'].isin(fd_list)].copy(deep=True)\n",
        "df_r1=df_t2.append(df_r0)\n",
        "\n",
        "#df_r1=df_t2.merge(df_r0[['FIPS','desc','overall']],on='FIPS',how='outer')\n",
        "\n",
        "#rename desc values (to allow pivot_table to correctly sort). Will search/replace \"a_\", \"b_\" with nothing in Excel\n",
        "df_r1.rename(columns={'country_relig_x':'country_relig'},inplace=True)\n",
        "relig_replace_lst={'CIAProtAll':'a_Prot', 'CIACathAll':'b_Cath', 'CIAOrthAll':'c_Orth', \n",
        "                      'CIAChrAll':'d_Chr', 'CIAMusAll':'e_Mus', 'CIABudAll':'f_Bud', \n",
        "                      'CIAHindAll':'g_Hind', 'CIABudHindAll':'h_BudHind',\n",
        "                      'CIAJewAll':'i_Jew', 'CIAOthNoJew':'j_Oth-NoJew', \n",
        "                      'CIAOthWithJew':'k_Oth-Jew', 'CIABelieverAll':'l_Believers',\n",
        "                      'CIANoRelAll':'m_No-Rel'}\n",
        "df_r1.replace({'country_relig':relig_replace_lst},inplace=True)\n",
        "\n",
        "fd_replace_lst= {'Financial Development Index':'a_FD',\n",
        "               'Financial Institutions Index':'b_FI',\n",
        "               'Financial Institutions Depth Index':'c_FID',\n",
        "               'Financial Institutions Access Index':'d_FIA',\n",
        "               'Financial Institutions Efficiency Index':'e_FIE',\n",
        "               'Financial Markets Index':'f_FM',\n",
        "               'Financial Markets Depth Index':'g_FMD',\n",
        "               'Financial Markets Access Index':'h_FMA',\n",
        "               'Financial Markets Efficiency Index':'i_FME'}\n",
        "df_r1.replace({'desc':fd_replace_lst},inplace=True)\n",
        "\n",
        "print('\\ndf_r1.head\\n',df_r1.head(3),'overall:\\n',df_r1['overall'].unique())\n",
        "#df_t2 has level, relig, fips\n",
        "#df_tmp has fips, fd\n",
        "\n",
        "df_r1.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#create one pivot table for 'overall' for each FD; another for 'FIPS' nunique() for bin:country_relig index\n",
        "GDP_cnt=pivot_table_w_subtotals(df_r1, indices=['bin_GDP_avg_freq','country_relig'], columns=['desc'], values=['FIPS'], aggfunc=[pd.Series.nunique], fill_value='')\n",
        "print('\\nwgi_cnt\\n',wgi_cnt)\n",
        "\n",
        "GDP_avg=pivot_table_w_subtotals(df_r1, indices=['bin_GDP_avg_freq','country_relig'], columns=['desc'], values=['value'], aggfunc=[np.mean], fill_value='')\n",
        "\n",
        "print('\\nGDP_avg: FD avg\\n',GDP_avg)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_GDP_avg_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "GDP_avg.to_excel(joined_path)\n",
        "\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_GDP_avg_counts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "GDP_cnt.to_excel(joined_path)\n",
        "\n",
        "#compare_means(df_r1, 'other')\n",
        "\n"
      ],
      "metadata": {
        "id": "DXF4zGWr_Ain"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(df_merged.loc[df_merged['wgi_avg']>1.6],'\\n\\n',df_t2.loc[df_t2['wgi_avg']>1.6])"
      ],
      "metadata": {
        "id": "Mu2DYN6yxdno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB9ne1sGkQTq"
      },
      "source": [
        "###OLD -not used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yORCkiQVtim"
      },
      "outputs": [],
      "source": [
        "'''df_tmp=df_merged.copy(deep=True)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "df_tmp=df_tmp.reset_index(drop=False)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "#---------------------------\n",
        "# clean-up column names --remove spaces and special characters\n",
        "#---------------------------\n",
        "avg_col_lst = [curr_col.strip() for curr_col in avg_col_lst] #remove leading & trailing spaces\n",
        "\n",
        "new_strings = []\n",
        "for string in avg_col_lst:\n",
        "    new_string = string.replace(\" \", \"_\") \n",
        "    new_string = new_string.replace(\":\", \"\") \n",
        "    new_string = new_string.replace(\"/\", \"_and_\") \n",
        "    new_strings.append(new_string) \n",
        "new_avg_col_lst = new_strings\n",
        "\n",
        "#Convert to set to eliminate redundant pairs\n",
        "\n",
        "combo=new_avg_col_lst+avg_col_lst\n",
        "perm_set=list( permutations( combo, 2 ) )\n",
        "perm_set=set(perm_set)\n",
        "df_tmp['desc']\n",
        "\n",
        "rename_lst = {'Voice and Accountability: Estimate':'Voice_and_Accountability_Estimate',\n",
        "            'Government Effectiveness: Estimate': 'Government_Effectiveness_Estimate', \n",
        "            'Regulatory Quality: Estimate': 'Regulatory_Quality_Estimate', \n",
        "            'Control of Corruption: Estimate': 'Control_of_Corruption_Estimate', \n",
        "            'Rule of Law: Estimate': 'Rule_of_Law_Estimate', \n",
        "            'Political Stability and Absence of Violence/Terrorism: Estimate': 'Political_Stability_and_Absence_of_Violence_and_Terrorism_Estimate'}\n",
        " \n",
        "for k, v in rename_lst.items():\n",
        "    df_tmp['desc']=df_tmp['desc'].replace(k,v)\n",
        "\n",
        "print(df_tmp['desc'].unique())\n",
        "\n",
        "#slice df to only keep desired metrics\n",
        "\n",
        "print (avg_col_lst, df_tmp.info(verbose=True))\n",
        "\n",
        "desc_col_lst =['Financial Development Index',\n",
        "               'Financial Institutions Index',\n",
        "               'Financial Institutions Depth Index',\n",
        "               'Financial Institutions Access Index',\n",
        "               'Financial Institutions Efficiency Index',\n",
        "               'Financial Markets Index',\n",
        "               'Financial Markets Depth Index',\n",
        "               'Financial Markets Access Index',\n",
        "               'Financial Markets Efficiency Index']\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                'country',\n",
        "                'country_relig'\n",
        "                ]\n",
        "\n",
        "temp=[]\n",
        "for curr_relig in relig_list:\n",
        "    df_a=df_tmp.loc[(df_tmp['country_relig']==curr_relig)]\n",
        "    for curr_col in desc_col_lst:\n",
        "            df_col=df_a.loc[(df_a['desc']==curr_col)]\n",
        "            \n",
        "            #calculate the average for 6 wgi measures\n",
        "            wgi_avg=df_col[new_avg_col_lst].mean(axis=1)\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                        'desc':curr_col,\n",
        "                        'value':df_col['overall'].mean(),\n",
        "                        'wgi_avg':wgi_avg})\n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "print(dfloop.head(10))\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['country_relig','desc'], columns=['desc'], \n",
        "    values=['value'], sort=False),3)\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "row_order =['HDI Score', \n",
        "         'UN Income Index',\n",
        "         'Gini index (World Bank estimate)', \n",
        "         'Multidimensional Poverty Index',\n",
        "         'UN Life Expectancy Index', \n",
        "         'Life expectancy at birth, total (years)',\n",
        "         'UIS: Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes',\n",
        "         'Prison per 100K', \n",
        "         'sfi',\n",
        "         'actotal', \n",
        "         'autoc']\n",
        "\n",
        "piv = piv.reindex(row_order, axis=0)\n",
        "\n",
        "#Sort Col order\n",
        "#print(piv.columns.to_list)\n",
        "\n",
        "order = [   ('value',    'CIAProtAll'),\n",
        "            ('value',    'CIACathAll'),\n",
        "            ('value',    'CIAOrthAll'),\n",
        "            ('value',     'CIAChrAll'),\n",
        "            ('value',     'CIAMusAll'),\n",
        "            ('value',     'CIABudAll'),\n",
        "            ('value',    'CIAHindAll'),\n",
        "            ('value', 'CIABudHindAll'),\n",
        "            ('value',   'CIANoRelAll'),\n",
        "            ('value',     'CIAJewAll'),\n",
        "            ('value',   'CIAOthNoJew'),\n",
        "            ('value', 'CIAOthWithJew')\n",
        "        ]        \n",
        "#piv = piv.reindex(order, axis=1)\n",
        "\n",
        "#print ('piv',piv)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BfWh5FHhOdp"
      },
      "source": [
        "##Table Religiosity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGRrs4Yfg_9i"
      },
      "outputs": [],
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "desc_col_lst =['Financial Development Index',\n",
        "               'Financial Institutions Index',\n",
        "               'Financial Institutions Depth Index',\n",
        "               'Financial Institutions Access Index',\n",
        "               'Financial Institutions Efficiency Index',\n",
        "               'Financial Markets Index',\n",
        "               'Financial Markets Depth Index',\n",
        "               'Financial Markets Access Index',\n",
        "               'Financial Markets Efficiency Index']\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                'country',\n",
        "                'country_relig'\n",
        "                ]\n",
        "\n",
        "col_avg=['Bel_Afterlife_1_yes', \n",
        "         'Bel_God_1_yes',\n",
        "         'Bel_heaven_1_yes',\n",
        "         'Bel_hell_1_yes',\n",
        "         'God_importance_High',\n",
        "         'Relig_attend_lo',\n",
        "         'Relig_person_lo']\n",
        "\n",
        "keep_cols=[('overall',    'Bel_Afterlife_1_yes'), \n",
        "         ('overall',    'Bel_God_1_yes'),\n",
        "         ('overall',    'Bel_heaven_1_yes'),\n",
        "         ('overall',    'Bel_hell_1_yes'),\n",
        "         ('overall',    'God_importance_High'),\n",
        "         ('overall',    'Relig_attend_lo'),\n",
        "         ('overall',    'Relig_person_lo')]\n",
        "\n",
        "#Get the religiosity value alone, normalize & transform scales, then calc mean\n",
        "df_tmp0=np.round(df_tmp.pivot_table(index=['FIPS', 'country', 'country_relig'], columns=['desc'],values=['overall']),3)\n",
        "\n",
        "#Drop multilevel index and reset index to see FIPS\n",
        "df_tmp0.columns = df_tmp0.columns.droplevel(0) \n",
        "df_tmp0.reset_index(inplace=True, drop=False)\n",
        "\n",
        "df_tmp0.drop(df_tmp0.columns.difference(['FIPS', 'country', 'country_relig',\n",
        "                                        'Bel_Afterlife_1_yes', \n",
        "                                        'Bel_God_1_yes',\n",
        "                                        'Bel_heaven_1_yes',\n",
        "                                        'Bel_hell_1_yes',\n",
        "                                        'God_importance_High',\n",
        "                                        'Relig_attend_lo',\n",
        "                                        'Relig_person_lo']), axis=1,inplace=True)\n",
        "\n",
        "#reverse scale for low-scored values\n",
        "df_tmp0['Relig_attend_lo']=df_tmp0['Relig_attend_lo'].apply(lambda x: 1/x if x>0 else 1/1000)\n",
        "df_tmp0['Relig_person_lo']=df_tmp0['Relig_person_lo'].apply(lambda x: 1/x if x>0 else 1/1000)\n",
        "\n",
        "#print('after reverse scale\\n',df_tmp0.columns.to_list())\n",
        "#normalize col_avg values\n",
        "for curr_col in col_avg:\n",
        "    df_tmp0[curr_col] = (df_tmp0[curr_col] - df_tmp0[curr_col].min()) / (df_tmp0[curr_col].max() - df_tmp0[curr_col].min())    \n",
        "\n",
        "\n",
        "print ('\\ndf_tmp0.columns.to_list():\\n',df_tmp0.columns.to_list())\n",
        "df_tmp0['religiosity']=df_tmp0.mean(numeric_only=True, axis=1)\n",
        "df_tmp0['religiosity'].replace(0, np.nan, inplace=True)\n",
        "#df_merged['religiosity']=df_tmp0['religiosity']\n",
        "\n",
        "print(df_tmp0.loc[df_tmp0['religiosity']>.6])\n",
        "\n",
        "#make long\n",
        "c=pd.melt(df_tmp0,id_vars=['FIPS','country', 'country_relig','religiosity'])\n",
        "c.drop(columns='desc',inplace=True)                             \n",
        "c.drop_duplicates(inplace=True, ignore_index=True)\n",
        "#print(c.head(100))\n",
        "\n",
        "#save 'religiosity' column to df_merged \n",
        "c.set_index('FIPS', inplace=True)\n",
        "try:\n",
        "    df_merged.drop(columns={'religiosity'},inplace=True)\n",
        "except:\n",
        "    pass\n",
        "df_merged=df_merged.join(c['religiosity'],how='outer')\n",
        "print ('@@@@@\\n',df_merged['religiosity'].describe(),c['religiosity'].describe())\n",
        "\n",
        "#make bins\n",
        "labels=['lo_25','lomed_25', 'himed_25', 'hi_25']\n",
        "\n",
        "make_bins(df2split=c,\n",
        "          split_into=4,\n",
        "          val_col=['religiosity'],\n",
        "          col_name='religiosity',\n",
        "          labels=labels)\n",
        "\n",
        "bins=['bin_religiosity_jenkspy','bin_religiosity_freq', 'bin_religiosity_dist']\n",
        "'''\n",
        "for curr_bin in bins:\n",
        "    c=c.loc[c['religiosity']>0]\n",
        "    df_sort1 = c.reindex(c['religiosity'].sort_values(ascending=True).index)\n",
        "    fig1 = plt.figure(figsize=(20,8))\n",
        "    ax1 = sns.barplot(x='FIPS', y=\"religiosity\", hue=curr_bin, data=df_sort1)\n",
        "    #ax1.set_xticklabels(ax1.get_xticklabels(),rotation = 90)\n",
        "    plt.show()\n",
        "'''\n",
        "df_t2=df_tmp.merge(c,on=['FIPS','country','country_relig'],how='outer')\n",
        "\n",
        "df_religiosity=df_t2.copy(deep=True)\n",
        "\n",
        "\n",
        "#counts\n",
        "use_bin='bin_religiosity_freq'\n",
        "\n",
        "# by \"LEVEL\", by religion, by FD (column)\n",
        "temp=[]\n",
        "for curr_level in labels:\n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in desc_col_lst:\n",
        "            df_col=df_t2.loc[(df_t2['bin_religiosity_freq']==curr_level)&(df_t2['country_relig']==curr_relig)].copy(deep=True)\n",
        "            df_col1=df_col.loc[(df_col['desc']==curr_col)]\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                        'cnt':df_col['FIPS'].nunique(),\n",
        "                        'desc':curr_col,\n",
        "                        'bin_religiosity_freq':curr_level,\n",
        "                        'value':df_col1['overall'].mean()})\n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['bin_religiosity_freq','country_relig','cnt'], columns=['desc'], \n",
        "    values=['value'], sort=False),3)\n",
        "\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "\n",
        "#piv = piv.reindex(row_order, axis=0)\n",
        "\n",
        "#Sort Col order\n",
        "#print(piv.columns.to_list)\n",
        "\n",
        "     \n",
        "col_order=[('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')\n",
        "          ]\n",
        "piv = piv.reindex(col_order, axis=1)\n",
        "\n",
        "piv.sort_index(axis='rows', level='country_relig')\n",
        "print ('piv',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_religiosity_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "#compare_means(df_tmp, 'other')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_tmp0.set_index('FIPS')\n",
        "#df_merged.set_index('FIPS')\n",
        "\n",
        "#Sort Row order\n",
        "print(df_tmp0.index.to_list)\n",
        "\n",
        "#Sort Col order\n",
        "print(df_tmp0.columns.to_list)"
      ],
      "metadata": {
        "id": "rCxoXwIsoiIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "172r5AVQYfK6"
      },
      "source": [
        "##Table latitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAeXM-MVYjvy"
      },
      "outputs": [],
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "col_order=[('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')\n",
        "          ]\n",
        "\n",
        "desc_col_lst =['Financial Development Index',\n",
        "               'Financial Institutions Index',\n",
        "               'Financial Institutions Depth Index',\n",
        "               'Financial Institutions Access Index',\n",
        "               'Financial Institutions Efficiency Index',\n",
        "               'Financial Markets Index',\n",
        "               'Financial Markets Depth Index',\n",
        "               'Financial Markets Access Index',\n",
        "               'Financial Markets Efficiency Index']\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                'country',\n",
        "                'country_relig'\n",
        "                ]\n",
        "\n",
        "#Assign the level of latitude\n",
        "\n",
        "df_t1=df_tmp.copy(deep=True)\n",
        "df_t2=df_t1[['FIPS','country_relig', 'desc','overall','AbsLatitude']]\n",
        "df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "\n",
        "#make bins\n",
        "labels=['lo_25','lomed_25', 'himed_25', 'hi_25']\n",
        "\n",
        "make_bins(df2split=df_t2,\n",
        "          split_into=4,\n",
        "          val_col=['AbsLatitude'],\n",
        "          col_name='AbsLatitude',\n",
        "          labels=labels)\n",
        "\n",
        "bins=['bin_AbsLatitude_jenkspy','bin_AbsLatitude_freq', 'bin_AbsLatitude_dist']\n",
        "'''\n",
        "for curr_bin in bins:\n",
        "    df_t2=df_t2.loc[df_t2['AbsLatitude']>0]\n",
        "    df_sort = df_t2.reindex(df_t2['AbsLatitude'].sort_values(ascending=True).index)\n",
        "    fig = plt.figure(figsize=(40,8))\n",
        "    ax = sns.barplot(x='FIPS', y=\"AbsLatitude\", hue=curr_bin, data=df_sort)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
        "    plt.show()\n",
        "'''\n",
        "#counts\n",
        "use_bin='bin_AbsLatitude_freq'\n",
        "\n",
        "# by lat_level, by religion, by FD (column)\n",
        "temp=[]\n",
        "for curr_level in labels:\n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in desc_col_lst:\n",
        "            df_col=df_t2.loc[(df_t2['bin_AbsLatitude_freq']==curr_level)&(df_t2['country_relig']==curr_relig)].copy(deep=True)\n",
        "            df_col1=df_col.loc[(df_col['desc']==curr_col)]\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                        'cnt':df_col['FIPS'].nunique(),\n",
        "                        'desc':curr_col,\n",
        "                        'bin_AbsLatitude_freq':curr_level,\n",
        "                        'value':df_col1['overall'].mean()})\n",
        "            \n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "dan=pivot_table_w_subtotals(dfloop, values=['value'], indices=['bin_AbsLatitude_freq','country_relig','cnt'], columns=['desc'], aggfunc=[np.mean], fill_value='')\n",
        "print('\\ndan\\n',dan)\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['bin_AbsLatitude_freq','country_relig','cnt'], columns=['desc'], \n",
        "    values=['value'], sort=False),3)\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "\n",
        "\n",
        "#piv = piv.reindex(row_order, axis=0)\n",
        "\n",
        "#Sort Col order\n",
        "#print(piv.columns.to_list)\n",
        "\n",
        "piv = piv.reindex(col_order, axis=1)\n",
        "print('piv:\\n',piv.head(50))\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_latitude_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "#compare_means(df_tmp, 'other')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqxJvca1mA2T"
      },
      "source": [
        "##Table Population size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IwvthP8mEUk"
      },
      "outputs": [],
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "df_tmp=df_tmp.reset_index(drop=False)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "desc_col_lst =['Financial Development Index',\n",
        "               'Financial Institutions Index',\n",
        "               'Financial Institutions Depth Index',\n",
        "               'Financial Institutions Access Index',\n",
        "               'Financial Institutions Efficiency Index',\n",
        "               'Financial Markets Index',\n",
        "               'Financial Markets Depth Index',\n",
        "               'Financial Markets Access Index',\n",
        "               'Financial Markets Efficiency Index']\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                'country',\n",
        "                'country_relig'\n",
        "                ]\n",
        "\n",
        "#Assign the level of latitude\n",
        "\n",
        "df_t1=df_tmp.copy(deep=True)\n",
        "df_t2=df_t1[['FIPS','country_relig', 'desc','overall','ze_Population (millions)']]\n",
        "df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_t2.rename(columns={'ze_Population (millions)':'Population_millions'},inplace=True)\n",
        "\n",
        "#make bins\n",
        "labels=['lo_25','lomed_25', 'himed_25', 'hi_25']\n",
        "\n",
        "make_bins(df2split=df_t2,\n",
        "          split_into=4,\n",
        "          val_col=['Population_millions'],\n",
        "          col_name='Population_millions',\n",
        "          labels=labels)\n",
        "\n",
        "bins=['bin_Population_millions_jenkspy','bin_Population_millions_freq', 'bin_Population_millions_dist']\n",
        "\n",
        "# Calc FD metric averages for each grp\n",
        "temp=[]\n",
        "for curr_grp in labels: \n",
        "    df_a=df_t2.loc[df_t2['bin_Population_millions_freq']==curr_grp]\n",
        "    level_cnt=df_a['FIPS'].nunique()\n",
        "    for curr_col in desc_col_lst:\n",
        "        df_a=df_t2.loc[df_t2['desc']==curr_col]\n",
        "        df_a['overall']=pd.to_numeric(df_a['overall'], errors ='coerce')\n",
        "        df_a['overall'] = df_a['overall'].astype(float, errors = 'raise')\n",
        "        temp.append({'bin_Population_millions_freq':curr_grp,'desc':curr_col,'avg':df_a['overall'].mean(),'cnt':df_a['FIPS'].nunique(), 'lev_cnt':level_cnt})\n",
        "dfgroups = pd.DataFrame.from_dict(temp) \n",
        "print('Averages for each inc_grp:\\n', dfgroups)\n",
        "\n",
        "#counts\n",
        "use_bin='bin_Population_millions_freq'\n",
        "\n",
        "# by lat_level, by religion, by FD (column)\n",
        "temp=[]\n",
        "for curr_level in labels:\n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in desc_col_lst:\n",
        "            df_col=df_t2.loc[(df_t2['bin_Population_millions_freq']==curr_level)&(df_t2['country_relig']==curr_relig)].copy(deep=True)\n",
        "            df_col1=df_col.loc[(df_col['desc']==curr_col)]\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                        'cnt':df_col['FIPS'].nunique(),\n",
        "                        'desc':curr_col,\n",
        "                        'bin_Population_millions_freq':curr_level,\n",
        "                        'value':df_col1['overall'].mean()})\n",
        "            \n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['bin_Population_millions_freq','country_relig','cnt'], columns=['desc'], \n",
        "    values=['value'], sort=False),3)\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "\n",
        "row_order =['HDI', \n",
        "            'GNI per capita', \n",
        "            'Gini 2010-18', \n",
        "            'Population (millions)', \n",
        "            'Inequality in income %', \n",
        "            'Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)',\n",
        "            'Poverty gap at $3.20 a day (2011 PPP) (%)',\n",
        "            'Poverty headcount ratio at $5.50 a day (2011 PPP) (% of population)',\n",
        "            'Multidimensional Poverty Index',\n",
        "            'Life expectancy at birth', \n",
        "            'Expected years of schooling', \n",
        "            'Prison population (per 100K) 2013-2018', \n",
        "            'sfi',\n",
        "            'actotal', \n",
        "            'autoc']\n",
        "\n",
        "#piv = piv.reindex(row_order, axis=0)\n",
        "\n",
        "#Sort Col order\n",
        "#print(piv.columns.to_list)\n",
        "\n",
        "order = [   ('value',    'CIAProtAll'),\n",
        "            ('value',    'CIACathAll'),\n",
        "            ('value',    'CIAOrthAll'),\n",
        "            ('value',     'CIAChrAll'),\n",
        "            ('value',     'CIAMusAll'),\n",
        "            ('value',     'CIABudAll'),\n",
        "            ('value',    'CIAHindAll'),\n",
        "            ('value', 'CIABudHindAll'),\n",
        "            ('value',     'CIAJewAll'),\n",
        "            ('value',   'CIAOthNoJew'),\n",
        "            ('value', 'CIAOthWithJew'),\n",
        "            ('value',   'CIABeliever'),  \n",
        "            ('value',   'CIANoRelAll')\n",
        "        ]        \n",
        "\n",
        "col_order=[('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')\n",
        "          ]\n",
        "piv = piv.reindex(col_order, axis=1)\n",
        "print('piv:\\n',piv.head(50))\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_Populations_size_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "myfile = 'nTable_Populations_group_means.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "dfgroups.to_excel(joined_path)\n",
        "\n",
        "\n",
        "#compare_means(df_tmp, 'other')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGBZIvpY3LkC"
      },
      "source": [
        "##Table Continents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg_s6vz73PI9"
      },
      "outputs": [],
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "df_tmp=df_tmp.reset_index(drop=False)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "desc_col_lst =['Financial Development Index',\n",
        "               'Financial Institutions Index',\n",
        "               'Financial Institutions Depth Index',\n",
        "               'Financial Institutions Access Index',\n",
        "               'Financial Institutions Efficiency Index',\n",
        "               'Financial Markets Index',\n",
        "               'Financial Markets Depth Index',\n",
        "               'Financial Markets Access Index',\n",
        "               'Financial Markets Efficiency Index']\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                'country',\n",
        "                'country_relig'\n",
        "                ]\n",
        "\n",
        "df_t2=df_tmp[['FIPS','country','country_relig','desc','overall','continent']].copy(deep=True)\n",
        "df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "\n",
        "temp=[]\n",
        "for curr_continent in df_t2['continent'].unique():\n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in desc_col_lst:\n",
        "            df_col=df_t2.loc[(df_t2['continent']==curr_continent)&(df_t2['country_relig']==curr_relig)].copy(deep=True)\n",
        "            df_col1=df_col.loc[(df_col['desc']==curr_col)]\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                         'cnt':df_col['FIPS'].nunique(),\n",
        "                         'desc':curr_col,\n",
        "                         'continent':curr_continent,\n",
        "                         'value':df_col1['overall'].mean()})\n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['continent','country_relig','cnt'], columns=['desc'], \n",
        "    values=['value'], sort=False),3)\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "row_order =['HDI Score', \n",
        "         'UN Income Index',\n",
        "         'Gini index (World Bank estimate)', \n",
        "         'Multidimensional Poverty Index',\n",
        "         'UN Life Expectancy Index', \n",
        "         'Life expectancy at birth, total (years)',\n",
        "         'UIS: Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes',\n",
        "         'Prison per 100K', \n",
        "         'sfi',\n",
        "         'actotal', \n",
        "         'autoc']\n",
        "\n",
        "#piv = piv.reindex(row_order, axis=0)\n",
        "\n",
        "#Sort Col order\n",
        "#print(piv.columns.to_list)\n",
        "\n",
        "order = [   ('value',    'CIAProtAll'),\n",
        "            ('value',    'CIACathAll'),\n",
        "            ('value',    'CIAOrthAll'),\n",
        "            ('value',     'CIAChrAll'),\n",
        "            ('value',     'CIAMusAll'),\n",
        "            ('value',     'CIABudAll'),\n",
        "            ('value',    'CIAHindAll'),\n",
        "            ('value', 'CIABudHindAll'),\n",
        "            ('value',     'CIAJewAll'),\n",
        "            ('value',   'CIAOthNoJew'),\n",
        "            ('value', 'CIAOthWithJew'),\n",
        "            ('value',   'CIABeliever'),  \n",
        "            ('value',   'CIANoRelAll')\n",
        "        ]        \n",
        "\n",
        "col_order=[('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')\n",
        "          ]\n",
        "piv = piv.reindex(col_order, axis=1)\n",
        "\n",
        "print ('piv',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_continent_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "#compare_means(df_tmp, 'other')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuAYxBDp7E5E"
      },
      "source": [
        "##Table Regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxV8s3277IWr"
      },
      "outputs": [],
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "df_tmp=df_tmp.reset_index(drop=False)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "desc_col_lst =['Financial Development Index',\n",
        "               'Financial Institutions Index',\n",
        "               'Financial Institutions Depth Index',\n",
        "               'Financial Institutions Access Index',\n",
        "               'Financial Institutions Efficiency Index',\n",
        "               'Financial Markets Index',\n",
        "               'Financial Markets Depth Index',\n",
        "               'Financial Markets Access Index',\n",
        "               'Financial Markets Efficiency Index']\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                'country',\n",
        "                'country_relig'\n",
        "                ]\n",
        "\n",
        "#Assign the level of: latitude\n",
        "\n",
        "df_t1=df_tmp.copy(deep=True)\n",
        "df_t2=df_tmp[['FIPS','country','country_relig','desc','overall','region']].copy(deep=True)\n",
        "df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "\n",
        "temp=[]\n",
        "for curr_region in df_t2['region'].unique():\n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in desc_col_lst:\n",
        "            df_col=df_t2.loc[(df_t2['region']==curr_region)&(df_t2['country_relig']==curr_relig)].copy(deep=True)\n",
        "            df_col1=df_col.loc[(df_col['desc']==curr_col)]\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                         'cnt':df_col['FIPS'].nunique(),\n",
        "                         'desc':curr_col,\n",
        "                         'region':curr_region,\n",
        "                         'value':df_col1['overall'].mean()})\n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['region','country_relig','cnt'], columns=['desc'], \n",
        "    values=['value'], sort=False),3)\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "row_order =['HDI Score', \n",
        "         'UN Income Index',\n",
        "         'Gini index (World Bank estimate)', \n",
        "         'Multidimensional Poverty Index',\n",
        "         'UN Life Expectancy Index', \n",
        "         'Life expectancy at birth, total (years)',\n",
        "         'UIS: Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes',\n",
        "         'Prison per 100K', \n",
        "         'sfi',\n",
        "         'actotal', \n",
        "         'autoc']\n",
        "\n",
        "#piv = piv.reindex(row_order, axis=0)\n",
        "\n",
        "#Sort Col order\n",
        "#print(piv.columns.to_list)\n",
        "\n",
        "order = [   ('value',    'CIAProtAll'),\n",
        "            ('value',    'CIACathAll'),\n",
        "            ('value',    'CIAOrthAll'),\n",
        "            ('value',     'CIAChrAll'),\n",
        "            ('value',     'CIAMusAll'),\n",
        "            ('value',     'CIABudAll'),\n",
        "            ('value',    'CIAHindAll'),\n",
        "            ('value', 'CIABudHindAll'),\n",
        "            ('value',     'CIAJewAll'),\n",
        "            ('value',   'CIAOthNoJew'),\n",
        "            ('value', 'CIAOthWithJew'),\n",
        "            ('value',   'CIABeliever'),  \n",
        "            ('value',   'CIANoRelAll')\n",
        "        ]        \n",
        "\n",
        "col_order=[('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')\n",
        "          ]\n",
        "piv = piv.reindex(col_order, axis=1)\n",
        "\n",
        "print ('piv',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_region_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "\n",
        "#compare_means(df_tmp, 'other')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndEOWLJRCUyV"
      },
      "source": [
        "##Table Colonized, Colonizer, NeitherCol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDg1ooxoCcys"
      },
      "outputs": [],
      "source": [
        "'''df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "desc_col_lst =['Financial Development Index',\n",
        "               'Financial Institutions Index',\n",
        "               'Financial Institutions Depth Index',\n",
        "               'Financial Institutions Access Index',\n",
        "               'Financial Institutions Efficiency Index',\n",
        "               'Financial Markets Index',\n",
        "               'Financial Markets Depth Index',\n",
        "               'Financial Markets Access Index',\n",
        "               'Financial Markets Efficiency Index']\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                'country',\n",
        "                'country_relig'\n",
        "                ]\n",
        "\n",
        "#Assign the level of: Colonized, Colonizer, NeitherCol\n",
        "\n",
        "df_t1=df_tmp.copy(deep=True)\n",
        "df_t2=df_t1[['FIPS', 'country', 'desc','country_relig','Colonized', 'Colonizer', 'NeitherCol']]\n",
        "df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "\n",
        "select_conditions = [\n",
        "    (df_t2['Colonized'] ==1),\n",
        "    (df_t2['Colonizer'] ==1),\n",
        "    (df_t2['NeitherCol'] ==1)\n",
        "    ]\n",
        "select_values = ['Colonized', 'Colonizer', 'NeitherCol']\n",
        "\n",
        "df_t2['my_level'] = np.select(select_conditions, select_values)\n",
        "\n",
        "df_level=df_t2.groupby(['my_level','country_relig'],as_index=False).agg({'FIPS':'count'})\n",
        "\n",
        "print('Separation into my_level groups:', df_level)\n",
        "\n",
        "# add additional columns\n",
        "df_r0 = df_tmp.loc[df_tmp['desc'].isin(desc_col_lst)].copy(deep=True)\n",
        "df_r1=df_t2.merge(df_r0,on='FIPS',how='outer')\n",
        "\n",
        "df_r1.rename(columns={'country_relig_x':'country_relig','desc_x':'desc', 'overall_x':'overall','Colonized_x':'Colonized', 'Colonizer_x':'Colonizer', 'NeitherCol_x':'NeitherCol'},inplace=True)\n",
        "print('df_r1.head',df_r1.head(10))\n",
        "#df_t2 has level, relig, fips\n",
        "#df_tmp has fips, fd\n",
        "\n",
        "\n",
        "# by lat_level, by religion, by FD (column)\n",
        "temp=[]\n",
        "for curr_level in select_values:\n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in desc_col_lst:\n",
        "            df_col=df_r1.copy(deep=True)\n",
        "            df_col1=df_col.loc[(df_col['my_level']==curr_level)&(df_col['desc']==curr_col)&(df_col['country_relig']==curr_relig)]\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                        'desc':curr_col,\n",
        "                        'my_level':curr_level,\n",
        "                        'value':df_col1['overall'].mean()})\n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "print ('dfloop:::::::',dfloop.head(15))\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['my_level','country_relig'], columns=['desc'], \n",
        "    values=['value'], sort=False),3)\n",
        "\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "\n",
        "#piv = piv.reindex(row_order, axis=0)\n",
        "\n",
        "#Sort Col order\n",
        "print(piv.columns.to_list)\n",
        "\n",
        "order = [   ('value',    'CIAProtAll'),\n",
        "            ('value',    'CIACathAll'),\n",
        "            ('value',    'CIAOrthAll'),\n",
        "            ('value',     'CIAChrAll'),\n",
        "            ('value',     'CIAMusAll'),\n",
        "            ('value',     'CIABudAll'),\n",
        "            ('value',    'CIAHindAll'),\n",
        "            ('value', 'CIABudHindAll'),\n",
        "            ('value',     'CIAJewAll'),\n",
        "            ('value',   'CIAOthNoJew'),\n",
        "            ('value', 'CIAOthWithJew'),\n",
        "            ('value',   'CIABeliever'),  \n",
        "            ('value',   'CIANoRelAll')\n",
        "        ]        \n",
        "   \n",
        "col_order=[('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')\n",
        "          ]\n",
        "piv = piv.reindex(col_order, axis=1)\n",
        "\n",
        "print ('piv',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_colonizer_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_colonizer_counts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "df_level.to_excel(joined_path)\n",
        "\n",
        "time_frame=['overall']\n",
        "\n",
        "#compare_means(df_tmp, other)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Ebfl_yHppw"
      },
      "source": [
        "##Table who_colonized\n",
        "\n",
        "'COLONY_AUSTRALIA', 'COLONY_AUSTRO_HUNG', 'COLONY_BELGIUM', 'COLONY_BRITAIN', 'COLONY_CHINA', 'COLONY_DENMARK', 'COLONY_FRANCE', 'COLONY_GERMANY', 'COLONY_ITALY', 'COLONY_JAPAN', 'COLONY_NETH', 'COLONY_NEW_ZEALAND', 'COLONY_NORWAY', 'COLONY_PORTUGAL', 'COLONY_RUSSIA', 'COLONY_SPAIN', 'COLONY_TURKEY', 'COLONY_USA'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eRWuNmvHttC"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "print(df_merged.info(verbose=True))\n",
        "\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "df_tmp=df_tmp.reset_index(drop=False)\n",
        "try:\n",
        "    df_tmp.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "df_tmp['colony_oth']= df_tmp['COLONY_AUSTRALIA'] + df_tmp['COLONY_AUSTRO_HUNG'] + df_tmp['COLONY_BELGIUM'] + df_tmp['COLONY_CHINA'] + df_tmp['COLONY_DENMARK'] + df_tmp['COLONY_GERMANY'] + df_tmp['COLONY_ITALY'] + df_tmp['COLONY_JAPAN'] + df_tmp['COLONY_NETH'] + df_tmp['COLONY_NEW_ZEALAND'] + df_tmp['COLONY_NORWAY'] + df_tmp['COLONY_PORTUGAL'] + df_tmp['COLONY_RUSSIA'] + df_tmp['COLONY_USA']\n",
        "colonizers=['COLONY_AUSTRALIA', 'COLONY_AUSTRO_HUNG', 'COLONY_BELGIUM', 'COLONY_BRITAIN', 'COLONY_CHINA', 'COLONY_DENMARK', 'COLONY_FRANCE', 'COLONY_GERMANY', 'COLONY_ITALY', 'COLONY_JAPAN', 'COLONY_NETH', 'COLONY_NEW_ZEALAND', 'COLONY_NORWAY', 'COLONY_PORTUGAL', 'COLONY_RUSSIA', 'COLONY_SPAIN', 'COLONY_TURKEY', 'COLONY_USA', 'colony_oth']\n",
        "\n",
        "\n",
        "desc_col_lst =['Financial Development Index',\n",
        "               'Financial Institutions Index',\n",
        "               'Financial Institutions Depth Index',\n",
        "               'Financial Institutions Access Index',\n",
        "               'Financial Institutions Efficiency Index',\n",
        "               'Financial Markets Index',\n",
        "               'Financial Markets Depth Index',\n",
        "               'Financial Markets Access Index',\n",
        "               'Financial Markets Efficiency Index']\n",
        "\n",
        "keep_col_lst=   ['FIPS',\n",
        "                'country',\n",
        "                'country_relig'\n",
        "                ]\n",
        "\n",
        "#Assign the level of: Colonized, Colonizer, NeitherCol\n",
        "\n",
        "'''\n",
        "'''\n",
        "#df_t1=df_tmp.copy(deep=True)\n",
        "#df_t2=df_t1[['FIPS','country_relig', 'desc','COLONY_AUSTRALIA', 'COLONY_AUSTRO_HUNG', 'COLONY_BELGIUM', 'COLONY_BRITAIN', 'COLONY_CHINA', 'COLONY_DENMARK', 'COLONY_FRANCE', 'COLONY_GERMANY', 'COLONY_ITALY', 'COLONY_JAPAN', 'COLONY_NETH', 'COLONY_NEW_ZEALAND', 'COLONY_NORWAY', 'COLONY_PORTUGAL', 'COLONY_RUSSIA', 'COLONY_SPAIN', 'COLONY_TURKEY', 'COLONY_USA','colony_oth']]\n",
        "#df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "\n",
        "#save colonizer name into ['my_level'] column\n",
        "for curr_colonizer in colonizers:\n",
        "    df_t2.loc[df_t2[curr_colonizer]==1,'my_level']=curr_colonizer\n",
        "\n",
        "df_level=df_t2.groupby(['my_level','country_relig'],as_index=False).agg({'FIPS':'count'})\n",
        "\n",
        "print('Separation into my_level groups:', df_level)\n",
        "\n",
        "df_level=df_tmp.groupby(['country_relig'])\n",
        "# add additional columns\n",
        "df_r0 = df_tmp.loc[df_tmp['desc'].isin(desc_col_lst)].copy(deep=True)\n",
        "df_r1=df_t2.merge(df_r0,on='FIPS',how='outer')\n",
        "\n",
        "df_r1.rename(columns={'country_relig_x':'country_relig','desc_x':'desc', \n",
        "                      'overall_x':'overall',\n",
        "                      'COLONY_AUSTRALIA_x':'COLONY_AUSTRALIA', \n",
        "                      'COLONY_AUSTRO_HUNG_x':'COLONY_AUSTRO_HUNG', \n",
        "                      'COLONY_BELGIUM_x':'COLONY_BELGIUM', \n",
        "                      'COLONY_BRITAIN_x':'COLONY_BRITAIN', \n",
        "                      'COLONY_CHINA_x':'COLONY_CHINA', \n",
        "                      'COLONY_DENMARK_x':'COLONY_DENMARK', \n",
        "                      'COLONY_FRANCE_x':'COLONY_FRANCE', \n",
        "                      'COLONY_GERMANY_x':'COLONY_GERMANY', \n",
        "                      'COLONY_ITALY_x':'COLONY_ITALY', \n",
        "                      'COLONY_JAPAN_x':'COLONY_JAPAN', \n",
        "                      'COLONY_NETH_x':'COLONY_NETH', \n",
        "                      'COLONY_NEW_ZEALAND_x':'COLONY_NEW_ZEALAND', \n",
        "                      'COLONY_NORWAY_x':'COLONY_NORWAY', \n",
        "                      'COLONY_PORTUGAL_x':'COLONY_PORTUGAL', \n",
        "                      'COLONY_RUSSIA_x':'COLONY_RUSSIA', \n",
        "                      'COLONY_SPAIN_x':'COLONY_SPAIN', \n",
        "                      'COLONY_TURKEY_x':'COLONY_TURKEY', \n",
        "                      'COLONY_USA_x':'COLONY_USA'},inplace=True)\n",
        "print('df_r1.head',df_r1.head(10))\n",
        "#df_t2 has level, relig, fips\n",
        "#df_tmp has fips, fd\n",
        "'''\n",
        "'''\n",
        "df_tmp=df_tmp[['FIPS','country_relig', 'desc','COLONY_AUSTRALIA', 'COLONY_AUSTRO_HUNG', 'COLONY_BELGIUM', 'COLONY_BRITAIN', 'COLONY_CHINA', 'COLONY_DENMARK', 'COLONY_FRANCE', 'COLONY_GERMANY', 'COLONY_ITALY', 'COLONY_JAPAN', 'COLONY_NETH', 'COLONY_NEW_ZEALAND', 'COLONY_NORWAY', 'COLONY_PORTUGAL', 'COLONY_RUSSIA', 'COLONY_SPAIN', 'COLONY_TURKEY', 'COLONY_USA','colony_oth']]\n",
        "\n",
        "df_tmp= df_tmp.loc[df_tmp['desc'].isin(desc_col_lst)].copy(deep=True)\n",
        "\n",
        "print(df_tmp.head(200))\n",
        "\n",
        "# by lat_level, by religion, by FD (column)\n",
        "temp=[]\n",
        "for curr_level in colonizers:\n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in desc_col_lst:\n",
        "            df_col=df_tmp.copy(deep=True)\n",
        "            df_col=df_col.loc[(df_col['my_level']==curr_level)&(df_col['desc']==curr_col)&(df_col['country_relig']==curr_relig)]\n",
        "            temp.append({'country_relig':curr_relig,\n",
        "                        'desc':curr_col,\n",
        "                        'my_level':curr_level,\n",
        "                        'value':df_col['overall'].mean()})\n",
        "dfloop = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "print ('dfloop:::::::',dfloop.head(15))\n",
        "piv=np.round(dfloop.pivot_table(\n",
        "    index=['my_level','country_relig'], columns=['desc'], \n",
        "    values=['value'], sort=False),3)\n",
        "'''\n",
        "'''\n",
        "piv.reset_index(inplace=True)\n",
        "try:\n",
        "    piv.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "'''\n",
        "'''\n",
        "#Sort Row order\n",
        "#print(piv.index.to_list)\n",
        "row_order =['HDI Score', \n",
        "         'UN Income Index',\n",
        "         'Gini index (World Bank estimate)', \n",
        "         'Multidimensional Poverty Index',\n",
        "         'UN Life Expectancy Index', \n",
        "         'Life expectancy at birth, total (years)',\n",
        "         'UIS: Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes',\n",
        "         'Prison per 100K', \n",
        "         'sfi',\n",
        "         'actotal', \n",
        "         'autoc']\n",
        "\n",
        "#piv = piv.reindex(row_order, axis=0)\n",
        "\n",
        "#Sort Col order\n",
        "print(piv.columns.to_list)\n",
        "\n",
        "order = [   ('value',    'CIAProtAll'),\n",
        "            ('value',    'CIACathAll'),\n",
        "            ('value',    'CIAOrthAll'),\n",
        "            ('value',     'CIAChrAll'),\n",
        "            ('value',     'CIAMusAll'),\n",
        "            ('value',     'CIABudAll'),\n",
        "            ('value',    'CIAHindAll'),\n",
        "            ('value', 'CIABudHindAll'),\n",
        "            ('value',     'CIAJewAll'),\n",
        "            ('value',   'CIAOthNoJew'),\n",
        "            ('value', 'CIAOthWithJew'),\n",
        "            ('value',   'CIABeliever'),  \n",
        "            ('value',   'CIANoRelAll')\n",
        "        ]        \n",
        "col_order=[('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')\n",
        "          ]\n",
        "piv = piv.reindex(col_order, axis=1)\n",
        "try:\n",
        "    piv.drop(columns=['level_0'],inplace=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print ('piv',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_who_colonized_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_who_colonized_counts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "df_level.to_excel(joined_path)\n",
        "\n",
        "#compare_means(df_tmp, 'other')\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31vjB4CFJcMF"
      },
      "source": [
        "##Table INC_GRP: Religion, income, finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkcxKUn9Pit1"
      },
      "outputs": [],
      "source": [
        "col_order = ['Financial Development Index', \n",
        "         'Financial Institutions Index', 'Financial Institutions Depth Index',\n",
        "         'Financial Institutions Access Index', 'Financial Institutions Efficiency Index',\n",
        "         'Financial Markets Index','Financial Markets Depth Index',\n",
        "         'Financial Markets Access Index','Financial Markets Efficiency Index']\n",
        "\n",
        "order =    [(        'Low income',             'Financial Development Index'),\n",
        "            (        'Low income',            'Financial Institutions Index'),\n",
        "            (        'Low income',      'Financial Institutions Depth Index'),\n",
        "            (        'Low income',     'Financial Institutions Access Index'),\n",
        "            (        'Low income', 'Financial Institutions Efficiency Index'),\n",
        "            (        'Low income',                 'Financial Markets Index'),\n",
        "            (        'Low income',           'Financial Markets Depth Index'),\n",
        "            (        'Low income',          'Financial Markets Access Index'),\n",
        "            (        'Low income',      'Financial Markets Efficiency Index'),\n",
        "            (        'Lower middle income',             'Financial Development Index'),\n",
        "            (        'Lower middle income',            'Financial Institutions Index'),\n",
        "            (        'Lower middle income',      'Financial Institutions Depth Index'),\n",
        "            (        'Lower middle income',     'Financial Institutions Access Index'),\n",
        "            (        'Lower middle income', 'Financial Institutions Efficiency Index'),\n",
        "            (        'Lower middle income',                 'Financial Markets Index'),\n",
        "            (        'Lower middle income',           'Financial Markets Depth Index'),\n",
        "            (        'Lower middle income',          'Financial Markets Access Index'),\n",
        "            (        'Lower middle income',      'Financial Markets Efficiency Index'),\n",
        "            (        'Upper middle income',             'Financial Development Index'),\n",
        "            (        'Upper middle income',            'Financial Institutions Index'),\n",
        "            (        'Upper middle income',      'Financial Institutions Depth Index'),\n",
        "            (        'Upper middle income',     'Financial Institutions Access Index'),\n",
        "            (        'Upper middle income', 'Financial Institutions Efficiency Index'),\n",
        "            (        'Upper middle income',                 'Financial Markets Index'),\n",
        "            (        'Upper middle income',           'Financial Markets Depth Index'),\n",
        "            (        'Upper middle income',          'Financial Markets Access Index'),\n",
        "            (        'Upper middle income',      'Financial Markets Efficiency Index'),\n",
        "            (        'High income',             'Financial Development Index'),\n",
        "            (        'High income',            'Financial Institutions Index'),\n",
        "            (        'High income',      'Financial Institutions Depth Index'),\n",
        "            (        'High income',     'Financial Institutions Access Index'),\n",
        "            (        'High income', 'Financial Institutions Efficiency Index'),\n",
        "            (        'High income',                 'Financial Markets Index'),\n",
        "            (        'High income',           'Financial Markets Depth Index'),\n",
        "            (        'High income',          'Financial Markets Access Index'),\n",
        "            (        'High income',      'Financial Markets Efficiency Index')]\n",
        "\n",
        "\n",
        "'''\n",
        "c=pd.melt(df_iset_subset,id_vars=['country','FIPS','desc'])\n",
        "c.rename(columns={'variable':'year'},inplace=True)\n",
        "\n",
        "#Merge with religion data\n",
        "b=c.merge(df_all_religions,on='FIPS',how='outer')\n",
        "a=b.merge(df_country,on='FIPS',how='outer')\n",
        "a.rename(columns={'country_x':'country'},inplace=True)\n",
        "\n",
        "a=a.loc[~a['value'].isnull()]\n",
        "a=a.loc[a['year']=='overall']\n",
        "#print (a.head(100))\n",
        "'''\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc', 'inc_grp','overall']\n",
        "\n",
        "a=df_merged.loc[df_merged['desc'].isin(fd_list)]\n",
        "\n",
        "#reduce columns of dataframe\n",
        "a.drop(a.columns.difference(my_col_list),axis=1, inplace=True )\n",
        "#a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "my_inc_grp = a['inc_grp'].unique()\n",
        "\n",
        "# Calc FD metric averages for each inc_grp\n",
        "temp=[]\n",
        "for curr_inc_grp in my_inc_grp: \n",
        "    for curr_col in col_order:\n",
        "        df_a=df_tmp.loc[(df_tmp['inc_grp']==curr_inc_grp)&(df_tmp['desc']==curr_col)]\n",
        "        df_a['overall']=pd.to_numeric(df_a['overall'], errors ='coerce')\n",
        "        df_a['overall'] = df_a['overall'].astype(float, errors = 'raise')\n",
        "        temp.append({'inc_grp':curr_inc_grp,'desc':curr_col,'avg':df_a['overall'].mean()})\n",
        "dfgroups = pd.DataFrame.from_dict(temp) \n",
        "print('Averages for each inc_grp:\\n', dfgroups)\n",
        "\n",
        "#Calculate country_relig averages within each inc_grp\n",
        "temp=[]\n",
        "for curr_inc_grp in my_inc_grp: \n",
        "    grp_cnt=a.loc[a['inc_grp']==curr_inc_grp]['FIPS'].nunique()\n",
        "    for curr_relig in relig_list:\n",
        "        relig_cnt=a.loc[(a['inc_grp']==curr_inc_grp)&(a['country_relig']==curr_relig)]['FIPS'].nunique()\n",
        "        collst=a['desc'].unique()\n",
        "        for curr_col in collst:\n",
        "            df_a=a.loc[(a['desc']==curr_col) & (a['country_relig']==curr_relig) & (a['inc_grp']==curr_inc_grp)].copy(deep=True)\n",
        "            temp.append({'grp_cnt':grp_cnt,'relig_cnt':relig_cnt,'country_relig':curr_relig,'inc_grp':curr_inc_grp,'desc':curr_col,'overall':df_a['overall'].mean()})\n",
        "dftemp = pd.DataFrame.from_dict(temp) \n",
        "dftemp[\"country_relig\"] = dftemp[\"country_relig\"].astype('category')\n",
        "dftemp[\"desc\"] = dftemp[\"desc\"].astype('category')\n",
        "\n",
        "# Main df\n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "piv=np.round(dftemp.pivot_table(index=['inc_grp','desc'], \n",
        "                                    columns=['country_relig'], \n",
        "                                    values=['overall'], sort=False),3)\n",
        "#print(piv.index.to_list)\n",
        "\n",
        "# Group & Relig count dfs\n",
        "piv_relig_cnt=np.round(dftemp.pivot_table(index=['inc_grp'],columns=['country_relig'], values=['relig_cnt'],sort=False),3)\n",
        "piv_grp_cnt=np.round(dftemp.pivot_table(index=['inc_grp'], values=['grp_cnt'],sort=False))\n",
        "\n",
        "#sort rows in pivot_table\n",
        "piv = piv.reindex(order, axis=0)\n",
        "#piv_relig_cnt = piv_relig_cnt.reindex(order, axis=0)\n",
        "#piv_grp_cnt = piv_grp_cnt.reindex(order, axis=0)\n",
        "\n",
        "\n",
        "print ('\\n',curr_inc_grp,'\\n',piv)\n",
        "\n",
        "\n",
        "print('piv_relig_cnt',piv_relig_cnt)\n",
        "print('piv_grp_cnt',piv_grp_cnt)\n",
        "\n",
        "# Group values df\n",
        "for curr_inc_grp in my_inc_grp:\n",
        "    dftemp = pd.DataFrame.from_dict(temp)\n",
        "    dftemp = dftemp.loc[dftemp['inc_grp']==curr_inc_grp].copy(deep=True) \n",
        "\n",
        "dftemp = pd.DataFrame.from_dict(temp)\n",
        "dftemp[\"desc\"] = dftemp[\"desc\"].astype('category')\n",
        "\n",
        "inc_grp_vals=np.round(dftemp.groupby(['inc_grp','desc'])['overall'].mean(),3)\n",
        "inc_grp_vals = inc_grp_vals.reindex(order, axis=0)\n",
        "\n",
        "print('inc_grp_val:\\n',inc_grp_vals)\n",
        "\n",
        "piv.rename(columns={'CIAProtAll':'Prot', 'CIACathAll':'Cath', 'CIAOrthAll':'Orth', 'CIAChrAll':'Chr', \n",
        "            'CIAMusAll':'Mus', 'CIABudAll':'Bud', 'CIAHindAll':'Hind', 'CIABudHindAll':'BudHind',\n",
        "            'CIAJewAll':'Jew', 'CIAOthNoJew':'Oth-NoJew', 'CIAOthWithJew':'Oth-Jew', \n",
        "            'CIABelieverAll':'Believers','CIANoRelAll':'No-Rel'},inplace=True)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_INC_GRP_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_INC_GRP_relig_cnts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv_relig_cnt.to_excel(joined_path)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_INC_GRP_group_cnts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv_grp_cnt.to_excel(joined_path)\n",
        "\n",
        "print(inc_grp_vals)\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_INC_GRP_group_vals.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "inc_grp_vals.to_excel(joined_path)\n",
        "\n",
        "#compare_means(dftemp, 'other')\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h82tkJgTDmco"
      },
      "source": [
        "##Table INC_GRP using GNI per capita bins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp3ljVkzDupc"
      },
      "outputs": [],
      "source": [
        "#########################\n",
        "###   Create bins using ['GNI per capita'] column\n",
        "#########################\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','GNI per capita']\n",
        "\n",
        "col_order = ['Financial Development Index', \n",
        "         'Financial Institutions Index', 'Financial Institutions Depth Index',\n",
        "         'Financial Institutions Access Index', 'Financial Institutions Efficiency Index',\n",
        "         'Financial Markets Index','Financial Markets Depth Index',\n",
        "         'Financial Markets Access Index','Financial Markets Efficiency Index']\n",
        "\n",
        "order =    [(        'lo_25',             'Financial Development Index'),\n",
        "            (        'lo_25',            'Financial Institutions Index'),\n",
        "            (        'lo_25',      'Financial Institutions Depth Index'),\n",
        "            (        'lo_25',     'Financial Institutions Access Index'),\n",
        "            (        'lo_25', 'Financial Institutions Efficiency Index'),\n",
        "            (        'lo_25',                 'Financial Markets Index'),\n",
        "            (        'lo_25',           'Financial Markets Depth Index'),\n",
        "            (        'lo_25',          'Financial Markets Access Index'),\n",
        "            (        'lo_25',      'Financial Markets Efficiency Index'),\n",
        "            (        'lomed_25',             'Financial Development Index'),\n",
        "            (        'lomed_25',            'Financial Institutions Index'),\n",
        "            (        'lomed_25',      'Financial Institutions Depth Index'),\n",
        "            (        'lomed_25',     'Financial Institutions Access Index'),\n",
        "            (        'lomed_25', 'Financial Institutions Efficiency Index'),\n",
        "            (        'lomed_25',                 'Financial Markets Index'),\n",
        "            (        'lomed_25',           'Financial Markets Depth Index'),\n",
        "            (        'lomed_25',          'Financial Markets Access Index'),\n",
        "            (        'lomed_25',      'Financial Markets Efficiency Index'),\n",
        "            (        'himed_25',             'Financial Development Index'),\n",
        "            (        'himed_25',            'Financial Institutions Index'),\n",
        "            (        'himed_25',      'Financial Institutions Depth Index'),\n",
        "            (        'himed_25',     'Financial Institutions Access Index'),\n",
        "            (        'himed_25', 'Financial Institutions Efficiency Index'),\n",
        "            (        'himed_25',                 'Financial Markets Index'),\n",
        "            (        'himed_25',           'Financial Markets Depth Index'),\n",
        "            (        'himed_25',          'Financial Markets Access Index'),\n",
        "            (        'himed_25',      'Financial Markets Efficiency Index'),\n",
        "            (        'hi_25',             'Financial Development Index'),\n",
        "            (        'hi_25',            'Financial Institutions Index'),\n",
        "            (        'hi_25',      'Financial Institutions Depth Index'),\n",
        "            (        'hi_25',     'Financial Institutions Access Index'),\n",
        "            (        'hi_25', 'Financial Institutions Efficiency Index'),\n",
        "            (        'hi_25',                 'Financial Markets Index'),\n",
        "            (        'hi_25',           'Financial Markets Depth Index'),\n",
        "            (        'hi_25',          'Financial Markets Access Index'),\n",
        "            (        'hi_25',      'Financial Markets Efficiency Index')]\n",
        "\n",
        "a=df_merged.loc[df_merged['desc'].isin(fd_list)]\n",
        "a.drop(a.columns.difference(my_col_list),axis=1, inplace=True )\n",
        "\n",
        "a.rename(columns={'GNI per capita':'GNIperCapita'},inplace=True)\n",
        "\n",
        "#make bins\n",
        "\n",
        "labels=['lo_25','lomed_25', 'himed_25', 'hi_25']\n",
        "\n",
        "make_bins(df2split=a,\n",
        "          split_into=4,\n",
        "          val_col=['GNIperCapita'],\n",
        "          col_name='GNIperCapita',\n",
        "          labels=labels)\n",
        "\n",
        "bins=['bin_GNIperCapita_jenkspy','bin_GNIperCapita_freq', 'bin_GNIperCapita_dist']\n",
        "\n",
        "#counts\n",
        "use_bin='bin_GNIperCapita_freq'\n",
        "\n",
        "# Calc FD metric averages for each inc_grp\n",
        "temp=[]\n",
        "for curr_inc_grp in labels: \n",
        "    for curr_col in col_order:\n",
        "        df_a=a.loc[(a['bin_GNIperCapita_freq']==curr_inc_grp)&(a['desc']==curr_col)]\n",
        "        df_a['overall']=pd.to_numeric(df_a['overall'], errors ='coerce')\n",
        "        df_a['overall'] = df_a['overall'].astype(float, errors = 'raise')\n",
        "        temp.append({'bin_GNIperCapita_freq':curr_inc_grp,'desc':curr_col,'avg':df_a['overall'].mean()})\n",
        "dfgroups = pd.DataFrame.from_dict(temp) \n",
        "print('Averages for each inc_grp:\\n', dfgroups)\n",
        "\n",
        "temp=[]\n",
        "for curr_inc_grp in labels: \n",
        "    grp_cnt=a.loc[a['bin_GNIperCapita_freq']==curr_inc_grp]['FIPS'].nunique()\n",
        "    for curr_relig in relig_list:\n",
        "        relig_cnt=a.loc[(a['bin_GNIperCapita_freq']==curr_inc_grp)&(a['country_relig']==curr_relig)]['FIPS'].nunique()\n",
        "        collst=a['desc'].unique()\n",
        "        for curr_col in collst:\n",
        "            df_a=a.loc[(a['desc']==curr_col) & (a['country_relig']==curr_relig) & (a['bin_GNIperCapita_freq']==curr_inc_grp)].copy(deep=True)\n",
        "            temp.append({'grp_cnt':grp_cnt,'relig_cnt':relig_cnt,'country_relig':curr_relig,'GNI_inc_grp':curr_inc_grp,'desc':curr_col,'overall':df_a['overall'].mean()})\n",
        "dftemp = pd.DataFrame.from_dict(temp) \n",
        "dftemp[\"country_relig\"] = dftemp[\"country_relig\"].astype('category')\n",
        "dftemp[\"desc\"] = dftemp[\"desc\"].astype('category')\n",
        "\n",
        "# Main df\n",
        "\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "piv=np.round(dftemp.pivot_table(index=['GNI_inc_grp','desc'], \n",
        "                                    columns=['country_relig'], \n",
        "                                    values=['overall'], sort=False),3)\n",
        "\n",
        "\n",
        "#sort rows in pivot_table\n",
        "piv = piv.reindex(order, axis=0)\n",
        "print ('\\n',piv)\n",
        "\n",
        "# Group & Relig count dfs\n",
        "piv_relig_cnt=np.round(dftemp.pivot_table(index=['GNI_inc_grp'],columns=['country_relig'], values=['relig_cnt'],sort=False),3)\n",
        "piv_grp_cnt=np.round(dftemp.pivot_table(index=['GNI_inc_grp'], values=['grp_cnt'],sort=False))\n",
        "\n",
        "print('piv_relig_cnt',piv_relig_cnt)\n",
        "print('piv_grp_cnt',piv_grp_cnt)\n",
        "\n",
        "# Group values df\n",
        "for curr_inc_grp in labels:\n",
        "    dftemp = pd.DataFrame.from_dict(temp)\n",
        "    dftemp = dftemp.loc[dftemp['GNI_inc_grp']==curr_inc_grp].copy(deep=True) \n",
        "\n",
        "dftemp = pd.DataFrame.from_dict(temp)\n",
        "dftemp[\"desc\"] = dftemp[\"desc\"].astype('category')\n",
        "\n",
        "inc_grp_vals=np.round(dftemp.groupby(['GNI_inc_grp','desc'])['overall'].mean(),3)\n",
        "\n",
        "inc_grp_vals = inc_grp_vals.reindex(order, axis=0)\n",
        "\n",
        "print('inc_grp_val:\\n',inc_grp_vals)\n",
        "\n",
        "piv.rename(columns={'CIAProtAll':'Prot', 'CIACathAll':'Cath', 'CIAOrthAll':'Orth', 'CIAChrAll':'Chr', \n",
        "            'CIAMusAll':'Mus', 'CIABudAll':'Bud', 'CIAHindAll':'Hind', 'CIABudHindAll':'BudHind',\n",
        "            'CIAJewAll':'Jew', 'CIAOthNoJew':'Oth-NoJew', 'CIAOthWithJew':'Oth-Jew', \n",
        "            'CIABelieverAll':'Believers','CIANoRelAll':'No-Rel'},inplace=True)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_GNI_INC_GRP_main.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_GNI_INC_GRP_relig_cnts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv_relig_cnt.to_excel(joined_path)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_GNI_INC_GRP_group_cnts.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv_grp_cnt.to_excel(joined_path)\n",
        "\n",
        "print(inc_grp_vals)\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables/' \n",
        "myfile = 'nTable_GNI_INC_GRP_group_vals.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "inc_grp_vals.to_excel(joined_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gZA1yEJVn4_"
      },
      "source": [
        "##Table LAW_means: Religion, law origin, finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHwryCgeVxwA"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "#  Ihsan needs to figure-out what to do with multiple-assigned Rule of law\n",
        "#######\n",
        "#adjust column order\n",
        "order =    [('value',             'Financial Development Index'),\n",
        "            ('value',            'Financial Institutions Index'),\n",
        "            ('value',      'Financial Institutions Depth Index'),\n",
        "            ('value',     'Financial Institutions Access Index'),\n",
        "            ('value', 'Financial Institutions Efficiency Index'),\n",
        "            ('value',                 'Financial Markets Index'),\n",
        "            ('value',           'Financial Markets Depth Index'),\n",
        "            ('value',          'Financial Markets Access Index'),\n",
        "            ('value',      'Financial Markets Efficiency Index')]\n",
        "\n",
        "order_col= [('value',    'CIAProtAll'),\n",
        "            ('value',    'CIACathAll'),\n",
        "            ('value',    'CIAOrthAll'),\n",
        "            ('value',     'CIAChrAll'),\n",
        "            ('value',     'CIAMusAll'),\n",
        "            ('value',     'CIABudAll'),\n",
        "            ('value',    'CIAHindAll'),\n",
        "            ('value', 'CIABudHindAll'),\n",
        "            ('value',     'CIAJewAll'),\n",
        "            ('value',   'CIAOthNoJew'),\n",
        "            ('value', 'CIAOthWithJew'),\n",
        "            ('value',   'CIABeliever'),  \n",
        "            ('value',   'CIANoRelAll'),\n",
        "            ('value',    'CIAProtAll'),\n",
        "            ('value',    'CIACathAll'),\n",
        "            ('value',    'CIAOrthAll'),\n",
        "            ('value',     'CIAChrAll'),\n",
        "            ('value',     'CIAMusAll'),\n",
        "            ('value',     'CIABudAll'),\n",
        "            ('value',    'CIAHindAll'),\n",
        "            ('value', 'CIABudHindAll'),\n",
        "            ('value',     'CIAJewAll'),\n",
        "            ('value',   'CIAOthNoJew'),\n",
        "            ('value', 'CIAOthWithJew'),\n",
        "            ('value',   'CIABeliever'),  \n",
        "            ('value',   'CIANoRelAll')\n",
        "        ]       \n",
        "\n",
        "a=df_merged.copy(deep=True)\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil']\n",
        "\n",
        "a=df_merged.loc[df_merged['desc'].isin(fd_list)]\n",
        "\n",
        "#reduce columns of dataframe\n",
        "a.drop(a.columns.difference(my_col_list),axis=1, inplace=True )\n",
        "a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Generate data for dfs\n",
        "my_rule_law_lst=['IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil']\n",
        "\n",
        "#calc group counts/vals\n",
        "temp=[]\n",
        "for curr_rule_law in my_rule_law_lst:\n",
        "    tt=a.loc[a[curr_rule_law]==1]\n",
        "    for curr_col in fd_list:\n",
        "        ttt=tt.loc[tt['desc']==curr_col]\n",
        "        grp_val=ttt['value'].mean()\n",
        "        temp.append({'grp':curr_rule_law, 'col':curr_col, 'value':grp_val})\n",
        "dfgrptemp = pd.DataFrame.from_dict(temp) \n",
        "grppiv=np.round(dfgrptemp.pivot_table(index=['grp'], columns=['col'], values=['value'], sort=False),3)\n",
        "grppiv = grppiv.reindex(order, axis=1)\n",
        "\n",
        "rel_lst = a['country_relig'].unique()\n",
        "temp=[]\n",
        "for curr_rule_law in my_rule_law_lst:\n",
        "    tt=a.loc[a[curr_rule_law]==1]\n",
        "    for curr_col in rel_lst:\n",
        "        ttt=tt.loc[tt['country_relig']==curr_col]\n",
        "        grp_cnt=ttt['FIPS'].nunique()\n",
        "        temp.append({'grp':curr_rule_law, 'col':curr_col, 'value':grp_cnt})\n",
        "dfgrpcnt = pd.DataFrame.from_dict(temp) \n",
        "grp_cnt_piv=np.round(dfgrpcnt.pivot_table(index=['grp'], columns=['col'], values=['value'], sort=False),3)\n",
        "grp_cnt_piv = grp_cnt_piv.reindex(order_col, axis=1)\n",
        "print ('MY COUNTS:\\n',grp_cnt_piv)\n",
        "\n",
        "#calc cnt/val by relig\n",
        "temp=[]\n",
        "for curr_rule_law in my_rule_law_lst:\n",
        "    for curr_relig in rel_lst:\n",
        "        tt=a.loc[(a[curr_rule_law]==1)&(a['country_relig']==curr_relig)]\n",
        "        relig_cnt=tt['FIPS'].nunique() \n",
        "        for curr_col in fd_list:\n",
        "            df_a=tt.loc[tt['desc']==curr_col].copy(deep=True)\n",
        "            temp.append({'relig_cnt':relig_cnt,'country_relig':curr_relig,'rule_of_law':curr_rule_law,'desc':curr_col,'value':df_a['value'].mean()})\n",
        "dftemp = pd.DataFrame.from_dict(temp) \n",
        "\n",
        "# Main df\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "piv=np.round(dftemp.pivot_table(index=['rule_of_law','country_relig'], columns=['desc'], values=['value'], sort=False),3)\n",
        "\n",
        "piv2=np.round(dftemp.pivot_table(index=['rule_of_law','country_relig'], values=['relig_cnt'], sort=False),3)\n",
        "\n",
        "print(piv2)\n",
        "\n",
        "#view existing row order (index)\n",
        "#print(piv.index.to_list)\n",
        "\n",
        "#view existing col order\n",
        "#print(piv.columns.to_list)\n",
        "\n",
        "#sort cols in pivot_table\n",
        "piv = piv.reindex(order, axis=1)\n",
        "grppiv = grppiv.reindex(order, axis=1)\n",
        "\n",
        "print ('\\n main table:',piv)\n",
        "print ('\\n grp_vals',grppiv)\n",
        "\n",
        "\n",
        "\n",
        "piv.rename(columns={'CIAProtAll':'Prot', 'CIACathAll':'Cath', 'CIAOrthAll':'Orth', 'CIAChrAll':'Chr', \n",
        "            'CIAMusAll':'Mus', 'CIABudAll':'Bud', 'CIAHindAll':'Hind', 'CIABudHindAll':'BudHind',\n",
        "            'CIAJewAll':'Jew', 'CIAOthNoJew':'Oth-NoJew', 'CIAOthWithJew':'Oth-Jew', \n",
        "            'CIABelieverAll':'Believers','CIANoRelAll':'No-Rel'},inplace=True)\n",
        "\n",
        "for curr_rule_law in my_rule_law_lst:\n",
        "    dftemp = pd.DataFrame.from_dict(temp)\n",
        "    dftemp = dftemp.loc[dftemp['rule_of_law']==curr_rule_law].copy(deep=True) \n",
        "    dftemp[\"country_relig\"] = dftemp[\"country_relig\"].astype('category')\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_LAW_main_means.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "myfile = 'nTable_LAW_grp_means.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "grppiv.to_excel(joined_path)\n",
        "\n",
        "myfile = 'nTable_LAW_grp_count.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "grp_cnt_piv.to_excel(joined_path)\n",
        "\n",
        "\n",
        "#compare_means(dftemp, 'other')\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ivqFBigrMH"
      },
      "source": [
        "#LAW regression options:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StMezx-hQcdM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "########################################\n",
        "#\n",
        "#  https://stackoverflow.com/questions/50733014/linear-regression-with-dummy-categorical-variables\n",
        "#\n",
        "#  MUST CONSIDER categorical variables and multicollinearity\n",
        "#\n",
        "########################################\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zWmYeR5AYCo"
      },
      "outputs": [],
      "source": [
        "a=df_merged.copy(deep=True)\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','IsikCommon', 'IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil']\n",
        "\n",
        "a=df_merged.loc[df_merged['desc'].isin(fd_list)]\n",
        "\n",
        "#reduce columns of dataframe\n",
        "a.drop(a.columns.difference(my_col_list),axis=1, inplace=True )\n",
        "a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Generate data for dfs\n",
        "my_rule_law_lst=['IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil']\n",
        "collst=a['desc'].unique()\n",
        "fips_list=a['FIPS'].unique()\n",
        "temp=[]\n",
        "'''for curr_rule_law in my_rule_law_lst:\n",
        "    for curr_relig in relig_list:\n",
        "        for curr_col in collst:\n",
        "            for curr_fips in fips_list:\n",
        "                df_a=a.loc[(a['desc']==curr_col) & (a['country_relig']==curr_relig) & (a[curr_rule_law]==1) & (a['FIPS']==curr_fips)].copy(deep=True)\n",
        "                if df_a['FIPS'].isnull:\n",
        "                    pass\n",
        "                else:\n",
        "                    temp.append({'FIPS':curr_fips,'country_relig':curr_relig,'rule_of_law':curr_rule_law,'desc':curr_col,'value':df_a['value']})\n",
        "dftemp = pd.DataFrame.from_dict(temp) \n",
        "'''\n",
        "#a_melt=pd.melt(a,id_vars=['FIPS','country_relig','desc','value'], var_name=['rule_law'], value_vars=['IsikCommon', 'IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil'],)\n",
        "a_piv=np.round(a.pivot_table(index=['FIPS','IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil'],columns=['desc'],values=['value']),3)\n",
        "#a_piv.reset_index(inplace=True)\n",
        "print (a_piv.head(20))\n",
        "\n",
        "# Main df\n",
        "#sort=False keeps the order of the columns=['country_relig'] in pivot_table\n",
        "#piv=np.round(dftemp.pivot_table(index=['FIPS','rule_of_law','country_relig'], columns=['desc'], values=['value'], sort=False),3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k57ucpf9qds7"
      },
      "source": [
        "##LAW assumption testing: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2E0l1RTYglm"
      },
      "outputs": [],
      "source": [
        "# Build linear regression model \n",
        "# To test assumptions, we must run the rgression, then inspect the predicted vs. actual results \n",
        "# Split data into predictors X and output Y\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['Financial Development Index',\n",
        "              'Financial Institutions Index',\n",
        "              'Financial Institutions Depth Index',\n",
        "              'Financial Institutions Access Index', \n",
        "              'Financial Institutions Efficiency Index',\n",
        "              'Financial Markets Index',\n",
        "              'Financial Markets Depth Index', \n",
        "              'Financial Markets Access Index', \n",
        "              'Financial Markets Efficiency Index']\n",
        "\n",
        "# Generate data for dfs\n",
        "my_rule_law_lst=['IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed']\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed']\n",
        "\n",
        "a=df_merged.loc[df_merged['desc'].isin(fd_list)]\n",
        "\n",
        "#reduce columns of dataframe\n",
        "a.drop(a.columns.difference(my_col_list),axis=1, inplace=True )\n",
        "a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "temp=[]\n",
        "for curr_rule_law in my_rule_law_lst:\n",
        "    df_a=a.copy(deep=True)\n",
        "    df_a=df_a.fillna(0)\n",
        "    df_a.reset_index(drop=False,inplace=True)\n",
        "    Y = df_a[curr_rule_law]\n",
        "    X = df_a['value']\n",
        "    X = sm.add_constant(X)  \n",
        "\n",
        "    assumption_check = linear_regression_assumptions(X, Y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ2QEIsBnBUx"
      },
      "source": [
        "##LAW: sklean LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SRF8h31mV5_"
      },
      "outputs": [],
      "source": [
        "# Build linear regression model \n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['Financial Development Index',\n",
        "              'Financial Institutions Index',\n",
        "              'Financial Institutions Depth Index',\n",
        "              'Financial Institutions Access Index', \n",
        "              'Financial Institutions Efficiency Index',\n",
        "              'Financial Markets Index',\n",
        "              'Financial Markets Depth Index', \n",
        "              'Financial Markets Access Index', \n",
        "              'Financial Markets Efficiency Index']\n",
        "\n",
        "# Generate data for dfs\n",
        "my_rule_law_lst=['IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed']\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed']\n",
        "\n",
        "a=df_merged.loc[df_merged['desc'].isin(fd_list)]\n",
        "\n",
        "#reduce columns of dataframe\n",
        "a.drop(a.columns.difference(my_col_list),axis=1, inplace=True )\n",
        "a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "temp=[]\n",
        "for curr_rule_law in my_rule_law_lst:\n",
        "    df_a=a.copy(deep=True)\n",
        "    df_a=df_a.fillna(0)\n",
        "    df_a.reset_index(drop=False,inplace=True)\n",
        "    Y = df_a[curr_rule_law]\n",
        "    X = df_a['value']\n",
        "    X = sm.add_constant(X)  \n",
        "\n",
        "    lm = LinearRegression()\n",
        "    result = lm.fit(X, Y)\n",
        "\n",
        "    print(f'\\n{curr_rule_law}')\n",
        "    print(f'intercept = {result.intercept_}\\nBeta vals:')\n",
        "    cnt=1\n",
        "    for pred in predictors:\n",
        "        try:\n",
        "            print(f'{pred}: {result.coef_[cnt]} ')    \n",
        "        except:\n",
        "            print('no coef for cnt = ',cnt, pred)\n",
        "        cnt+=1\n",
        "\n",
        "    #temp.append({'country_relig':curr_relig,'rule_of_law':curr_rule_law,'desc':curr_col,'value':df_a['value'].mean()})\n",
        "\n",
        "#dftemp = pd.DataFrame.from_dict(temp) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwyXRxP2zQHh"
      },
      "source": [
        "##LAW: Tobit regression (without providing artificial censorship values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZJ3aCnWzSZR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Build linear regression model \n",
        "# To test assumptions, we must run the rgression, then inspect the predicted vs. actual results \n",
        "# Split data into predictors X and output Y\n",
        "\n",
        "predictors = ['Financial Development Index',\n",
        "              'Financial Institutions Index',\n",
        "              'Financial Institutions Depth Index',\n",
        "              'Financial Institutions Access Index', \n",
        "              'Financial Institutions Efficiency Index',\n",
        "              'Financial Markets Index',\n",
        "              'Financial Markets Depth Index', \n",
        "              'Financial Markets Access Index', \n",
        "              'Financial Markets Efficiency Index']\n",
        "\n",
        "# Generate data for dfs\n",
        "my_rule_law_lst=['IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed']\n",
        "\n",
        "a=df_merged.copy(deep=True)\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','IsikCommon', 'IsikCivil', 'za_IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil']\n",
        "\n",
        "a=a.loc[df_merged['desc'].isin(fd_list)]\n",
        "\n",
        "#reduce columns of dataframe\n",
        "a.drop(a.columns.difference(my_col_list),axis=1, inplace=True )\n",
        "a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "ns=df_a.size\n",
        "cens = pd.Series(np.zeros((ns,))) \n",
        "\n",
        "df_a=a.copy(deep=True)\n",
        "df_a=df_a.fillna(0)\n",
        "df_a.reset_index(drop=False,inplace=True)\n",
        "\n",
        "temp=[]\n",
        "for curr_rule_law in my_rule_law_lst:\n",
        "    Y = df_a[curr_rule_law]\n",
        "    X = df_a['value']\n",
        "    X = sm.add_constant(X)  \n",
        "\n",
        "    tr = TobitModel()\n",
        "    result = tr.fit(X, Y,cens, verbose=False)\n",
        "    print(f'\\n{curr_rule_law}')\n",
        "    print(f'sigma is the std dist b/t obs and the regression line ~~ 95% obs will [-2*sigma, +2*sigma]:\\nsigma = {result.sigma_}\\nintercept = {result.intercept_}\\nBeta vals:')\n",
        "    cnt=1\n",
        "    for pred in predictors:\n",
        "        try:\n",
        "            print(f'{pred}: {result.coef_[cnt]} ')    \n",
        "        except:\n",
        "            print('no coef for:',cnt, pred)\n",
        "        cnt+=1\n",
        "    score = tr.score(X, Y, scoring_function=explained_variance_score) #r2_score is providing a huge value????\n",
        "    print ('Model R2:', score)\n",
        "'''    lm = LinearRegression()\n",
        "    model = lm.fit(X, Y)\n",
        "    print(f'\\n{curr_rule_law}: alpha = {model.intercept_}')\n",
        "    print(f'betas = {model.coef_}')\n",
        "    for pred in predictors:\n",
        "      print(pred)\n",
        "''' \n",
        "    #temp.append({'country_relig':curr_relig,'rule_of_law':curr_rule_law,'desc':curr_col,'value':df_a['value'].mean()})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Corr Matrix & Descriptive Statistics"
      ],
      "metadata": {
        "id": "Z2qAkuyMfilm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "df_tmp.reset_index(inplace=True)\n",
        "df_tmp.rename(columns={'overall':'value'}, inplace=True)\n",
        "\n",
        "# Calculate Institutions composite (zc_wgi_avg)\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t1=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t1=df_t1.groupby(['FIPS','country_relig'],as_index=False)['value'].mean()\n",
        "df_t1.rename(columns={'overall':'zc_wgi_avg'},inplace=True)\n",
        "df_t1.reset_index(inplace=True)\n",
        "#print(df_t1.head())\n",
        "\n",
        "#df_t2=[] = df_tmp.copy(deep=True)\n",
        "df_t2=df_tmp.loc[df_tmp['desc']=='openness'].copy(deep=True)\n",
        "df_t2=df_t2[['FIPS','desc','value']]\n",
        "df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_t2=df_t2.pivot(index='FIPS', columns='desc',values='value')\n",
        "#df_t2.rename(columns={'openness':'zc_openness'},inplace=True)\n",
        "df_t2.reset_index(inplace=True)\n",
        "\n",
        "\n",
        "print(df_t2.head())\n",
        "\n",
        "'''\n",
        "\n",
        "df_tmp['FD']=df_tmp.loc[df_tmp['desc']=='Financial Development Index']['overall']\n",
        "df_tmp['FI']=df_tmp.loc[df_tmp['desc']=='Financial Institutions Index']['overall']\n",
        "df_tmp['FM']=df_tmp.loc[df_tmp['desc']=='Financial Markets Index']['overall']\n",
        "df_tmp['FID']=df_tmp.loc[df_tmp['desc']=='Financial Institutions Depth Index']['overall']\n",
        "df_tmp['FIA']=df_tmp.loc[df_tmp['desc']=='Financial Institutions Access Index']['overall']\n",
        "df_tmp['FIE']=df_tmp.loc[df_tmp['desc']=='Financial Institutions Efficiency Index']['overall']\n",
        "df_tmp['FMD']=df_tmp.loc[df_tmp['desc']=='Financial Markets Depth Index']['overall']\n",
        "df_tmp['FMA']=df_tmp.loc[df_tmp['desc']=='Financial Markets Access Index']['overall']\n",
        "df_tmp['FME']=df_tmp.loc[df_tmp['desc']=='Financial Markets Efficiency Index']['overall']\n",
        "\n",
        "\n",
        "df_dan=df_tmp.loc[df_tmp['desc']=='Financial Development Index']\n",
        "\n",
        "df_dan['diff'] = np.where((df_dan['FD'] == df_dan['overall']),1,np.nan)\n",
        "print('\\nmy unique\\n',df_dan['diff'].unique())\n",
        "'''\n",
        "df_tmp.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'zd_sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_tmp.loc[df_tmp['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "\n",
        "#print(df_sfi.head())\n",
        "\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "df_a=df_tmp\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "#print('\\ndf_a_1\\n',df_a.head())\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "#print('\\ndf_a_2\\n',df_a.head())\n",
        "df_a=df_a.merge(df_t1, on='FIPS', how='outer')\n",
        "#print('\\ndf_a_3\\n',df_a.head())\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "#df_a['openness']=np.log(df_a['openness'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "df_test=df_a.copy(deep=True)\n",
        "\n",
        "df_a = pd.get_dummies(df_a, columns=['continent'])\n",
        "df_a.rename(columns={'continent_Africa':'zc_Africa',\n",
        "             'continent_Asia':'zc_Asia', \n",
        "             'continent_North America':'zc_N_Amer', \n",
        "             'continent_Oceania':'zc_Oceania', \n",
        "             'continent_South America':'zc_S_Amer'},inplace=True)\n",
        "df_a = pd.get_dummies(df_a, columns=['region'])\n",
        "#print(df_a.columns.to_list())\n",
        "\n",
        "df_a.rename(columns={\n",
        "    'region_Northern Europe':'zc_N_Europe',  \n",
        "    'region_Southern Asia':'zc_S_Asia', \n",
        "    'region_Southern Europe':'zc_S_Europe', \n",
        "    'region_Western Africa':'zc_W_Africa', \n",
        "    'region_Western Asia':'zc_W_Asia', \n",
        "    'region_Australia and New Zealand':'zc_Australia_NZ', \n",
        "    'region_Caribbean':'zc_Caribbean', \n",
        "    'region_Central America':'zc_C_Amer', \n",
        "    'region_Central Asia':'zc_C_Asia', \n",
        "    'region_Eastern Africa':'zc_E_Africa', \n",
        "    'region_Eastern Asia':'zc_E_Asia', \n",
        "    'region_Eastern Europe':'zc_E_Europe', \n",
        "    'region_Melanesia':'zc_Melanesia', \n",
        "    'region_Micronesia':'zc_Micronesia', \n",
        "    'region_Middle Africa':'zc_M_Africa', \n",
        "    'region_Northern Africa':'zc_N_Africa', \n",
        "    'region_Northern America':'zc_N_Amer', \n",
        "    'region_Polynesia':'zc_Polynesia', \n",
        "    'region_South America':'zc_S_Amer', \n",
        "    'region_South-eastern Asia':'zc_SE_Asia', \n",
        "    'region_Southern Africa':'zc_S_Africa'},inplace=True)\n",
        "\n",
        "####################### \n",
        "# Corr Matrix\n",
        "#\n",
        "df_test=df_a[['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "              'za_IsikFrench','zb_BeckSettlerMortality','zd_sfi','ze_Population (millions)',\n",
        "              'CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', 'CIAOthWithJew%',\n",
        "              'openness', 'AbsLatitude',\n",
        "              'FD','FI','FIA','FID','FIE','FM','FMA','FMD',\n",
        "              'zc_N_Europe', 'zc_S_Asia', 'zc_S_Europe','zc_W_Africa', 'zc_W_Asia', 'zc_Australia_NZ',\n",
        "              'zc_Caribbean','zc_C_Amer', 'zc_C_Asia', 'zc_E_Africa', 'zc_E_Asia', 'zc_E_Europe', \n",
        "              'zc_Melanesia','zc_Micronesia','zc_M_Africa', 'zc_N_Africa',\n",
        "              'zc_N_Amer', 'zc_Polynesia','zc_S_Amer', 'zc_SE_Asia', \n",
        "              'zc_S_Africa', 'zc_Africa','zc_Asia', 'zc_Oceania']]\n",
        "#print(df_test.columns.to_list())\n",
        "#df_test=df_a[['FD','FI','FIA','FID','FIE','FM','FMA','FMD']]\n",
        "corr_matrix=df_test.rcorr()\n",
        "print(df_test.corr())\n",
        "\n",
        "#corr_matrix=pg.pairwise_corr(df_test)\n",
        "print (corr_matrix)\n",
        "print(df_test.corr())\n",
        "\n",
        "myfile = 'Corr_Matrix.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "corr_matrix.to_excel(joined_path)\n",
        "\n",
        "####################### \n",
        "# Descriptive Statistics\n",
        "#\n",
        "df_test=df_a[['za_IsikFrench','zb_BeckSettlerMortality','zd_sfi','ze_Population (millions)',\n",
        "              'CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', 'CIAOthWithJew%',\n",
        "              'AbsLatitude',\n",
        "              'FD','FI','FIA','FID','FIE','FM','FMA','FMD']]\n",
        "\n",
        "my_res=univariate_metrics(df_test)\n",
        "my_descript=my_res.pivot(index=['desc'],columns=['Col_Name'],values=['value'])\n",
        "\n",
        "print(my_res)\n",
        "\n",
        "myfile = 'Descriptive_stats.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "my_descript.to_excel(joined_path)\n",
        "\n",
        "####################### \n",
        "#TEST ASSUMPTIONS\n",
        "\n",
        "'''for curr_dv in predictors:\n",
        "    df_loop=df_a.loc[df_a['desc']==curr_dv].copy(deep=True)\n",
        "    Y = df_loop['value']\n",
        "    X = df_a[['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew']]\n",
        "    X = sm.add_constant(X)  \n",
        "    LinearRegression.fit(X=X,y=Y)\n",
        "    assumption_check = linear_regression_assumptions(X, Y)\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8u_dC6vyfvi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regression Tables"
      ],
      "metadata": {
        "id": "QwFo4z7n-x5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OLD_NOT_USED  Regression Table 2.1"
      ],
      "metadata": {
        "id": "Siky_nv4uYAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['Financial Development Index',\n",
        "              'Financial Institutions Index',\n",
        "              'Financial Markets Index']\n",
        "\n",
        "\n",
        "# Calculate Institutions composite (wgi_avg)\n",
        "\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'zc_wgi_avg'},inplace=True)\n",
        "\n",
        "\n",
        "##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "\n",
        "iv_list=[['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zd_sfi', 'ze_Population (millions)'],\n",
        "         ['za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zd_sfi','ze_Population (millions)'],\n",
        "         ['zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "         ['zc_wgi_avg'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_wgi_avg'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_wgi_avg','zd_sfi','ze_Population (millions)'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality','zc_wgi_avg','zd_sfi','ze_Population (millions)'],\n",
        "        ]\n",
        "\n",
        "temp=[]\n",
        "for curr_fd in predictors:\n",
        "    # Only look at current FD value\n",
        "    df_loop=df_a.loc[df_a['desc']==curr_fd].copy(deep=True)\n",
        "    # define DV\n",
        "    Y = df_loop['value']  \n",
        "    print ('\\nnbr of countries in DV:',df_loop['FIPS'].nunique())\n",
        "\n",
        "    for curr_iv in iv_list:\n",
        "        X = df_loop[curr_iv]\n",
        "        X = sm.add_constant(X)  \n",
        "        pglm = pg.linear_regression(X, Y)\n",
        "        pglm=pglm[['names', 'coef','pval','r2']]\n",
        "        pglm=pglm.loc[pglm['names']!='Intercept']\n",
        "        \n",
        "        #assign pval designator\n",
        "        select_conditions = [\n",
        "            (pglm['pval'] <=.01),\n",
        "            ((pglm['pval'] <=.05)&(pglm['pval'] >.01)),\n",
        "            ((pglm['pval'] <=.1)&(pglm['pval'] >.05)),\n",
        "            (pglm['pval'] >.1)\n",
        "            ]\n",
        "        select_values = ['a', 'b', 'c','']\n",
        "        pglm['my_level'] = np.select(select_conditions, select_values)\n",
        "\n",
        "        pglm.drop(columns=['pval'],inplace=True)\n",
        "\n",
        "        #transpose data for output table\n",
        "        pglm=pglm.transpose()\n",
        "        temp.append(pglm.round(3))\n",
        "\n",
        "df_table_2_1 = pd.concat(temp)\n",
        "print('df_table_2_1',df_table_2_1)\n",
        "print(X.rcorr())\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_1.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "df_table_2_1.to_excel(joined_path)\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "for curr_fd in fd_list:\n",
        "    df_loop=df_a.loc[df_a['desc']==curr_fd].copy(deep=True)\n",
        "    Y = df_loop['value']\n",
        "    X = df_loop['IsikCivil']\n",
        "    X = sm.add_constant(X)  \n",
        "\n",
        "    pglm = pg.linear_regression(X, Y)\n",
        "    print(f'\\n{curr_fd}')\n",
        "    print(pglm.round(2))\n",
        "'''\n",
        "######################################\n",
        "##### response variable = Financial index; --reset multi-valued index\n",
        "######################################\n",
        "'''df_a=a.copy(deep=True)\n",
        "df_a=df_a.fillna(0)\n",
        "df_a.reset_index(drop=False,inplace=True)\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "df_a_piv=np.round(df_a.pivot_table(index=['FIPS','country_relig'],columns=['desc'],values=['value']),3)\n",
        "df_a_piv.columns = df_a_piv.columns.droplevel(0)\n",
        "df_a=df_a_piv.merge(a,on='FIPS',how='outer')\n",
        "df_a = pd.get_dummies(df_a, columns=['country_relig'])\n",
        "print(df_a.columns.to_list())\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "RLRimjGXuVO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Table 2.1 Relig dummies"
      ],
      "metadata": {
        "id": "AMXbK64kuXcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "df_tmp.reset_index(inplace=True)\n",
        "df_tmp.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "\n",
        "# Calculate Institutions composite (zc_wgi_avg)\n",
        "\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "'''\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t1=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "df_t1=df_t1.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t1.rename(columns={'overall':'zc_wgi_avg'},inplace=True)\n",
        "df_t1.reset_index(inplace=True)\n",
        "#print(df_t1.head())\n",
        "df_a=df_a.merge(df_t1, on='FIPS', how='outer')\n",
        "\n",
        "'''\n",
        "##########new comment remove fd from filter\n",
        "##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')|(df_tmp['desc']=='openness')]\n",
        "\n",
        "\n",
        "# Convert 'zd_sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_tmp[['FIPS','desc','value']]\n",
        "df_sfi=df_sfi.loc[df_sfi['desc']=='sfi']\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "df_a=df_tmp.merge(df_sfi, on='FIPS', how='outer')\n",
        "#print('\\ndf_a_3\\n',df_a.head())\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "df_a = pd.get_dummies(df_a, columns=['continent'])\n",
        "df_a.rename(columns={'continent_Africa':'zc_Africa_cont',\n",
        "             'continent_Asia':'zc_Asia_cont', \n",
        "             'continent_North America':'zc_N_Amer_cont', \n",
        "             'continent_Oceania':'zc_Oceania_cont', \n",
        "             'continent_South America':'zc_S_Amer_cont'},inplace=True)\n",
        "df_a = pd.get_dummies(df_a, columns=['region'])\n",
        "print(df_a.head(50))\n",
        "\n",
        "df_a.rename(columns={\n",
        "    'region_Northern Europe':'zc_N_Europe',  \n",
        "    'region_Southern Asia':'zc_S_Asia', \n",
        "    'region_Southern Europe':'zc_S_Europe', \n",
        "    'region_Western Africa':'zc_W_Africa', \n",
        "    'region_Western Asia':'zc_W_Asia', \n",
        "    'region_Australia and New Zealand':'zc_Australia_NZ', \n",
        "    'region_Caribbean':'zc_Caribbean', \n",
        "    'region_Central America':'zc_C_Amer', \n",
        "    'region_Central Asia':'zc_C_Asia', \n",
        "    'region_Eastern Africa':'zc_E_Africa', \n",
        "    'region_Eastern Asia':'zc_E_Asia', \n",
        "    'region_Eastern Europe':'zc_E_Europe', \n",
        "    'region_Melanesia':'zc_Melanesia', \n",
        "    'region_Micronesia':'zc_Micronesia', \n",
        "    'region_Middle Africa':'zc_M_Africa', \n",
        "    'region_Northern Africa':'zc_N_Africa', \n",
        "    'region_Northern America':'zc_N_Amer', \n",
        "    'region_Polynesia':'zc_Polynesia', \n",
        "    'region_South America':'zc_S_Amer', \n",
        "    'region_South-eastern Asia':'zc_SE_Asia', \n",
        "    'region_Southern Africa':'zc_S_Africa'},inplace=True)\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "\n",
        "iv_list=[['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zd_sfi', 'ze_Population (millions)'],\n",
        "         ['za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zd_sfi','ze_Population (millions)'],\n",
        "         ['zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zd_sfi','ze_Population (millions)']\n",
        "        ]\n",
        "print('DF_A\\n',df_a.head(),'\\nPREDICTORS\\n',predictors,'\\nIV_LIST\\n',iv_list)\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_1.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "\n",
        "####################### \n",
        "# Descriptive Statistics\n",
        "#\n",
        "df_test=df_a[['FD','FI','FM','aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "              'za_IsikFrench','zb_BeckSettlerMortality','zd_sfi','ze_Population (millions)',\n",
        "              ]]\n",
        "#print(df_test.columns.to_list())\n",
        "\n",
        "corr_matrix=df_test.rcorr()\n",
        "print (corr_matrix)\n",
        "\n",
        "myfile = 'nCorr_Matrix.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "corr_matrix.to_excel(joined_path)\n",
        "\n",
        "my_res=univariate_metrics(df_test)\n",
        "my_descript=my_res.pivot(index=['desc'],columns=['Col_Name'],values=['value'])\n",
        "\n",
        "print(my_descript)\n",
        "\n",
        "myfile = 'nTable_2_1_descriptive_stats.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "my_descript.to_excel(joined_path)\n",
        "\n",
        "####################### \n",
        "#TEST ASSUMPTIONS\n",
        "\n",
        "'''for curr_dv in predictors:\n",
        "    df_loop=df_a.loc[df_a['desc']==curr_dv].copy(deep=True)\n",
        "    Y = df_loop['value']\n",
        "    X = df_a[['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew']]\n",
        "    X = sm.add_constant(X)  \n",
        "    LinearRegression.fit(X=X,y=Y)\n",
        "    assumption_check = linear_regression_assumptions(X, Y)\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PJJkvjQuOGEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I7FY_PWgeKF"
      },
      "source": [
        "##Regression Table 2.2: relig %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa_wjKt-go_T"
      },
      "outputs": [],
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "df_tmp.reset_index(inplace=True)\n",
        "\n",
        "print(df_tmp.head())\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "\n",
        "# Calculate Institutions composite (zc_wgi_avg)\n",
        "\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t1=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t1=df_t1.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t1.rename(columns={'overall':'zc_wgi_avg'},inplace=True)\n",
        "df_t1.reset_index(inplace=True)\n",
        "#print(df_t1.head())\n",
        "\n",
        "##########new comment remove fd from filter\n",
        "##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')|(df_tmp['desc']=='openness')]\n",
        "\n",
        "df_sfi=df_tmp.loc[df_tmp['desc']=='sfi']\n",
        "df_sfi.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'zd_sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "\n",
        "#print(df_sfi.head())\n",
        "\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_tmp.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t1, on='FIPS', how='outer')\n",
        "#print('\\ndf_a_3\\n',df_a.head())\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "df_test=df_a.copy(deep=True)\n",
        "\n",
        "df_a = pd.get_dummies(df_a, columns=['continent'])\n",
        "df_a.rename(columns={'continent_Africa':'zc_Africa',\n",
        "             'continent_Asia':'zc_Asia', \n",
        "             'continent_North America':'zc_N_Amer', \n",
        "             'continent_Oceania':'zc_Oceania', \n",
        "             'continent_South America':'zc_S_Amer'},inplace=True)\n",
        "df_a = pd.get_dummies(df_a, columns=['region'])\n",
        "#print(df_a.columns.to_list())\n",
        "\n",
        "df_a.rename(columns={\n",
        "    'region_Northern Europe':'zc_N_Europe',  \n",
        "    'region_Southern Asia':'zc_S_Asia', \n",
        "    'region_Southern Europe':'zc_S_Europe', \n",
        "    'region_Western Africa':'zc_W_Africa', \n",
        "    'region_Western Asia':'zc_W_Asia', \n",
        "    'region_Australia and New Zealand':'zc_Australia_NZ', \n",
        "    'region_Caribbean':'zc_Caribbean', \n",
        "    'region_Central America':'zc_C_Amer', \n",
        "    'region_Central Asia':'zc_C_Asia', \n",
        "    'region_Eastern Africa':'zc_E_Africa', \n",
        "    'region_Eastern Asia':'zc_E_Asia', \n",
        "    'region_Eastern Europe':'zc_E_Europe', \n",
        "    'region_Melanesia':'zc_Melanesia', \n",
        "    'region_Micronesia':'zc_Micronesia', \n",
        "    'region_Middle Africa':'zc_M_Africa', \n",
        "    'region_Northern Africa':'zc_N_Africa', \n",
        "    'region_Northern America':'zc_N_Amer', \n",
        "    'region_Polynesia':'zc_Polynesia', \n",
        "    'region_South America':'zc_S_Amer', \n",
        "    'region_South-eastern Asia':'zc_SE_Asia', \n",
        "    'region_Southern Africa':'zc_S_Africa'},inplace=True)\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "iv_list=[['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%', 'CIANoRel%', \n",
        "          'CIAOthWithJew%'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%', 'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'zd_sfi', 'ze_Population (millions)'],\n",
        "         ['za_IsikFrench'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%', 'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%', 'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench','zd_sfi','ze_Population (millions)'],\n",
        "         ['zb_BeckSettlerMortality'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'zb_BeckSettlerMortality'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench','zb_BeckSettlerMortality'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "         ['zc_wgi_avg'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'zc_wgi_avg'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'zc_wgi_avg','zd_sfi','ze_Population (millions)'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench','zb_BeckSettlerMortality','zc_wgi_avg',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "        ]\n",
        "\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_2_relig_percent.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "####################### \n",
        "# Descriptive Statistics\n",
        "#\n",
        "df_test=df_a[['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench','zb_BeckSettlerMortality','zc_wgi_avg',\n",
        "          'zd_sfi','ze_Population (millions)']]\n",
        "#print(df_test.columns.to_list())\n",
        "my_res=univariate_metrics(df_test)\n",
        "my_descript=my_res.pivot(index=['desc'],columns=['Col_Name'],values=['value'])\n",
        "\n",
        "print(my_descript)\n",
        "\n",
        "myfile = 'nTable_2_2_descriptive_stats.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "my_descript.to_excel(joined_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Table 2.2 (religiosity interaction term) --NOT USED"
      ],
      "metadata": {
        "id": "hv5Ta4afcr3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['Financial Development Index',\n",
        "              'Financial Institutions Index',\n",
        "              'Financial Markets Index']\n",
        "'''\n",
        "\n",
        "'''\n",
        "df_tmp=df_religiosity.copy(deep=True)\n",
        "\n",
        "df_relig_intens=df_tmp.groupby(['country_relig'],as_index=False)['religiosity'].mean()\n",
        "df_relig_intens=df_relig_intens.transpose()\n",
        "\n",
        "print (df_relig_intens)\n",
        "'''\n",
        "'''df_tmp=df_merged.copy(deep=True)\n",
        "'''\n",
        "''' wgi_avg...\n",
        "# Generate data for dfs\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'wgi_avg'},inplace=True)\n",
        "print('\\n\\ndf_t2-wgi_avg',df_t2['wgi_avg'].mean())\n",
        "'''\n",
        "'''##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "#print (df_a.head())\n",
        "\n",
        "# Convert 'sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "ddf_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi & wgi_avg columns with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "'''\n",
        "'''\n",
        "print('\\n######',df_a.loc[df_a['desc']=='sfi']['value'].mean())\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "print('######',df_a['wgi_avg'].mean())\n",
        "'''\n",
        "#natural log transformations\n",
        "'''df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "'''\n",
        "'''#interaction terms (religiosity * religious dummy)\n",
        "df_a['ProtIntens']=df_a['aCIAProtAll']*df_a['religiosity']\n",
        "df_a['OrthIntens']=df_a['cCIAOrthAll']*df_a['religiosity']\n",
        "df_a['MusIntens']=df_a['eCIAMusAll']*df_a['religiosity']\n",
        "df_a['BudHindIntens']=df_a['hCIABudHindAll']*df_a['religiosity']\n",
        "df_a['NoRelIntens']=df_a['mCIANoRelAll']*df_a['religiosity']\n",
        "\n",
        "\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "iv_list=[['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%', 'CIANoRel%', \n",
        "          'CIAOthWithJew%','ProtIntens','OrthIntens','MusIntens','BudHindIntens','NoRelIntens'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%', 'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'zd_sfi', 'ze_Population (millions)'],\n",
        "         ['za_IsikFrench'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%', 'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%', 'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench','zd_sfi','ze_Population (millions)'],\n",
        "         ['zb_BeckSettlerMortality'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'zb_BeckSettlerMortality'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench','zb_BeckSettlerMortality'],\n",
        "         ['CIAProt%', 'CIAOrth%', 'CIAMus%', 'CIABudHind%',  'CIANoRel%', \n",
        "          'CIAOthWithJew%', 'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zd_sfi','ze_Population (millions)']\n",
        "        ]\n",
        "\n",
        "temp=[]\n",
        "for curr_fd in predictors:\n",
        "    # Only look at current FD value\n",
        "    df_loop=df_a.loc[df_a['desc']==curr_fd].copy(deep=True)\n",
        "    # define DV\n",
        "    Y = df_loop['value']  \n",
        "    print ('\\nnbr of countries in DV:',df_loop['FIPS'].nunique())\n",
        "\n",
        "    for curr_iv in iv_list:\n",
        "        X = df_loop[curr_iv]\n",
        "        X = sm.add_constant(X)  \n",
        "        pglm = pg.linear_regression(X, Y)\n",
        "        pglm=pglm[['names', 'coef','pval','r2']]\n",
        "        pglm=pglm.loc[pglm['names']!='Intercept']\n",
        "        \n",
        "        #assign pval designator\n",
        "        select_conditions = [\n",
        "            (pglm['pval'] <=.01),\n",
        "            ((pglm['pval'] <=.05)&(pglm['pval'] >.01)),\n",
        "            ((pglm['pval'] <=.1)&(pglm['pval'] >.05)),\n",
        "            (pglm['pval'] >.1)\n",
        "            ]\n",
        "        select_values = ['a', 'b', 'c','']\n",
        "        pglm['my_level'] = np.select(select_conditions, select_values)\n",
        "\n",
        "        pglm.drop(columns=['pval'],inplace=True)\n",
        "\n",
        "        #transpose data for output table\n",
        "        pglm=pglm.transpose()\n",
        "        temp.append(pglm.round(3))\n",
        "\n",
        "df_table_2_2_interaction = pd.concat(temp)\n",
        "print('df_table_2_2_interaction',df_table_2_2_interaction)\n",
        "print(X.rcorr())\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_2_interaction.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "df_table_2_2_interaction.to_excel(joined_path)\n",
        "'''\n",
        "\n",
        "\n",
        "######################################\n",
        "##### response variable = Financial index; --reset multi-valued index\n",
        "######################################\n",
        "'''df_a=a.copy(deep=True)\n",
        "df_a=df_a.fillna(0)\n",
        "df_a.reset_index(drop=False,inplace=True)\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "df_a_piv=np.round(df_a.pivot_table(index=['FIPS','country_relig'],columns=['desc'],values=['value']),3)\n",
        "df_a_piv.columns = df_a_piv.columns.droplevel(0)\n",
        "df_a=df_a_piv.merge(a,on='FIPS',how='outer')\n",
        "df_a = pd.get_dummies(df_a, columns=['country_relig'])\n",
        "print(df_a.columns.to_list())\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "QKO80Z9Ncos9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Table 2.3 --data from 1900"
      ],
      "metadata": {
        "id": "7ZUKV4Kk2ncA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900', 'Prot1900%', 'Orth1900%', 'Mus1900%', 'BudHind1900%', 'NoRel1900%', 'OthWithJew1900%', \n",
        "\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "# Generate data for dfs\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "#'Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900', 'Prot1900%', 'Orth1900%', 'Mus1900%', 'BudHind1900%', 'NoRel1900%', 'OthWithJew1900%', \n",
        "\n",
        "#assign 1900_country_relig designator\n",
        "select_conditions = [\n",
        "(df_tmp['Prot1900'] == 1),\n",
        "(df_tmp['Orth1900'] == 1),\n",
        "(df_tmp['Mus1900'] == 1),\n",
        "(df_tmp['BudHind1900'] == 1),\n",
        "(df_tmp['NoRel1900'] == 1),\n",
        "(df_tmp['OthWithJew1900'] == 1)\n",
        "]\n",
        "select_values = ['Prot', 'Ortho', 'Mus','BudHind', 'NoRel', 'Oth+Jew']\n",
        "df_tmp['1900_country_relig'] = np.select(select_conditions, select_values)\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "################\n",
        "# recalculate wgi_avg based on the religion of 1900? \n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','1900_country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'wgi_avg'},inplace=True)\n",
        "\n",
        "###df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi & wgi_avg columns with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "\n",
        "iv_list=[['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900'],\n",
        "         ['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'zd_sfi', 'ze_Population (millions)'],\n",
        "         ['za_IsikFrench'],\n",
        "         ['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'za_IsikFrench'],\n",
        "         ['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'za_IsikFrench','zd_sfi','ze_Population (millions)'],\n",
        "         ['zb_BeckSettlerMortality'],\n",
        "         ['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'zb_BeckSettlerMortality'],\n",
        "         ['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'za_IsikFrench','zb_BeckSettlerMortality'],\n",
        "         ['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "         ['wgi_avg'],\n",
        "         ['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'wgi_avg'],\n",
        "         ['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'wgi_avg','zd_sfi','ze_Population (millions)'],\n",
        "         ['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'za_IsikFrench','zb_BeckSettlerMortality','wgi_avg','zd_sfi','ze_Population (millions)'],\n",
        "        ]\n",
        "\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_3_1900.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "####################### \n",
        "# Descriptive Statistics\n",
        "#\n",
        "df_test=df_a[['Prot1900', 'Orth1900', 'Mus1900', 'BudHind1900', 'NoRel1900', 'OthWithJew1900',\n",
        "          'za_IsikFrench','zb_BeckSettlerMortality','wgi_avg','zd_sfi','ze_Population (millions)']]\n",
        "#print(df_test.columns.to_list())\n",
        "my_res=univariate_metrics(df_test)\n",
        "my_descript=my_res.pivot(index=['desc'],columns=['Col_Name'],values=['value'])\n",
        "\n",
        "print(my_descript)\n",
        "\n",
        "myfile = 'nTable_2_3_descriptive_stats.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "my_descript.to_excel(joined_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Svyw92EU2iZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regress Table 2.4 High religiosity & 2.5, Low religiosity using NoRel% median"
      ],
      "metadata": {
        "id": "BAPo5UmUdir-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df_NoRel_avg\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "try:\n",
        "    df_a.rename(columns={'Population (millions)':'ze_Population (millions)'},inplace=True)\n",
        "except: \n",
        "    pass\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "iv_list=[['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zd_sfi', 'ze_Population (millions)'],\n",
        "         ['za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zd_sfi','ze_Population (millions)'],\n",
        "         ['zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zd_sfi','ze_Population (millions)']\n",
        "        ]\n",
        "#to create table for each quartile... \n",
        "#my_vals = ['lo_25', 'himed_25', 'hi_25', 'lomed_25']\n",
        "#for curr_quartile in my_vals:\n",
        "#    my_FIPS=df_NoRel_avg.loc[df_NoRel_avg['bin_CIANoRel_per_freq']==curr_quartile]['FIPS'].unique()\n",
        "\n",
        "##############\n",
        "#table 2.4 is high religiosity which is low NoRel%\n",
        "\n",
        "#Only consider NoRel% greater than zero\n",
        "#df_NoRel_avg=df_NoRel_avg.loc[df_NoRel_avg['CIANoRel_per']>0]\n",
        "#\n",
        "my_FIPS=df_NoRel_avg.loc[(df_NoRel_avg['CIANoRel_per']<df_NoRel_avg['CIANoRel_per'].median())]['FIPS']\n",
        "df_tmp0=df_a.loc[df_a['FIPS'].isin(my_FIPS)].copy(deep=True)\n",
        "print('\\nmy_FIPS\\n',df_tmp0['FIPS'].unique())\n",
        "piv = regress_table(df_tmp0, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_4_High_Religiosity.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "###################\n",
        "#Table 2.5 Low religiosity \n",
        "#\n",
        "my_FIPS=df_NoRel_avg.loc[df_NoRel_avg['CIANoRel_per']>=df_NoRel_avg['CIANoRel_per'].median()]['FIPS']\n",
        "df_tmp0=df_a.loc[df_a['FIPS'].isin(my_FIPS)].copy(deep=True)\n",
        "print('\\nmy_FIPS\\n',df_tmp0['FIPS'].unique())\n",
        "piv = regress_table(df_tmp0, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_5_Low_Religiosity.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DiIWmH5hGv1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Table 2.4 --high_religiosity only (EVS/WVS)"
      ],
      "metadata": {
        "id": "i9J-mCCe8xET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "#only include high religiosity\n",
        "\n",
        "my_FIPS=df_religiosity.loc[df_religiosity['religiosity']>=df_religiosity['religiosity'].median()]['FIPS'].unique()\n",
        "\n",
        "df_tmp=df_tmp.loc[df_tmp['FIPS'].isin(my_FIPS)]\n",
        "\n",
        "print(df_tmp['FIPS'].unique())\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "\n",
        "# Calculate Institutions composite (wgi_avg)\n",
        "\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'wgi_avg'},inplace=True)\n",
        "\n",
        "\n",
        "##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "\n",
        "iv_list=[['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zd_sfi', 'ze_Population (millions)'],\n",
        "         ['za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zd_sfi','ze_Population (millions)'],\n",
        "         ['zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "         ['wgi_avg'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'wgi_avg'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'wgi_avg','zd_sfi','ze_Population (millions)'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality','wgi_avg','zd_sfi','ze_Population (millions)'],\n",
        "        ]\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_4_hi_wvs.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "l95iQc7EezQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Table 2.5 --low religiosity data (EVS/WVS)"
      ],
      "metadata": {
        "id": "JWMG6sxaCjmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "#only include low religiosity\n",
        "\n",
        "my_FIPS=df_religiosity.loc[df_religiosity['religiosity']<df_religiosity['religiosity'].median()]['FIPS'].unique()\n",
        "\n",
        "df_tmp=df_tmp.loc[df_tmp['FIPS'].isin(my_FIPS)]\n",
        "\n",
        "print(df_tmp['FIPS'].unique())\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "\n",
        "# Calculate Institutions composite (wgi_avg)\n",
        "\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'wgi_avg'},inplace=True)\n",
        "\n",
        "\n",
        "##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "\n",
        "iv_list=[['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zd_sfi', 'ze_Population (millions)'],\n",
        "         ['za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zd_sfi','ze_Population (millions)'],\n",
        "         ['zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "         ['wgi_avg'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'wgi_avg'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'wgi_avg','zd_sfi','ze_Population (millions)'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality','wgi_avg','zd_sfi','ze_Population (millions)'],\n",
        "        ]\n",
        "\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_5_lo_wvs.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "4r6DxO0O80c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Table 2.6 --LN(openness)"
      ],
      "metadata": {
        "id": "o4pxD9QhJy2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "# Calculate Institutions composite (zc_wgi_avg)\n",
        "#df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')|(df_tmp['desc']=='openness')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'zd_sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "# Convert 'openness' from 'desc' value to a column value, then merge with df_loop\n",
        "df_t2=df_a.loc[df_a['desc']=='openness'].copy(deep=True)\n",
        "df_t2=df_t2[['FIPS','desc','value']]\n",
        "df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_t2=df_t2.pivot(index='FIPS', columns='desc',values='value')\n",
        "df_t2.rename(columns={'openness':'zc_openness'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a['zc_openness']=np.log(df_a['zc_openness'])\n",
        "\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "iv_list=[['zc_openness'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_openness'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_openness','zd_sfi','ze_Population (millions)'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality','zc_openness','zd_sfi','ze_Population (millions)'],\n",
        "        ]\n",
        "\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_6_openness.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kP6_RKPcJ3mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Table 2.7 --Openness-religion interaction term"
      ],
      "metadata": {
        "id": "7UoldlRSQS2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "# Calculate Institutions composite (zc_wgi_avg)\n",
        "#df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')|(df_tmp['desc']=='openness')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'zd_sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "# Convert 'openness' from 'desc' value to a column value, then merge with df_loop\n",
        "df_t2=df_a.loc[df_a['desc']=='openness'].copy(deep=True)\n",
        "df_t2=df_t2[['FIPS','desc','value']]\n",
        "df_t2.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_t2=df_t2.pivot(index='FIPS', columns='desc',values='value')\n",
        "df_t2.rename(columns={'openness':'zc_openness'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a['zc_openness']=np.log(df_a['zc_openness'])\n",
        "\n",
        "#add interaction terms\n",
        "df_a['xa_open_Prot']=df_a['aCIAProtAll']*df_a['zc_openness']\n",
        "df_a['xb_open_Cath']=df_a['bCIACathAll']*df_a['zc_openness']\n",
        "df_a['xc_open_Ortho']=df_a['cCIAOrthAll']*df_a['zc_openness']\n",
        "df_a['xd_open_Mus']=df_a['eCIAMusAll']*df_a['zc_openness']\n",
        "df_a['xe_open_BudHind']=df_a['hCIABudHindAll']*df_a['zc_openness']\n",
        "df_a['xf_open_NoRel']=df_a['mCIANoRelAll']*df_a['zc_openness']\n",
        "\n",
        "\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "iv_list=[['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'xa_open_Prot','xb_open_Cath','xc_open_Ortho','xd_open_Mus','xe_open_BudHind','xf_open_NoRel',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "        ]\n",
        "\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv.transpose())\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_7_openness_interaction.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.transpose().to_excel(joined_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8DXXS4yaQcQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Table 2.8 --Latitude"
      ],
      "metadata": {
        "id": "xsSmnptlD7Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "# Calculate Institutions composite (zc_wgi_avg)\n",
        "\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'zc_wgi_avg'},inplace=True)\n",
        "\n",
        "\n",
        "##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'zd_sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "df_a.rename(columns={'AbsLatitude':'zc_AbsLatitude'},inplace=True)\n",
        "#create list of lists for regression IV's\n",
        "#latitude\n",
        "iv_list=[['zc_AbsLatitude'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_AbsLatitude'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_AbsLatitude','zd_sfi','ze_Population (millions)'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality','zc_AbsLatitude','zd_sfi','ze_Population (millions)'],\n",
        "        ]\n",
        "\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_8_latitude.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "y_96FdbYD-Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Table 2.9 --continent"
      ],
      "metadata": {
        "id": "MWHdP2ZNGxaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'continents: 'Asia' 'Africa' 'South America' 'North America' 'Europe' 'Oceania' 0\n",
        "#'continent_Africa', 'continent_Asia', 'continent_North America', 'continent_Oceania', 'continent_South America'\n",
        "\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "\n",
        "\n",
        "# Calculate Institutions composite (zc_wgi_avg)\n",
        "\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'zc_wgi_avg'},inplace=True)\n",
        "\n",
        "\n",
        "##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'zd_sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "\n",
        "#my_cont=df_a['continent'].unique()\n",
        "#print(my_cont)\n",
        "df_a = pd.get_dummies(df_a, columns=['continent'])\n",
        "\n",
        "df_a.rename(columns={'continent_Africa':'zc_Africa',\n",
        "             'continent_Asia':'zc_Asia', \n",
        "             'continent_North America':'zc_N_Amer', \n",
        "             'continent_Oceania':'zc_Oceania', \n",
        "             'continent_South America':'zc_S_Amer'},inplace=True)\n",
        "\n",
        "#'zc_Africa','zc_Asia', 'zc_N_Amer','zc_Oceania','zc_S_Amer'\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "\n",
        "iv_list=[['zc_Africa','zc_Asia', 'zc_N_Amer','zc_Oceania','zc_S_Amer'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_Africa','zc_Asia', 'zc_N_Amer','zc_Oceania','zc_S_Amer'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_Africa','zc_Asia', 'zc_N_Amer','zc_Oceania','zc_S_Amer',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zc_Africa','zc_Asia', 'zc_N_Amer','zc_Oceania','zc_S_Amer',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "        ]\n",
        "\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_9_Continent.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LnB4936fGz1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Table 2.10 --region\n",
        "\n"
      ],
      "metadata": {
        "id": "SH8C776e5Cx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_tmp=df_merged.copy(deep=True)\n",
        "\n",
        "# Build linear regression model \n",
        "# Split data into predictors X and output Y\n",
        "predictors = ['FD','FI','FM']\n",
        "# Calculate Institutions composite (zc_wgi_avg)\n",
        "avg_col_lst = ['Voice and Accountability: Estimate',\n",
        "               'Political Stability and Absence of Violence/Terrorism: Estimate',\n",
        "               'Government Effectiveness: Estimate',\n",
        "               'Regulatory Quality: Estimate',\n",
        "               'Rule of Law: Estimate',\n",
        "               'Control of Corruption: Estimate']\n",
        "\n",
        "\n",
        "#limit the values in the desc column to calc wgi mean-values \n",
        "df_t2=df_tmp.loc[(df_tmp['desc'].isin(avg_col_lst))].copy(deep=True)\n",
        "\n",
        "df_t2=df_t2.groupby(['FIPS','country_relig'],as_index=False)['overall'].mean()\n",
        "df_t2.rename(columns={'overall':'zc_wgi_avg'},inplace=True)\n",
        "\n",
        "##df_a=df_tmp.loc[(df_tmp['desc'].isin(predictors))|(df_tmp['desc']=='sfi')]\n",
        "df_a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "# Convert 'zd_sfi' from 'desc' value to a column value, then merge with df_loop\n",
        "df_sfi=df_a.loc[df_a['desc']=='sfi'].copy(deep=True)\n",
        "df_sfi=df_sfi[['FIPS','desc','value']]\n",
        "df_sfi.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df_sfi=df_sfi.pivot(index='FIPS', columns='desc',values='value')\n",
        "df_sfi.rename(columns={'sfi':'zd_sfi'},inplace=True)\n",
        "\n",
        "#merge sfi column with FD values df \n",
        "df_a=df_a.merge(df_sfi, on='FIPS', how='outer')\n",
        "df_a=df_a.merge(df_t2, on='FIPS', how='outer')\n",
        "\n",
        "#natural log transformations\n",
        "df_a['ze_Population (millions)']=np.log(df_a['ze_Population (millions)'])\n",
        "df_a['zb_BeckSettlerMortality']=np.log(df_a['zb_BeckSettlerMortality'])\n",
        "df_a.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_a=df_a.fillna(0)\n",
        "\n",
        "df_a = pd.get_dummies(df_a, columns=['region'])\n",
        "#print(df_a.columns.to_list())\n",
        "\n",
        "df_a.rename(columns={\n",
        "    'region_Northern Europe':'zc_N_Europe',  \n",
        "    'region_Southern Asia':'zc_S_Asia', \n",
        "    'region_Southern Europe':'zc_S_Europe', \n",
        "    'region_Western Africa':'zc_W_Africa', \n",
        "    'region_Western Asia':'zc_W_Asia', \n",
        "    'region_Australia and New Zealand':'zc_Australia_NZ', \n",
        "    'region_Caribbean':'zc_Caribbean', \n",
        "    'region_Central America':'zc_C_Amer', \n",
        "    'region_Central Asia':'zc_C_Asia', \n",
        "    'region_Eastern Africa':'zc_E_Africa', \n",
        "    'region_Eastern Asia':'zc_E_Asia', \n",
        "    'region_Eastern Europe':'zc_E_Europe', \n",
        "    'region_Melanesia':'zc_Melanesia', \n",
        "    'region_Micronesia':'zc_Micronesia', \n",
        "    'region_Middle Africa':'zc_M_Africa', \n",
        "    'region_Northern Africa':'zc_N_Africa', \n",
        "    'region_Northern America':'zc_N_Amer', \n",
        "    'region_Polynesia':'zc_Polynesia', \n",
        "    'region_South America':'zc_S_Amer', \n",
        "    'region_South-eastern Asia':'zc_SE_Asia', \n",
        "    'region_Southern Africa':'zc_S_Africa'},inplace=True)\n",
        "\n",
        "# 'zc_N_Europe', 'zc_S_Asia', 'zc_S_Europe','zc_W_Africa', 'zc_W_Asia', 'zc_Australia_NZ','zc_Caribbean','zc_C_Amer', 'zc_C_Asia', 'zc_E_Africa', 'zc_E_Asia', 'zc_E_Europe', 'zc_Melanesia','zc_Micronesia','zc_M_Africa', 'zc_N_Africa','zc_N_Amer', 'zc_Polynesia','zc_S_Amer', 'zc_SE_Asia', 'zc_S_Africa',\n",
        "\n",
        "#create list of lists for regression IV's\n",
        "\n",
        "iv_list=[['zc_N_Europe', 'zc_S_Asia', 'zc_S_Europe','zc_W_Africa', 'zc_W_Asia', 'zc_Australia_NZ','zc_Caribbean','zc_C_Amer', 'zc_C_Asia', 'zc_E_Africa', 'zc_E_Asia', 'zc_E_Europe', 'zc_Melanesia','zc_Micronesia','zc_M_Africa', 'zc_N_Africa','zc_N_Amer', 'zc_Polynesia','zc_S_Amer', 'zc_SE_Asia', 'zc_S_Africa'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_N_Europe', 'zc_S_Asia', 'zc_S_Europe','zc_W_Africa', 'zc_W_Asia', 'zc_Australia_NZ','zc_Caribbean','zc_C_Amer', 'zc_C_Asia', 'zc_E_Africa', 'zc_E_Asia', 'zc_E_Europe', 'zc_Melanesia','zc_Micronesia','zc_M_Africa', 'zc_N_Africa','zc_N_Amer', 'zc_Polynesia','zc_S_Amer', 'zc_SE_Asia', 'zc_S_Africa'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'zc_N_Europe', 'zc_S_Asia', 'zc_S_Europe','zc_W_Africa', 'zc_W_Asia', 'zc_Australia_NZ','zc_Caribbean','zc_C_Amer', 'zc_C_Asia', 'zc_E_Africa', 'zc_E_Asia', 'zc_E_Europe', 'zc_Melanesia','zc_Micronesia','zc_M_Africa', 'zc_N_Africa','zc_N_Amer', 'zc_Polynesia','zc_S_Amer', 'zc_SE_Asia', 'zc_S_Africa',\n",
        "          'zd_sfi','ze_Population (millions)'],\n",
        "         ['aCIAProtAll', 'cCIAOrthAll', 'eCIAMusAll', 'hCIABudHindAll', 'mCIANoRelAll', 'kCIAOthWithJew', \n",
        "          'za_IsikFrench','zb_BeckSettlerMortality',\n",
        "          'zc_N_Europe', 'zc_S_Asia', 'zc_S_Europe','zc_W_Africa', 'zc_W_Asia', 'zc_Australia_NZ','zc_Caribbean','zc_C_Amer', 'zc_C_Asia', 'zc_E_Africa', 'zc_E_Asia', 'zc_E_Europe', 'zc_Melanesia','zc_Micronesia','zc_M_Africa', 'zc_N_Africa','zc_N_Amer', 'zc_Polynesia','zc_S_Amer', 'zc_SE_Asia', 'zc_S_Africa',\n",
        "          'zd_sfi','ze_Population (millions)']\n",
        "        ]\n",
        "\n",
        "piv = regress_table(df_a, predictors, iv_list)\n",
        "print('\\npiv\\n:',piv)\n",
        "\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/tables' \n",
        "myfile = 'nTable_2_10_UNregion.xlsx'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "piv.to_excel(joined_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "egqmBbsJ49vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnWvtY3p2zgB"
      },
      "source": [
        "##Classification (pycaret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVVGvB_AZjUe"
      },
      "outputs": [],
      "source": [
        "predictors = ['Financial Development Index',\n",
        "              'Financial Institutions Index',\n",
        "              'Financial Institutions Depth Index',\n",
        "              'Financial Institutions Access Index', \n",
        "              'Financial Institutions Efficiency Index',\n",
        "              'Financial Markets Index',\n",
        "              'Financial Markets Depth Index', \n",
        "              'Financial Markets Access Index', \n",
        "              'Financial Markets Efficiency Index']\n",
        "                    \n",
        "\n",
        "# Generate data for dfs\n",
        "my_rule_law_lst=['IsikCommon', 'IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed']\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','IsikCommon', 'IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil']\n",
        "\n",
        "a=df_merged.loc[df_merged['desc'].isin(fd_list)].copy(deep=True)\n",
        "a.rename(columns={'overall':'value'},inplace=True)\n",
        "\n",
        "X=a[['country_relig','desc','IsikCommon','IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', \n",
        "          ('value', 'Financial Development Index'), \n",
        "          ('value', 'Financial Institutions Index'), \n",
        "          ('value', 'Financial Institutions Depth Index'), \n",
        "          ('value', 'Financial Institutions Access Index'), \n",
        "          ('value', 'Financial Institutions Efficiency Index'),\n",
        "          ('value', 'Financial Markets Index'),\n",
        "          ('value', 'Financial Markets Depth Index'), \n",
        "          ('value', 'Financial Markets Access Index'), \n",
        "          ('value', 'Financial Markets Efficiency Index')]]\n",
        "\n",
        "my_model = setup(data=X, # we are setting up the model using the training dataset\n",
        "                target='desc', # this is the response variable (since this is a classification model, the target should be a categorical feature)\n",
        "                numeric_imputation='mean', #this parameter says to replace NULL values with the mean of a column\n",
        "                #ignore_features = ['class', 'who', 'adult_male', 'alive', 'embarked', 'alone', 'deck'], #ignore the duplicate features and features with excessive NULLs\n",
        "                #categorical_features = ['sex', 'embark_town', 'pclass'],\n",
        "                #numeric_features = ['age', 'fare', 'sibsp', 'parch'],\n",
        "                silent=True \n",
        "                )\n",
        "compare_models()\n",
        "my_lr_mod=create_model('lr')\n",
        "#evaluate_model(my_lr_mod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGMydmAo27yA"
      },
      "outputs": [],
      "source": [
        "predictors = ['Financial Development Index',\n",
        "              'Financial Institutions Index',\n",
        "              'Financial Institutions Depth Index',\n",
        "              'Financial Institutions Access Index', \n",
        "              'Financial Institutions Efficiency Index',\n",
        "              'Financial Markets Index',\n",
        "              'Financial Markets Depth Index', \n",
        "              'Financial Markets Access Index', \n",
        "              'Financial Markets Efficiency Index']\n",
        "\n",
        "# Generate data for dfs\n",
        "my_rule_law_lst=['IsikCommon', 'IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed']\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','IsikCommon', 'IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil']\n",
        "\n",
        "a=df_merged.loc[df_merged['desc'].isin(fd_list)].copy(deep=True)\n",
        "a.rename(columns={'overall':'value'},inplace=True)\n",
        "a_piv=np.round(a.pivot_table(index=['FIPS','country_relig'],columns=['desc'],values=['value']),3)\n",
        "print ('before', a_piv.head())\n",
        "\n",
        "#a_piv.columns = a_piv.columns.map(''.join)\n",
        "a_piv.columns = a_piv.columns.droplevel(0)\n",
        "print ('after', a_piv.head())\n",
        "\n",
        "#a_piv.columns.set_levels('a','b','c',level=1,inplace=True)\n",
        "a=a_piv.merge(a,on='FIPS',how='outer')\n",
        "#a.rename(columuns=('value', 'Financial Institutions Index'),\n",
        "\n",
        "print(a.head(10))\n",
        "\n",
        "\n",
        "X=a[['country_relig','desc','IsikCommon','IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'Financial Development Index',\n",
        "              'Financial Institutions Index',\n",
        "              'Financial Institutions Depth Index',\n",
        "              'Financial Institutions Access Index', \n",
        "              'Financial Institutions Efficiency Index',\n",
        "              'Financial Markets Index',\n",
        "              'Financial Markets Depth Index', \n",
        "              'Financial Markets Access Index', \n",
        "              'Financial Markets Efficiency Index']]\n",
        "\n",
        "my_model = setup(data=X, # we are setting up the model using the training dataset\n",
        "                target='Financial Development Index', # this is the response variable (since this is a classification model, the target should be a categorical feature)\n",
        "                numeric_imputation='mean', #this parameter says to replace NULL values with the mean of a column\n",
        "                #ignore_features = ['class', 'who', 'adult_male', 'alive', 'embarked', 'alone', 'deck'], #ignore the duplicate features and features with excessive NULLs\n",
        "                #categorical_features = ['sex', 'embark_town', 'pclass'],\n",
        "                #numeric_features = ['age', 'fare', 'sibsp', 'parch'],\n",
        "                silent=True \n",
        "                )\n",
        "compare_models()\n",
        "my_lr_mod=create_model('lr')\n",
        "#evaluate_model(my_lr_mod)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faNv2MJz4Gy9"
      },
      "outputs": [],
      "source": [
        "predictors = ['Financial Development Index',\n",
        "              'Financial Institutions Index',\n",
        "              'Financial Institutions Depth Index',\n",
        "              'Financial Institutions Access Index', \n",
        "              'Financial Institutions Efficiency Index',\n",
        "              'Financial Markets Index',\n",
        "              'Financial Markets Depth Index', \n",
        "              'Financial Markets Access Index', \n",
        "              'Financial Markets Efficiency Index']\n",
        "\n",
        "# Generate data for dfs\n",
        "my_rule_law_lst=['IsikCommon', 'IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed']\n",
        "\n",
        "my_col_list=['FIPS','country_relig','desc','overall','IsikCommon', 'IsikCivil', 'IsikFrench', 'IsikGerman', 'IsikNordic', 'IsikSoc', 'IsikMixed', 'IsikCivil']\n",
        "\n",
        "a=df_merged.loc[df_merged['desc'].isin(fd_list)].copy(deep=True)\n",
        "\n",
        "a_piv=np.round(a.pivot_table(index=['FIPS','country_relig'],columns=['desc'],values=['overall']),3)\n",
        "a=a_piv.merge(a,on='FIPS',how='outer')\n",
        "\n",
        "X=a[['country_relig','desc','IsikCommon']]\n",
        "\n",
        "my_model = setup(data=X, # we are setting up the model using the training dataset\n",
        "                target='IsikCommon', # this is the response variable (since this is a classification model, the target should be a categorical feature)\n",
        "                numeric_imputation='mean', #this parameter says to replace NULL values with the mean of a column\n",
        "                #ignore_features = ['class', 'who', 'adult_male', 'alive', 'embarked', 'alone', 'deck'], #ignore the duplicate features and features with excessive NULLs\n",
        "                #categorical_features = ['sex', 'embark_town', 'pclass'],\n",
        "                #numeric_features = ['age', 'fare', 'sibsp', 'parch'],\n",
        "                silent=True \n",
        "                )\n",
        "compare_models()\n",
        "my_lr_mod=create_model('lr')\n",
        "#evaluate_model(my_lr_mod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cuii1ST26pqo"
      },
      "outputs": [],
      "source": [
        "evaluate_model(my_lr_mod)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTYVpstU62E9"
      },
      "outputs": [],
      "source": [
        "print(result.coef_,'\\nitercept\\n', result.intercept_,'\\nsigma\\n', result.sigma_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUfoGCKfjGz9"
      },
      "source": [
        "#multiple t-tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJI3rHhuLUi2"
      },
      "outputs": [],
      "source": [
        "print(stop program here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ry5yUYYonaD"
      },
      "outputs": [],
      "source": [
        "#REFRESH the data for the counts\n",
        "c=pd.melt(df_iset_subset,id_vars=['country','FIPS','desc'])\n",
        "c.rename(columns={'variable':'year'},inplace=True)\n",
        "\n",
        "#Merge with religion data\n",
        "b=c.merge(df_all_religions,on='FIPS',how='outer')\n",
        "a=b.merge(df_country,on='FIPS',how='outer')\n",
        "a.rename(columns={'country_x':'country'},inplace=True)\n",
        "a=a.loc[~a['value'].isnull()]\n",
        "a=a.loc[a['year']=='overall']\n",
        "#print (a.head(100))\n",
        "\n",
        "print (a.head())\n",
        "\n",
        "#Use relig_list from code-block that creates df_all_religions\n",
        "#Create permutation of religions\n",
        "perm_set=list( permutations( relig_list, 2 ) )\n",
        "\n",
        "#Convert to set to eliminate redundant pairs\n",
        "perm_set=set(perm_set)\n",
        "\n",
        "#Convert back to a list for ease of use\n",
        "for x in perm_set:\n",
        "    relig_pair=list(x)\n",
        "    #relig_pair[0] vs. relig_pair[1] for t-test\n",
        "\n",
        "    # 2-sample t-test for each DV variable\n",
        "    collst=predictors #a['desc'].unique()\n",
        "\n",
        "    for curr_col in collst:\n",
        "        \n",
        "        # Dataframes to use for statistical tests\n",
        "        df_a=a.loc[(a['desc']==curr_col) & (a['country_relig']==relig_pair[0]) & (a['year']=='overall')].copy(deep=True)\n",
        "        df_b=a.loc[(a['desc']==curr_col) & (a['country_relig']==relig_pair[1]) & (a['year']=='overall')].copy(deep=True)\n",
        "\n",
        "        #perform Bartlett's test homogeneity of variance\n",
        "        stat, p = stats.bartlett(df_a['value'], df_b['value'])\n",
        "        print(\"H0 is equal variances; Barlett's p-value\",p, curr_col)\n",
        "        \n",
        "        # Levene's Test in Python with Scipy:\n",
        "        stat, p = stats.levene(df_a['value'], df_b['value'])\n",
        "        print(\"H0 is equal variances; Levene's p-value (more robust to Normal distrib violations)\",p, curr_col)\n",
        "\n",
        "        ####### p-value ~since we are looking to see if OIC is less than the non-OIC, we only need one-tail. Since scipy give 2-tail, we can divide p by 2\n",
        "\n",
        "        if p > .05: # p value from Levene homoscedasticity test\n",
        "            t_stat, p_val = stats.ttest_ind(df_a['value'], df_b['value'], equal_var=True)\n",
        "            #p_val=p_val/2 #one-tail test\n",
        "            if p_val < .001:\n",
        "                print('***',curr_col,'for','\\n     ',relig_pair[0], 'compared to',relig_pair[1])\n",
        "            elif p_val < .01:\n",
        "                print('**',curr_col,'for','\\n     ',relig_pair[0], 'compared to',relig_pair[1])\n",
        "            elif p_val < .05:\n",
        "                    print('*',curr_col,'for','\\n     ',relig_pair[0], 'compared to',relig_pair[1])\n",
        "            else:\n",
        "                print('#',curr_col,'for','\\n     ',relig_pair[0], 'compared to',relig_pair[1], 'is not significantly different')\n",
        "        else:\n",
        "            print('NEED to do ttest equiv for heteroscedasticity for', relig_pair)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvX0tf22otNU"
      },
      "source": [
        "#Multiple comparision of means Tukey (df_tukey_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk9nY07temJx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use MOD function to determine which 'desc' value is being evaluated\n",
        "uniq_desc_num=df_iset['desc'].nunique()\n",
        "uniq_desc_list=df_iset['desc'].unique()\n",
        "\n",
        "#make long\n",
        "a=pd.melt(df_iset,id_vars=['country','FIPS','desc'])\n",
        "a.rename(columns={'variable':'year'},inplace=True)\n",
        "\n",
        "a=a.merge(df_all_religions,on='FIPS',how='outer')\n",
        "a.rename(columns={'country_x':'country'},inplace=True)\n",
        "a=a.loc[~a['value'].isnull()]\n",
        "print(a.head())\n",
        "'''\n",
        "a['dupe']=a.duplicated()\n",
        "print ('DUPES:\\n', a.loc[a['dupe']==True], '\\nhead()',a.head())\n",
        "'''\n",
        "\n",
        "desc_key=[]\n",
        "decade_key=[]\n",
        "atables=[]\n",
        "ttables=[]\n",
        "df_tukey_results=[]\n",
        "\n",
        "# iterate over each group\n",
        "my_decades_list=['overall','1980_89','1990_99','2000_09','2010_20']\n",
        "for decade in my_decades_list: \n",
        "    for curr_desc in uniq_desc_list:\n",
        "        my_endog = a.loc[(a['desc']==curr_desc) & (a['year']==decade)]['value']\n",
        "        my_endog=pd.to_numeric(my_endog, errors='coerce')\n",
        "        my_endog=my_endog.dropna()\n",
        "        my_curr_grp = a.loc[(a['desc']==curr_desc) & (a['year']==decade)]['country_relig']\n",
        "        my_curr_grp=my_curr_grp.dropna()\n",
        "\n",
        "        #model=ols('{} ~ country_relig'.format(curr_desc), data=my_endog).fit() #added 'overall' column\n",
        "        #anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "        exclude_lst=['taoism','druze','Poverty headcount ratio at national poverty lines (% of population)']\n",
        "        if ((my_endog.size == my_curr_grp.size) & (my_endog.size>2) & (curr_desc not in exclude_lst)):\n",
        "            '''            \n",
        "            if curr_desc in exclude_lst:\n",
        "                continue\n",
        "            else:            \n",
        "            '''\n",
        "            tukey_results = pairwise_tukeyhsd(endog=my_endog,\n",
        "                                    groups=my_curr_grp,\n",
        "                                    alpha=0.1)\n",
        "            desc_key.append(curr_desc)\n",
        "            decade_key.append(decade)\n",
        "            #atables.append(anova_table)\n",
        "            ttables.append(tukey_results)\n",
        "\n",
        "            #print(anova_table)\n",
        "            print (decade, 'for', currcol,'\\nTukey HSD H0 = means are equal\\n')\n",
        "            #print(tukey_results) #this prints the results from the tukey class object\n",
        "\n",
        "            #To save tukey results into a dataframe...\n",
        "            #https://stackoverflow.com/questions/40516810/saving-statmodels-tukey-hsd-into-a-python-pandas-dataframe\n",
        "            df_tukey_results.append(pd.DataFrame(data=tukey_results._results_table.data[1:], columns=tukey_results._results_table.data[0]))\n",
        "df_tukey_results=pd.DataFrame(df_tukey_results)\n",
        "print(df_tukey_results.info(verbose=True)) #this prints the results from the pd.DataFrame()\n",
        "\n",
        "    #Save df_iset, df_demograph, df_incgrp to disk\n",
        "\n",
        "#df_ttables = pd.DataFrame(data=ttables.data[1:], columns=ttables.data[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzzV3EIMpFV4"
      },
      "source": [
        "#Create Tukey plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnm9jwnArKkC"
      },
      "outputs": [],
      "source": [
        "from pandas.api.types import is_string_dtype\n",
        "\n",
        "print('\\nTukey plots KEY:\\n   Dot = mean for group; Bar=90% CI; Tukey HSD is based on CI; Blue=Reference Group; Red=Different than Reference Group\\n\\n')\n",
        "i=0\n",
        "while i < len(ttables):\n",
        "    my_label = desc_key[i]+'_'+decade_key[i]\n",
        "    '''    # create xlabel for each figure\n",
        "    if uniq_desc_num > 0:\n",
        "        curr_decade=my_decades_list[i//uniq_desc_num] \n",
        "        curr_desc=uniq_desc_list[np.mod(i,uniq_desc_num)]\n",
        "    else:\n",
        "        curr_decade=np.nan\n",
        "        curr_desc=np.nan\n",
        "\n",
        "    if(isinstance(curr_decade, type('str')) & isinstance(curr_desc, type('str'))):\n",
        "        my_label=my_decades_list[i//uniq_desc_num]+'_'+uniq_desc_list[np.mod(i,uniq_desc_num)]\n",
        "    else:\n",
        "        my_label='None'\n",
        "        print('no label for decade=',curr_decade, type(curr_decade), 'and desc=', curr_desc, type(curr_desc))\n",
        "    '''    \n",
        "    try:\n",
        "        ttables[i].plot_simultaneous(comparison_name='CIAProtAll',xlabel=my_label)\n",
        "    except:\n",
        "        ttables[i].plot_simultaneous(xlabel=my_label)\n",
        "    \n",
        "    my_filename = ''.join(char for char in my_label if char.isalnum())\n",
        "    mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/TukeyPlots' \n",
        "    myfile = my_filename + '.pdf'\n",
        "    joined_path = os.path.join(mypath, myfile)\n",
        "    plt.savefig(joined_path)                  \n",
        "    i+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_9F9J1IqZBC"
      },
      "source": [
        "#Print country:religion assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjeOhhWnwEXB"
      },
      "outputs": [],
      "source": [
        "cols=df_all_religions.columns.to_list()\n",
        "print (cols)\n",
        "print('\\nTotal countries:',df_all_religions['country'].nunique())\n",
        "for curr_rel in relig_list:\n",
        "    a=df_all_religions.loc[df_all_religions['country_relig']==curr_rel]\n",
        "    #the following line of code removes duplicated columns in data\n",
        "    a = a.loc[:,~a.columns.duplicated()]\n",
        "    print(curr_rel,'Count:',a['country'].nunique() ,'\\n',a['country'].unique())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GOaNWOpt-Nj"
      },
      "source": [
        "#Pre-process data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmGHotbLKgUh"
      },
      "source": [
        "##KNN to replace NaN\n",
        "####naming convention:\n",
        "*  tmp=no KNN, not transformed\n",
        "*  i=inc_grp KNN\n",
        "*  t=transformed\n",
        "*  o=overall KNN\n",
        "\n",
        ">e.g.,\n",
        "\n",
        ">>\n",
        "*    tmp_OIC = no KNN, not transformed\n",
        "*    df_OIC_i = inc_grp KNN, not transformed\n",
        "*    df_OIC_o = overall KNN, not *transformed\n",
        "*    df_OIC_it = inc_grp KNN, transformed\n",
        "*    df_OIC_ot = overall KNN, transformed\n",
        "\n",
        "####  groups:\n",
        "*    OIC\n",
        "* not \n",
        "* all \n",
        "* OECD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSDIsyMyPdco"
      },
      "outputs": [],
      "source": [
        "####\n",
        "#\n",
        "#\n",
        "# Calculate the fillna(knn) with OIC & IncGrp & Region?\n",
        "# Generate boxplots by region too\n",
        "# Can I show plot by region & incGrp & OIC\n",
        "#\n",
        "#\n",
        "######\n",
        "#df2=df2.fillna({'Muslims':0})\n",
        "#'region', 'inc_grp', 'OECD'\n",
        "\n",
        "#create subsets of overall dataframe\n",
        "OIC_demo=df_demograph.loc[df_demograph['OIC']==1]\n",
        "notOIC_demo=df_demograph.loc[df_demograph['OIC']!=1]\n",
        "OECD_demo=df_demograph.loc[df_demograph['OECD']==1]\n",
        "#turn into dataframe\n",
        "OIC_countries = pd.DataFrame(OIC_demo['country'].unique(), columns = ['country'])\n",
        "notOIC_countries = pd.DataFrame(notOIC_demo['country'].unique(), columns = ['country'])\n",
        "OECD_countries = pd.DataFrame(OECD_demo['country'].unique(), columns = ['country'])\n",
        "all_countries = pd.DataFrame(df_demograph['country'].unique(), columns = ['country'])\n",
        "class_codes = df_demograph[['country','FIPS','region','inc_grp', 'OIC', 'OECD']]\n",
        "#merge with other data\n",
        "tmp_OIC=tmp.merge(OIC_countries,on='country',how='inner')\n",
        "tmp_OIC=tmp_OIC.merge(class_codes, on='country',how='inner')\n",
        "tmp_notOIC=tmp.merge(notOIC_countries,on='country',how='inner')\n",
        "tmp_notOIC=tmp_notOIC.merge(class_codes, on='country',how='inner')\n",
        "tmp_OECD=tmp.merge(OECD_countries,on='country',how='inner')\n",
        "tmp_OECD=tmp_OECD.merge(class_codes, on='country',how='inner')\n",
        "tmp_all=tmp.merge(all_countries, on='country',how='inner')\n",
        "tmp_all=tmp_all.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "'''\n",
        "KNN = fillna with imputed KNN value\n",
        "\n",
        "df naming convention:\n",
        "  tmp=no KNN, not transformed\n",
        "  i=inc_grp KNN\n",
        "  t=transformed\n",
        "  o=overall KNN\n",
        "\n",
        "  e.g.,\n",
        "    tmp_OIC = no KNN, not transformed\n",
        "    df_OIC_i = inc_grp KNN, not transformed\n",
        "    df_OIC_o = overall KNN, not transformed\n",
        "    df_OIC_it = inc_grp KNN, transformed\n",
        "    df_OIC_ot = overall KNN, transformed\n",
        "\n",
        "  groups:\n",
        "    OIC, not, all, OECD\n",
        "'''\n",
        "\n",
        "\n",
        "''' /////// overall & by inc_grp //////// '''\n",
        "df_OECD_o = tmp_OECD.copy(deep=True)\n",
        "df_all_o = tmp_all.copy(deep=True)\n",
        "df_OIC_o = tmp_OIC.copy(deep=True)\n",
        "df_not_o = tmp_notOIC.copy(deep=True)\n",
        "\n",
        "df_OIC_l = tmp_OIC.loc[tmp_OIC['inc_grp']=='Low income']\n",
        "df_OIC_lm = tmp_OIC.loc[tmp_OIC['inc_grp']=='Lower middle income']\n",
        "df_OIC_um = tmp_OIC.loc[tmp_OIC['inc_grp']=='Upper middle income']\n",
        "df_OIC_h = tmp_OIC.loc[tmp_OIC['inc_grp']=='High income']\n",
        "\n",
        "df_not_l = tmp_notOIC.loc[tmp_notOIC['inc_grp']=='Low income']\n",
        "df_not_lm = tmp_notOIC.loc[tmp_notOIC['inc_grp']=='Lower middle income']\n",
        "df_not_um = tmp_notOIC.loc[tmp_notOIC['inc_grp']=='Upper middle income']\n",
        "df_not_h = tmp_notOIC.loc[tmp_notOIC['inc_grp']=='High income']\n",
        "\n",
        "#instantiate new lists\n",
        "df_OIC_ot=df_not_ot=df_OECD_ot=df_all_ot=[]\n",
        "df_OIC_it=df_not_it=df_all_it=[]\n",
        "df_OIC_i=df_not_i=df_all_i=[]\n",
        "\n",
        "dfs=[df_OIC_l,df_OIC_lm, df_OIC_um, df_OIC_h,\n",
        "     df_not_l, df_not_lm, df_not_um, df_not_h,\n",
        "     df_OIC_o, df_not_o, df_OECD_o, df_all_o, \n",
        "     df_OIC_ot, df_not_ot, df_OECD_ot, df_all_ot, \n",
        "     df_OIC_i, df_not_i, df_all_i, \n",
        "     df_OIC_it, df_not_it, df_all_it, \n",
        "     tmp_OIC, tmp_notOIC, tmp_OECD, tmp_all\n",
        "    ]\n",
        "#setup a dict and lists for the dataframes\n",
        "dfs_dict={'df_OIC_l':df_OIC_l, 'df_OIC_lm':df_OIC_lm, 'df_OIC_um':df_OIC_um, 'df_OIC_h':df_OIC_h,\n",
        "     'df_not_l':df_not_l, 'df_not_lm':df_not_lm, 'df_not_um':df_not_um, 'df_not_h':df_not_h,\n",
        "     'df_OIC_o':df_OIC_o, 'df_not_o':df_not_o,'df_OECD_o':df_OECD_o, 'df_all_o':df_all_o, \n",
        "     'df_OIC_ot':df_OIC_ot, 'df_not_ot':df_not_ot,'df_OECD_ot':df_OECD_ot, 'df_all_ot':df_all_ot, \n",
        "     'df_OIC_i':df_OIC_i, 'df_not_i':df_not_i,'df_all_i':df_all_i, \n",
        "     'df_OIC_it':df_OIC_it, 'df_not_it':df_not_it,'df_all_it':df_all_it, \n",
        "     'tmp_OIC':tmp_OIC, 'tmp_notOIC':tmp_notOIC, 'tmp_OECD':tmp_OECD, 'tmp_all':tmp_all\n",
        "     }\n",
        "\n",
        "dflst=[df_OIC_l, df_OIC_lm, df_OIC_um, df_OIC_h,\n",
        "       df_not_l, df_not_lm, df_not_um, df_not_h,\n",
        "       df_OECD_o, df_all_o, df_OIC_o, df_not_o]\n",
        "\n",
        "# Convert to wide before transformations\n",
        "df_OIC_l=df_OIC_l.pivot_table(index='country', columns='desc', values='value')\n",
        "df_OIC_lm=df_OIC_lm.pivot_table(index='country', columns='desc', values='value')\n",
        "df_OIC_um=df_OIC_um.pivot_table(index='country', columns='desc', values='value')      \n",
        "df_OIC_h=df_OIC_h.pivot_table(index='country', columns='desc', values='value')\n",
        "df_not_l =df_not_l.pivot_table(index='country', columns='desc', values='value')\n",
        "df_not_lm =df_not_lm.pivot_table(index='country', columns='desc', values='value')\n",
        "df_not_um=df_not_um.pivot_table(index='country', columns='desc', values='value')\n",
        "df_not_h=df_not_h.pivot_table(index='country', columns='desc', values='value')\n",
        "df_all_o=df_all_o.pivot_table(index='country', columns='desc', values='value')\n",
        "df_OIC_o =df_OIC_o.pivot_table(index='country', columns='desc', values='value')\n",
        "df_not_o =df_not_o.pivot_table(index='country', columns='desc', values='value')\n",
        "df_OECD_o=df_OECD_o.pivot_table(index='country', columns='desc', values='value')\n",
        "tmp_all=tmp_all.pivot_table(index='country', columns='desc', values='value')\n",
        "tmp_OIC =tmp_OIC.pivot_table(index='country', columns='desc', values='value')\n",
        "tmp_notOIC =tmp_notOIC.pivot_table(index='country', columns='desc', values='value')\n",
        "tmp_OECD=tmp_OECD.pivot_table(index='country', columns='desc', values='value')\n",
        "\n",
        "df_univar_OIC = df_OIC_o.copy(deep=True)\n",
        "df_univar_not = df_not_o.copy(deep=True)\n",
        "df_univar_all = df_all_o.copy(deep=True)\n",
        "\n",
        "#KNN imputation for missing values by OIC income group and by Not_OIC income group\n",
        "imputer = KNNImputer(n_neighbors= 5) #use the 5 closest neighbors\n",
        "\n",
        "dfs_dict={'df_OIC_l':df_OIC_l, 'df_OIC_lm':df_OIC_lm, 'df_OIC_um':df_OIC_um, 'df_OIC_h':df_OIC_h}\n",
        "d=[] \n",
        "for curkey, curval in dfs_dict.items():\n",
        "  t=imputer.fit_transform(curval)\n",
        "  r=pd.DataFrame(t,columns=curval.columns,index=curval.index)\n",
        "  d.append(r)\n",
        "df_OIC_i = pd.concat(d) #answer to how to save dataframe inside loop, need to define d[] and to d.append during iteration\n",
        "df_OIC_i=df_OIC_i.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "#same loop for not, \n",
        "dfs_dict={'df_not_l':df_not_l, 'df_not_lm':df_not_lm, 'df_not_um':df_not_um, 'df_not_h':df_not_h}\n",
        "d=[] \n",
        "for curkey, curval in dfs_dict.items():\n",
        "  t=imputer.fit_transform(curval)\n",
        "  r=pd.DataFrame(t,columns=curval.columns,index=curval.index)\n",
        "  d.append(r)\n",
        "df_not_i = pd.concat(d) #answer to how to save dataframe inside loop, need to define d[] and to d.append during iteration\n",
        "df_not_i=df_not_i.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "df_all_i =pd.concat([df_not_i, df_OIC_i])\n",
        "\n",
        "dfs_dict={'df_OIC_o':df_OIC_o}\n",
        "d=[] \n",
        "for curkey, curval in dfs_dict.items():\n",
        "  t=imputer.fit_transform(curval)\n",
        "  r=pd.DataFrame(t,columns=curval.columns,index=curval.index)\n",
        "  d.append(r)\n",
        "df_OIC_o = pd.concat(d) #answer to how to save dataframe inside loop, need to define d[] and to d.append during iteration\n",
        "df_OIC_o=df_OIC_o.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "dfs_dict={'df_not_o':df_not_o}\n",
        "d=[] \n",
        "for curkey, curval in dfs_dict.items():\n",
        "  t=imputer.fit_transform(curval)\n",
        "  r=pd.DataFrame(t,columns=curval.columns,index=curval.index)\n",
        "  d.append(r)\n",
        "df_not_o = pd.concat(d) #answer to how to save dataframe inside loop, need to define d[] and to d.append during iteration\n",
        "df_not_o=df_not_o.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "dfs_dict={'df_OECD_o':df_OECD_o}\n",
        "d=[] \n",
        "for curkey, curval in dfs_dict.items():\n",
        "  t=imputer.fit_transform(curval)\n",
        "  r=pd.DataFrame(t,columns=curval.columns,index=curval.index)\n",
        "  d.append(r)\n",
        "df_OECD_o = pd.concat(d) #answer to how to save dataframe inside loop, need to define d[] and to d.append during iteration\n",
        "df_OECD_o=df_OECD_o.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "df_all_o=pd.concat([df_not_o, df_OIC_o])\n",
        "\n",
        "#print (df_all_o.info(verbose=True),'\\n',df_not_o.info(verbose=True))\n",
        "#print (df_all_o.tail(),'\\n',df_not_o.tail())\n",
        "#\n",
        "#\n",
        "####///////////////////SUCCESS!!\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
        "###//////////////how to save df in a loop\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
        "# see loop above this comment (before loop -> d=[], in loop -> d.append(new_list), after loop-> d=pd.concat(d))\n",
        "#\n",
        "\n",
        "'''\n",
        "# data transformation (could use pycaret to transform data)\n",
        "df_all_it = df_all_i\n",
        "df_OIC_it = df_OIC_i\n",
        "df_not_it = df_not_i\n",
        "df_all_ot = df_all_o\n",
        "df_OIC_ot = df_OIC_o\n",
        "df_not_ot = df_not_o\n",
        "df_OECD_ot = df_OECD_o\n",
        "\n",
        "dfs={'df_all_it':df_all_it, 'df_OIC_it':df_OIC_it, 'df_not_it':df_not_it, \n",
        "       'df_all_ot':df_all_ot, 'df_OIC_ot':df_OIC_ot, 'df_not_ot':df_not_ot, 'df_OECD_ot':df_not_ot}\n",
        "\n",
        "for curkey, curval in dfs_dict.items():\n",
        "  my_list=curval.columns.to_list()\n",
        "  for curr_col in my_list:\n",
        "    if curr_col in ('Stock market capitalization to GDP (%)'):\n",
        "      df[curkey]=(currdf[curr_col])**(1./2)\n",
        "    elif curr_col in ('Outstanding deposits with commercial banks','GNI per capita (constant 2010 US$)','Outstanding loans from commercial banks'):\n",
        "      if currdf[curr_col].notnull:\n",
        "        currdf[curr_col]=np.log(currdf[curr_col])   \n",
        "    elif (curr_col not in ['OIC','OECD'])&(is_numeric_dtype(currdf[curr_col])):\n",
        "      currdf[curr_col]=(currdf[curr_col])**(1./3)\n",
        "'''\n",
        "\n",
        "### TODO ###\n",
        "# (-) Why are still nulls in Stock Market Cap?\n",
        "# (-) Transform data at regression run-time using pycaret\n",
        "# (-) Use 'region', '%muslim' in regression models (not charts)\n",
        "# (-) use same variables as Ihsan's paper\n",
        "# (-) review Ihsan's paper to see if I'm missing anything else\n",
        "# (-) figure out best box plots (huge difference between knn and null)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_pOdzOKgUEI"
      },
      "source": [
        "#Run classification models\n",
        "####using pycaret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGykYGVL6SWV"
      },
      "source": [
        "##df_iset Classification for unprocessed data (has null values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCE6R05kgb81"
      },
      "outputs": [],
      "source": [
        "#Classification model for unprocessed data (data has null values)\n",
        "dfa=tmp_all.copy(deep=True) #has null values\n",
        "dfa=dfa.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "#dfa=df_all_i #.fillna with KNN by inc_grp & OIC  \n",
        "\n",
        "#dfa=df_all_o #.fillna with KNN by OIC\n",
        "\n",
        "dfa=dfa.loc[~dfa.OIC.isnull()]\n",
        "delcols=['country','FIPS','region','inc_grp','OECD']\n",
        "dfa.drop(delcols,inplace=True,axis=1)\n",
        "\n",
        "dftrain = dfa.copy(deep=True)\n",
        "my_model = setup(data=dftrain, # using my training data partition\n",
        "                 target='OIC', # variable of interest (my dependent variable)\n",
        "                 silent=True\n",
        "                 )\n",
        "\n",
        "# create the best-fit model\n",
        "top = compare_models() \n",
        "mymod=create_model(top)\n",
        "best = tune_model(mymod)\n",
        "\n",
        "# compare all baseline models and select top 3\n",
        "'''\n",
        "#if you want to blend the top 3 models...\n",
        "top = compare_models(n_select = 3) \n",
        "# tune top base models\n",
        "tuned_top = [tune_model(i) for i in top]\n",
        "# ensemble top tuned models\n",
        "bagged_top = [ensemble_model(i) for i in tuned_top]\n",
        "# blend top base models \n",
        "blender = blend_models(estimator_list = top) \n",
        "# select best model \n",
        "best = automl(optimize = 'f1')\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNQsj6fG__FC"
      },
      "outputs": [],
      "source": [
        "evaluate_model(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYBde8rC6uyZ"
      },
      "source": [
        "##Classification for data with .fillna(KNN) for OIC vs. notOIC (df_all_o)\n",
        "#####removed the colums exhibiting multicollinearity: \n",
        "```python\n",
        "col_to_delete = ['Made or received digital payments in the past year (% age 15+)', \n",
        "                 'Debit card (% age 15+)',\n",
        "                 'BANK DEPOSITS to GDP (%)',\n",
        "                 'Used a debit or credit card to make a purchase in the past year (% age 15+)',\n",
        "                 'Outstanding deposits with commercial banks',\n",
        "                 'Credit card (% age 15+)'\n",
        "                ]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfoESXWJ63of"
      },
      "outputs": [],
      "source": [
        "dfa=df_all_o.copy(deep=True)\n",
        "\n",
        "dfa=dfa.loc[~dfa.OIC.isnull()]\n",
        "delcols=['country','FIPS','region','inc_grp','OECD']\n",
        "dfa.drop(delcols,inplace=True,axis=1)\n",
        "col_to_delete = ['Made or received digital payments in the past year (% age 15+)', \n",
        "                 'Debit card (% age 15+)',\n",
        "                 'BANK DEPOSITS to GDP (%)',\n",
        "                 'Used a debit or credit card to make a purchase in the past year (% age 15+)',\n",
        "                 'Outstanding deposits with commercial banks',\n",
        "                 'Credit card (% age 15+)'\n",
        "                ]\n",
        "dfa.drop(col_to_delete,inplace=True,axis=1)\n",
        "\n",
        "dftrain = dfa.copy(deep=True)\n",
        "my_model = setup(data=dftrain, # using my training data partition\n",
        "                 target='OIC', # variable of interest (my dependent variable)\n",
        "                 silent=True\n",
        "                 )\n",
        "\n",
        "# create the best-fit model\n",
        "top = compare_models() \n",
        "mymod=create_model(top)\n",
        "best = tune_model(mymod)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xuay7bFt7cix"
      },
      "outputs": [],
      "source": [
        "evaluate_model(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIq4Kd9kNuf-"
      },
      "source": [
        "##Classification model for df_ifdindex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hgxV_JkMpxZ"
      },
      "outputs": [],
      "source": [
        "df =df_ifdindex.merge(class_codes, on='country',how='inner')\n",
        "df1 =df.pivot_table(index=['country','OIC'], columns='desc', values='value')\n",
        "df2 =df.pivot_table(index=['country','OECD'], columns='desc', values='value')\n",
        "df1.reset_index(inplace = True)\n",
        "df2.reset_index(inplace = True)\n",
        "\n",
        "df_OI =df1.loc[df['OIC']==1]\n",
        "df_N =df1.loc[df['OIC']!=1]\n",
        "df_OE =df2.loc[df['OECD']==1]\n",
        "\n",
        "dfa=df1.copy(deep=True)\n",
        "print(dfa.head())\n",
        "\n",
        "dfa=dfa.loc[~dfa.OIC.isnull()]\n",
        "try:\n",
        "  delcols=['country','FIPS','region','inc_grp','OECD']\n",
        "  dfa.drop(delcols,inplace=True,axis=1)\n",
        "except:\n",
        "  True\n",
        "  \n",
        "dftrain = dfa.copy(deep=True)\n",
        "my_model = setup(data=dftrain, # using my training data partition\n",
        "                 target='OIC', # variable of interest (my dependent variable)\n",
        "                 silent=True\n",
        "                 )\n",
        "\n",
        "# create the best-fit model\n",
        "top = compare_models() \n",
        "mymod=create_model(top)\n",
        "best = tune_model(mymod)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba7EDLx2N1u1"
      },
      "outputs": [],
      "source": [
        "evaluate_model(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmCkTV9n1cp-"
      },
      "source": [
        "#t-test on mean differences (df_iset)\n",
        "##????Where is the VIF dataframe being used????\n",
        "#VIF to check for multicollinearity (idenified several variables to drop)\n",
        "######VIF of greater than 5 indicates problem (must elim one variable at a time, then re-run VIF calc). The resulting list of columns to delete is below. There might also be additional columns to delete.\n",
        "```python\n",
        "col_to_delete = ['Made or received digital payments in the past year (% age 15+)', \n",
        "                 'Debit card (% age 15+)',\n",
        "                 'BANK DEPOSITS to GDP (%)',\n",
        "                 'Used a debit or credit card to make a purchase in the past year (% age 15+)',\n",
        "                 'Outstanding deposits with commercial banks',\n",
        "                 'Credit card (% age 15+)'\n",
        "                ]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l36KWRrw1hCK"
      },
      "outputs": [],
      "source": [
        "  \n",
        "# Dataframes to use for statistical tests\n",
        "df_OI=df_OIC_o.copy(deep=True)\n",
        "df_N=df_not_o.copy(deep=True)\n",
        "\n",
        "# Save exist\n",
        "  \n",
        "# 2-sample (based on OIC) t-test for each DV variable\n",
        "collst=df_OI.columns.to_list()\n",
        "for curr_col in collst:\n",
        "    #  if ((curr_col not in ['OIC','OECD','FIPS'])&(is_numeric_dtype(df_OI[curr_col]))):\n",
        "    if (is_numeric_dtype(df_OI[curr_col])):\n",
        "        t_stat, p_val = stats.ttest_ind(df_OI[curr_col], df_N[curr_col], equal_var=True)\n",
        "       ####### p-value ~since we are looking to see if OIC is less than the non-OIC, we only need one-tail. Since scipy give 2-tail, we can divide p by 2\n",
        "        if p_val/2 < .001:\n",
        "            print('***',curr_col)\n",
        "        elif p_val/2 < .01:\n",
        "            print('**',curr_col)\n",
        "        elif p_val/2 < .05:\n",
        "            print('*',curr_col)\n",
        "        else:\n",
        "            print('#',curr_col, 'is not significantly different')\n",
        "\n",
        "\n",
        "###### calculating VIF for each feature\n",
        "\n",
        "vif_data =pd.DataFrame()\n",
        "vif_data =pd.concat([df_OIC_o,df_not_o], axis=0)\n",
        "vif_data.reset_index()\n",
        "\n",
        "vif_data = vif_data._get_numeric_data()\n",
        "#df.select_dtypes(include=np.number)\n",
        "\n",
        "try:\n",
        "    delcols=['FIPS','OIC','OECD']\n",
        "    vif_data.drop(delcols,inplace=True,axis=1)\n",
        "except:\n",
        "    True\n",
        "\n",
        "#### re-run VIF after col delete\n",
        "try:\n",
        "    delcols=['Made or received digital payments in the past year (% age 15+)']\n",
        "    vif_data.drop(delcols,inplace=True,axis=1)\n",
        "except:\n",
        "    True\n",
        "\n",
        "#### re-run VIF after col delete\n",
        "try:\n",
        "    delcols=['Debit card (% age 15+)']\n",
        "    vif_data.drop(delcols,inplace=True,axis=1)\n",
        "except:\n",
        "    True\n",
        "\n",
        "#### re-run VIF after col delete\n",
        "try:\n",
        "    delcols=['BANK DEPOSITS to GDP (%)']\n",
        "    vif_data.drop(delcols,inplace=True,axis=1)\n",
        "except:\n",
        "    True\n",
        "\n",
        "#### re-run VIF after col delete\n",
        "try:\n",
        "    delcols=['Used a debit or credit card to make a purchase in the past year (% age 15+)']\n",
        "    vif_data.drop(delcols,inplace=True,axis=1)\n",
        "except:\n",
        "    True\n",
        "\n",
        "#### re-run VIF after col delete\n",
        "try:\n",
        "    delcols=['Outstanding deposits with commercial banks']\n",
        "    vif_data.drop(delcols,inplace=True,axis=1)\n",
        "except:\n",
        "    True\n",
        "\n",
        "#### re-run VIF after col delete\n",
        "try:\n",
        "    delcols=['Credit card (% age 15+)']\n",
        "    vif_data.drop(delcols,inplace=True,axis=1)\n",
        "except:\n",
        "    True\n",
        "\n",
        "\n",
        "#Start of the VIF procedure...\n",
        "# the independent variables set\n",
        "mycols=vif_data.columns.to_list()\n",
        "X = add_constant(vif_data)\n",
        "\n",
        "# VIF dataframe\n",
        "vif_out=pd.DataFrame()\n",
        "vif_out[\"feature\"] = X.columns\n",
        "  \n",
        "# calculation\n",
        "vif_out[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "  \n",
        "print('\\n', vif_out)\n",
        "\n",
        "\n",
        "# 2-sample (based on OIC) t-test for each DV variable\n",
        "t=df_ifdindex.merge(df_incgrp,on='country',how='inner')\n",
        "t=t.merge(df,on='country',how='inner')\n",
        "\n",
        "print (t.info())\n",
        "df_OI = t.loc[t['OIC']==1]\n",
        "df_N = t.loc[t['OIC']!=1]\n",
        "#df_OI=df_ifdindex.loc [t['OIC']==1]\n",
        "#df_N=df_ifdindex.loc [t['OIC']!=1]\n",
        "\n",
        "collst=df_OI.columns.to_list()\n",
        "for curr_col in collst:\n",
        "    #  if ((curr_col not in ['OIC','OECD','FIPS'])&(is_numeric_dtype(df_OI[curr_col]))):\n",
        "    if (is_numeric_dtype(df_OI[curr_col])):\n",
        "        t_stat, p_val = stats.ttest_ind(df_OI[curr_col], df_N[curr_col], equal_var=True)\n",
        "       ####### p-value ~since we are looking to see if OIC is less than the non-OIC, we only need one-tail. Since scipy give 2-tail, we can divide p by 2\n",
        "        if p_val/2 < .001:\n",
        "            print('***',curr_col)\n",
        "        elif p_val/2 < .01:\n",
        "            print('**',curr_col)\n",
        "        elif p_val/2 < .05:\n",
        "            print('*',curr_col)\n",
        "        else:\n",
        "            print('#',curr_col, 'is not significantly different')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lOZcFKVOliU"
      },
      "source": [
        "#t-test difference of means (df_ifdinex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2X4UajeAE4U"
      },
      "source": [
        "#EDA statistics used to identify data to be cleansed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3PdsVzxJX4k"
      },
      "source": [
        "###Heatmap... not used because it was not useful\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XE9dV-dDW4S5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# using the transformed dataframe \n",
        "\n",
        "plt.figure(figsize=(7,25))\n",
        "\n",
        "#print(df_OIC.info(verbose=True),df_OIC0.info(verbose=True))\n",
        "\n",
        "cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n",
        "\n",
        "g = sns.heatmap(\n",
        "    df_OI.iloc[:,0:28], \n",
        "    square=True, # make cells square\n",
        "    cbar_kws={'fraction' : 0.01}, # shrink colour bar\n",
        "    cmap=cmap, \n",
        "    linewidth=1 # space between cells\n",
        ")\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIbxqXfIu5_p"
      },
      "source": [
        "##Univariate EDA statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaRwf8SZsO0D"
      },
      "outputs": [],
      "source": [
        "''' this code block is meant to test modality... it is not working\n",
        "#### pretty sure the issue is that data must = one column (not whole df)\n",
        "!pip install unidip\n",
        "from unidip import UniDip\n",
        "import unidip.dip as dip\n",
        "\n",
        "data=df_univar_all.copy(deep=True)\n",
        "data = np.msort(data)\n",
        "print(dip.diptst(data))\n",
        "intervals = UniDip(data).run()\n",
        "print(intervals)\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import normaltest\n",
        "from scipy.stats import anderson\n",
        "\n",
        "# show all rows & columns (wrap, truncate, see all, view)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option(\"expand_frame_repr\", False)\n",
        "\n",
        "# ADD more stuff: \n",
        "#     unimodal?, z-value, potential outliers, boxplots? qqplots? scatterplots? histograms?...\n",
        "# needs interpretation insights (how to read)\n",
        "\n",
        "# good usage of append, dict, df, and multiIndex \n",
        "#        https://www.kaggle.com/dimitreoliveira/model-stacking-feature-engineering-and-eda \n",
        "\n",
        "# calculate statistic metrics for features\n",
        "def univariate_metrics (df_passed, name_of_df):\n",
        "    #create blank lists\n",
        "    my_col_dtype = []\n",
        "    my_col_cnt = []\n",
        "    my_col_num_distinct = []\n",
        "    my_col_perc_missing = []\n",
        "    my_col_cnt_null = []\n",
        "    my_col_min = []\n",
        "    my_col_max = []\n",
        "    my_col_range = []\n",
        "    my_col_mean = []\n",
        "    my_col_std = []\n",
        "    my_col_median = []\n",
        "    my_col_Q25 = []\n",
        "    my_col_Q75 = []\n",
        "    my_col_CV = []\n",
        "    my_col_skew = []\n",
        "    my_col_kurtosis = []\n",
        "    my_col_interp_skew = []\n",
        "    my_col_interp_kurtosis = []\n",
        "    my_col_mode = []\n",
        "    my_col_unique_vals = []\n",
        "    my_col_shapiro_p = []\n",
        "    my_col_shapiro_p_interp = []\n",
        "    my_col_K2_p = []\n",
        "    my_col_K2_p_interp = []\n",
        "\n",
        "    for mycol in df_passed.columns:  #add values to lists\n",
        "        my_col_dtype.append({'Col_Name' : mycol, 'DType': df_passed.dtypes[mycol]})\n",
        "        my_col_cnt.append({'Col_Name' : mycol, 'NbrValues': len(df_passed)-df_passed[mycol].isnull().sum()})\n",
        "        my_col_cnt_null.append({'Col_Name' : mycol, 'CntNull': df_passed[mycol].isnull().sum()})\n",
        "        my_col_num_distinct.append({'Col_Name' : mycol,'NumUnique': df_passed[mycol].nunique()})\n",
        "        my_col_perc_missing.append({'Col_Name' : mycol, 'PercNull': df_passed[mycol].isnull().mean()})\n",
        "        my_col_cnt_null.append({'Col_Name' : mycol, 'CntNull': df_passed[mycol].isnull().sum()})\n",
        "        \n",
        "        if df_passed[mycol].dtype.kind in 'iuf': # these metrics return a single value per feature\n",
        "            my_col_min.append({'Col_Name' : mycol, 'Min' : df_passed[mycol].min()})\n",
        "            my_col_max.append({'Col_Name' : mycol,'Max' : df_passed[mycol].max()})\n",
        "            my_col_range.append({'Col_Name' : mycol,'Range' : df_passed[mycol].max() - df_passed[mycol].min()})\n",
        "            my_col_mean.append({'Col_Name' : mycol,'Mean' : df_passed[mycol].mean()})\n",
        "            my_col_std.append({'Col_Name' : mycol,'Std' : df_passed[mycol].std()})\n",
        "            my_col_median.append({'Col_Name' : mycol,'Median' : df_passed[mycol].median()})\n",
        "            my_col_Q25.append({'Col_Name' : mycol,'Q25' : df_passed[mycol].quantile(q=.25)})\n",
        "            my_col_Q75.append({'Col_Name' : mycol,'Q75' : df_passed[mycol].quantile(q=.75)})\n",
        "            if df_passed[mycol].mean() != 0: \n",
        "                my_col_CV.append({'Col_Name' : mycol,'CV' : df_passed[mycol].std() / df_passed[mycol].mean()})\n",
        "            my_col_skew.append({'Col_Name' : mycol,'Skew' : df_passed[mycol].skew()})  \n",
        "            curr_skew=df_passed[mycol].skew()\n",
        "            if curr_skew >= 1:\n",
        "                my_col_interp_skew.append({'Col_Name' : mycol, 'Skew Interp' : 'Significant positive skew'})\n",
        "            elif curr_skew > 0.5:\n",
        "                my_col_interp_skew.append({'Col_Name' : mycol,'Skew Interp' : 'Positive skew'})\n",
        "            elif curr_skew >= -0.5:\n",
        "                my_col_interp_skew.append({'Col_Name' : mycol,'Skew Interp' : 'Not skewed'})\n",
        "            elif curr_skew > -1:\n",
        "                my_col_interp_skew.append({'Col_Name' : mycol,'Skew Interp' : 'Negative skew'})\n",
        "            else:\n",
        "                my_col_interp_skew.append({'Col_Name' : mycol,'Skew Interp' : 'Significant negative skew'})\n",
        "            my_col_kurtosis.append({'Col_Name' : mycol,'Kurtosis' : df_passed[mycol].kurtosis()})            \n",
        "            curr_kurt = df_passed[mycol].kurtosis()\n",
        "            if curr_kurt >= 3:\n",
        "                my_col_interp_kurtosis.append({'Col_Name' : mycol, 'kurtosis Interp' : 'Positive kurtosis'})\n",
        "            elif curr_kurt <= -3:\n",
        "                my_col_interp_kurtosis.append({'Col_Name' : mycol,'kurtosis Interp' : 'Negative kurtosis'})\n",
        "            else:\n",
        "                my_col_interp_kurtosis.append({'Col_Name' : mycol,'kurtosis Interp' : 'No kurtosis'})\n",
        "            ''' Need to figure out why Normal tests are not working... \n",
        "            #######test normality\n",
        "            stat, p = shapiro(df_passed[mycol])\n",
        "            my_col_shapiro_p.append({'Col_Name' : mycol,'Shapiro p' : p})\n",
        "            alpha = .05\n",
        "            if p > alpha:\n",
        "                my_col_shapiro_p_interp.append({'Col_Name' : mycol,'Shapiro Interp' : 'Appears Normal'})\n",
        "            else:\n",
        "                my_col_shapiro_p_interp.append({'Col_Name' : mycol,'Shapiro Interp' : 'Appears Not Normal'})\n",
        "                \n",
        "            stat, p = normaltest(df_passed[mycol])\n",
        "            my_col_K2_p.append({'Col_Name' : mycol,'K2 p' : p})\n",
        "            if p > alpha:\n",
        "                my_col_K2_p_interp.append({'Col_Name' : mycol,'K2 Interp' : 'Appears Normal'})\n",
        "            else:\n",
        "                my_col_K2_p_interp.append({'Col_Name' : mycol,'K2 Interp' : 'Appears Not Normal'})\n",
        "            '''\n",
        "        if df_passed[mycol].dtype.kind not in 'bf': # these metrics return multiple values per feature\n",
        "            my_col_mode.append({'Col_Name' : mycol,'Mode' : df_passed[mycol].mode()})\n",
        "            my_col_unique_vals.append({'Col_Name' : mycol,'UniqueVals' : df_passed[mycol].unique()})\n",
        "            \n",
        "    # Merge dictionary-lists to dataframe on \"Col_Name\"\n",
        "    mylist = [my_col_dtype, my_col_num_distinct, my_col_cnt, my_col_cnt_null, my_col_perc_missing, \n",
        "              my_col_min, my_col_max, my_col_range, my_col_mean, my_col_std, my_col_Q25, \n",
        "              my_col_median, my_col_Q75, my_col_CV, my_col_skew, my_col_interp_skew, \n",
        "              my_col_kurtosis, my_col_interp_kurtosis]\n",
        "    df1 = []\n",
        "    df2 = []\n",
        "    df13 = []\n",
        "    df14 = []\n",
        "    for currlist in mylist:\n",
        "        if currlist == my_col_dtype:\n",
        "            df1 = pd.DataFrame.from_dict(currlist) #since my_col_dtype is the first element, create (rather than append) the df\n",
        "        else:\n",
        "            df2 = pd.DataFrame.from_dict(currlist)\n",
        "            df1 = pd.merge(left = df1, right = df2, on = 'Col_Name')\n",
        "    \n",
        "    #eliminate duplicate columns\n",
        "    df1 = df1.loc[:,~df1.columns.duplicated()]\n",
        "    #eliminate duplicate rows\n",
        "    df1.drop_duplicates(inplace=True, ignore_index=True)\n",
        "\n",
        "    df13 = pd.DataFrame.from_dict(my_col_mode)\n",
        "    df14 = pd.DataFrame.from_dict(my_col_unique_vals)\n",
        "\n",
        "    #######Currently not printing the unique values or mode (too long)\n",
        "    #print(df1.transpose(), '\\n\\nUnique Values:\\n', df14.transpose(), '\\n\\nMode:\\n', df13.transpose())\n",
        "    print(df1.transpose())\n",
        "    mypath = '/gdrive/MyDrive/Colab Notebooks/Research_FinAccessData/' \n",
        "    myfile = 'mycol_'+name_of_df+'.xlsx'\n",
        "    joined_path = os.path.join(mypath, myfile)\n",
        "    df1.to_excel(joined_path)\n",
        "\n",
        "\n",
        "\n",
        "###################\n",
        "#NEED to figure out:\n",
        "# why non-Nan value = num_unique\n",
        "# why K2 normaltest is NaN\n",
        "# why Shapiro is alway 1\n",
        "#\n",
        "###################\n",
        "print('\\n\\nOIC Univariate stats:')    \n",
        "univariate_metrics (df_univar_OIC,'df_univar_OIC')  \n",
        "\n",
        "print('\\n\\nNot OIC Univariate stats:')    \n",
        "univariate_metrics (df_univar_not,'df_univar_not')  \n",
        "\n",
        "print('\\n\\nAll Univariate stats:')    \n",
        "univariate_metrics (df_univar_all,'df_univar_all')  \n",
        "\n",
        "     \n",
        "#---------------------\n",
        "#  OR   print a threashold-percent of rows with zero/null/unique values\n",
        "#---------------------\n",
        "'''\n",
        "for col in df.columns:\n",
        "    pct_nan = df[col].isna().mean() \n",
        "    pct_zero = (df[col] == 0).mean()\n",
        "    pct_uniq = df[col].nunique()/df.shape[0]\n",
        "    if pct_nan > .3:\n",
        "        print('{0}: NULL value\\'s = {1:.2f}%'.format(col, pct_nan * 100))\n",
        "    if pct_zero > .3:\n",
        "        print('{0}: ZERO\\'s = {1:.2f}%'.format(col, pct_zero * 100))\n",
        "    if pct_uniq < .05:\n",
        "            print('\\nUnique values for feature: ', col, '\\n', df[col].unique()) \n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DSg0-2tvE4p"
      },
      "source": [
        "##commented out... Outlier detection using pycaret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0gcWPNAhRAp"
      },
      "outputs": [],
      "source": [
        "'''#seek out anomalies for the not OIC countries\n",
        "from pycaret.anomaly import *\n",
        "\n",
        "\n",
        "#detecting anolalies is an unsupervised model --there is no response variable. You are simply inspecting the data to \n",
        "#determine if there are outliers (data that does not look/act like the rest of the data set). This is performed by \n",
        "#looking for clustering of the data points in n-dimensional space, where n = number of features.\n",
        "\n",
        "dftrain =df_not_save.sample(frac=.9, random_state=111)\n",
        "dftest = df_not_save.drop(dftrain.index)\n",
        "mynomaly = setup(dftrain, normalize=True, session_id = 123)\n",
        "\n",
        "# creating different models to find anomolies\n",
        "iforest = create_model('iforest')\n",
        "knn = create_model('knn')\n",
        "\n",
        "# Comparing anomalies in models\n",
        "iforest_results = assign_model(iforest)\n",
        "knn_results = assign_model(knn)\n",
        "iforest_anomaly=iforest_results[iforest_results['Anomaly']==1]\n",
        "knn_anomaly=knn_results[knn_results['Anomaly']==1]\n",
        "\n",
        "mylist=iforest_anomaly.append(knn_anomaly)\n",
        "\n",
        "print(mylist.transpose())\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiRiZNQHMZOX"
      },
      "outputs": [],
      "source": [
        "#seek out anomalies for the OIC countries\n",
        "'''\n",
        "dftrain =df_OIC_save.sample(frac=.9, random_state=111)\n",
        "dftest = df_OIC_save.drop(dftrain.index)\n",
        "mynomaly = setup(dftrain, normalize=True, session_id = 123)\n",
        "\n",
        "iforest = create_model('iforest')\n",
        "knn = create_model('knn')\n",
        "\n",
        "# Comparing anomalies in models\n",
        "iforest_results = assign_model(iforest)\n",
        "knn_results = assign_model(knn)\n",
        "iforest_anomaly=iforest_results[iforest_results['Anomaly']==1]\n",
        "knn_anomaly=knn_results[knn_results['Anomaly']==1]\n",
        "\n",
        "mylist=iforest_anomaly.append(knn_anomaly)\n",
        "print(mylist.transpose())\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiPbS7-gDT7I"
      },
      "source": [
        "#Regression models:\n",
        "###Financial depth\n",
        "###Financial breadth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLCbPbv1DlRi"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "import statsmodels.api\n",
        "# Dataframes to use for regression models \n",
        "dfa=df_all_o.copy(deep=True)\n",
        "#KNN imputation for missing values \n",
        "imputer = KNNImputer(n_neighbors= 5) #use the 5 closest neighbors\n",
        "\n",
        "\n",
        "\n",
        "my_y= dfa[['Stock market capitalization to GDP (%)',\n",
        "          'Outstanding loans from commercial banks', \n",
        "          'Number of loan accounts with commercial banks per 1,000 adults', \n",
        "          'Private credit by deposit money banks to GDP (%)', \n",
        "          'Number of commercial bank branches per 100,000 adults',\n",
        "          'ATMs per 100,000 adults','Account at a formal financial institution (% age 15+)',\n",
        "          'Deposit accounts per 1,000 adults',\n",
        "          'Liquid liabilities to GDP (%)',\n",
        "          'Made payment using a mobile phone or the internet (% age 15+)',\n",
        "          'Number of ATMs per 1,000 km2',\n",
        "          'Number of commercial bank branches per 1,000 km2']]\n",
        "\n",
        "list_of_responses = my_y.columns.to_list()\n",
        "\n",
        "my_y=imputer.fit_transform(my_y)\n",
        "\n",
        "my_y=pd.DataFrame(my_y,columns=list_of_responses,index=dfa.index)\n",
        "dfa=dfa.fillna(0)\n",
        "\n",
        "# list of models\n",
        "models = []\n",
        "X=[]\n",
        "Y=[]\n",
        "for resp in list_of_responses:\n",
        "    '''    \n",
        "    formula = resp + \" ~  + 'OIC' + 'GNI per capita (constant 2010 US$)'\"\n",
        "    models.append(sm.OLS.from_formula(formula, data = dfa).fit())\n",
        "\n",
        "    #  can access params for each element in model\n",
        "    models[0].params\n",
        "    '''\n",
        "    print(resp)\n",
        "    X = dfa[['OIC', 'GNI per capita (constant 2010 US$)']] # here we have 2 variables for multiple regression. If you just want to use one variable for simple linear regression, then use X = df['Interest_Rate'] for example.Alternatively, you may add additional variables within the brackets\n",
        "    Y = my_y[resp]\n",
        "\n",
        "    ''' \n",
        "    # with sklearn\n",
        "    regr = linear_model.LinearRegression()\n",
        "    regr.fit(X, Y)\n",
        "\n",
        "    print('Intercept: \\n', regr.intercept_)\n",
        "    print('Coefficients: \\n', regr.coef_)\n",
        "\n",
        "    '''\n",
        "    '''\n",
        "    # example prediction with sklearn\n",
        "    New_Interest_Rate = 2.75\n",
        "    New_Unemployment_Rate = 5.3\n",
        "    print ('Predicted Stock Index Price: \\n', regr.predict([[New_Interest_Rate ,New_Unemployment_Rate]]))\n",
        "    '''\n",
        "    # with statsmodels\n",
        "    X = statsmodels.api.add_constant(X) # adding a constant\n",
        "    \n",
        "    model = statsmodels.api.OLS(Y, X).fit()\n",
        "    predictions = model.predict(X) \n",
        "    \n",
        "    #print('R^2:', model.rsquared, 'adj R^2:', model.rsquared_adj)\n",
        "    print_model = model.summary()\n",
        "    print('\\nstatmodels.api:\\n', print_model,'\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfl5TIs6CFnw"
      },
      "source": [
        "#Visualizations\n",
        "####bar charts\n",
        "####boxplots\n",
        "####ridge plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3aSmHeDOuK6"
      },
      "outputs": [],
      "source": [
        "# Dataframes to use for statistical tests \n",
        "df_OI=df_OIC_o\n",
        "df_N=df_not_o\n",
        "df_OE=df_OECD_o\n",
        "  \n",
        "# 2-sample (based on OIC) t-test for each DV variable\n",
        "collst=df_OI.columns.to_list()\n",
        "for curr_col in collst:\n",
        "  if (curr_col not in ['OIC','OECD'])&(is_numeric_dtype(df_OI[curr_col])):\n",
        "    t_stat, p_val = stats.ttest_ind(df_OI[curr_col], df_N[curr_col], equal_var=True)\n",
        "    ####### p-value ~since we are looking to see if OIC is less than the non-OIC, we only need one-tail. Since scipy give 2-tail, we can divide p by 2\n",
        "    if p_val/2 < .001:\n",
        "      print('***',curr_col)\n",
        "    elif p_val/2 < .01:\n",
        "      print('**',curr_col)\n",
        "    elif p_val/2 < .05:\n",
        "      print('*',curr_col)\n",
        "    else:\n",
        "      print('#',curr_col, 'is not significantly different')\n",
        "\n",
        "# Dataframes to use for statistical tests \n",
        "#build dataframes for df_ifdindex\n",
        "\n",
        "df =df_ifdindex.merge(class_codes, on='country',how='inner')\n",
        "df1 =df.pivot_table(index=['country','OIC'], columns='desc', values='value')\n",
        "df2 =df.pivot_table(index=['country','OECD'], columns='desc', values='value')\n",
        "df1.reset_index(inplace = True)\n",
        "df2.reset_index(inplace = True)\n",
        "\n",
        "df_OI =df1.loc[df['OIC']==1]\n",
        "df_N =df1.loc[df['OIC']!=1]\n",
        "df_OE =df2.loc[df['OECD']==1]\n",
        "\n",
        "# 2-sample (based on OIC) t-test for each DV variable\n",
        "collst=df_OI.columns.to_list()\n",
        "for curr_col in collst:\n",
        "  if (curr_col not in ['OIC','OECD'])&(is_numeric_dtype(df_OI[curr_col])):\n",
        "    t_stat, p_val = stats.ttest_ind(df_OI[curr_col], df_N[curr_col], equal_var=True)\n",
        "    ####### p-value ~since we are looking to see if OIC is less than the non-OIC, we only need one-tail. Since scipy give 2-tail, we can divide p by 2\n",
        "    if p_val/2 < .001:\n",
        "      print('***',curr_col)\n",
        "    elif p_val/2 < .01:\n",
        "      print('**',curr_col)\n",
        "    elif p_val/2 < .05:\n",
        "      print('*',curr_col)\n",
        "    else:\n",
        "      print('#',curr_col, 'is not significantly different')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSNeeIGFsAcG"
      },
      "source": [
        "##Bar charts by country (df_iset & df_ifdindex)\n",
        "###excludes multicollinearity columns#\n",
        "```python\n",
        "col_to_delete = ['Made or received digital payments in the past year (% age 15+)', \n",
        "                 'Debit card (% age 15+)',\n",
        "                 'BANK DEPOSITS to GDP (%)',\n",
        "                 'Used a debit or credit card to make a purchase in the past year (% age 15+)',\n",
        "                 'Outstanding deposits with commercial banks',\n",
        "                 'Credit card (% age 15+)'\n",
        "                ]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSmdgPmNr4zW"
      },
      "outputs": [],
      "source": [
        "# Dataframes to use for barplots\n",
        "df_OI=df_OIC_o\n",
        "df_N=df_not_o\n",
        "df_OE=df_OECD_o\n",
        "\n",
        "my_colors = ['#BDFCC9', '#00CDCD']\n",
        "sns.set_palette(sns.color_palette(my_colors))\n",
        "\n",
        "# Print set of bar chart plots\n",
        "col_out=[['GNI per capita (constant 2010 US$)','Stock market capitalization to GDP (%)',\n",
        "          'Outstanding loans from commercial banks', 'Number of loan accounts with commercial banks per 1,000 adults'], \n",
        "         ['Private credit by deposit money banks to GDP (%)', 'Number of commercial bank branches per 100,000 adults',\n",
        "          'ATMs per 100,000 adults','Account at a formal financial institution (% age 15+)'],\n",
        "         ['Deposit accounts per 1,000 adults',\n",
        "          \"Deposit money banks' assets to GDP (%)\",\n",
        "          'Liquid liabilities to GDP (%)'],\n",
        "         ['Made payment using a mobile phone or the internet (% age 15+)',\n",
        "          'Number of ATMs per 1,000 km2',\n",
        "          'Number of commercial bank branches per 1,000 km2']\n",
        "        ]\n",
        "\n",
        "mygrp=0\n",
        "while mygrp < len(col_out):\n",
        "  fig = plt.figure(figsize=[14,14]) #width by height\n",
        "  mycnt=0 #set temp variable to index the column variables\n",
        "  ax=pd.Series(range(0,50))\n",
        "  mycnt=0\n",
        "  for mycol in col_out[mygrp]:\n",
        "    #df_OI = df_OI_save    -----not sure why I did this\n",
        "    try:\n",
        "      df_OI = df_OI[df_OI[mycol]>0] # eliminate null and zero value rows from the plot\n",
        "      plt_cols=len(col_out[mygrp])\n",
        "      df_OI_sort = df_OI.reindex(df_OI[mycol].sort_values(ascending=True).index)\n",
        "      # figure-out how to eliminate nan & 0 values from the dataframe\n",
        "      ax[mycnt]=fig.add_subplot(1,plt_cols,mycnt+1)\n",
        "      ax[mycnt].barh(df_OI_sort.country,df_OI_sort[mycol],height=.5, color='limegreen', alpha=.4)\n",
        "\n",
        "      #  Add line plots (on top of bar) for averages: OIC, non-OIC, OECD (OECD not calc yet)\n",
        "      ax[mycnt].axvline(df_OI[mycol].mean(), color='green', linewidth=2, alpha=.8, linestyle=':', label='OIC') \n",
        "      ax[mycnt].axvline(df_N[mycol].mean(), color='orange', linewidth=2, alpha=.3, linestyle='dashdot', label='Not OIC')\n",
        "      ax[mycnt].axvline(df_OE[mycol].mean(), color='lightblue', linewidth=2, linestyle='--', label='OECD')\n",
        "    \n",
        "      #  Adjust the spines & axis labels\n",
        "      ax[mycnt].set_xlabel(mycol, rotation=5, fontweight='light')\n",
        "      ax[mycnt].tick_params(bottom=False)\n",
        "      ax[mycnt].spines['top'].set_visible(False)\n",
        "      ax[mycnt].spines['bottom'].set_visible(False)\n",
        "      ax[mycnt].spines['left'].set_visible(True)\n",
        "      ax[mycnt].spines['right'].set_visible(False)\n",
        "      ax[mycnt].set(xticklabels=[])  \n",
        "      ax[mycnt].grid(False)\n",
        "      mycnt+=1\n",
        "    except:\n",
        "      True\n",
        "  plt.axis('tight')\n",
        "  plt.legend(bbox_to_anchor=(1.04,.1), borderaxespad=0)\n",
        "  fig.tight_layout()\n",
        "  fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "  \n",
        "\n",
        "\n",
        "  #save the plot into a file\n",
        "  mypath = '/gdrive/MyDrive/Colab Notebooks/FinAccessData' \n",
        "  myfile = 'barplot_df_iset_'+str(mygrp)+'.jpg'\n",
        "  joined_path = os.path.join(mypath, myfile)\n",
        "  #plt.savefig(joined_path,bbox_inches = 'tight', pad_inches = 0)\n",
        "\n",
        "  fig.show()\n",
        "  mygrp+=1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsFSjFq_-gB2"
      },
      "outputs": [],
      "source": [
        "#build barplots df_ifdindex\n",
        "\n",
        "df =df_ifdindex.merge(class_codes, on='country',how='inner')\n",
        "df1 =df.pivot_table(index=['country','OIC'], columns='desc', values='value')\n",
        "df2 =df.pivot_table(index=['country','OECD'], columns='desc', values='value')\n",
        "df1.reset_index(inplace = True)\n",
        "df2.reset_index(inplace = True)\n",
        "\n",
        "df_OI =df1.loc[df['OIC']==1]\n",
        "df_N =df1.loc[df['OIC']!=1]\n",
        "df_OE =df2.loc[df['OECD']==1]\n",
        "\n",
        "col_out=[['FinDev','FinInstit','FinMarkets'],\n",
        "         ['DepthFinInstit','AccFinInstit','EffFinInstit'],\n",
        "         ['DepthFinMarkets','AccFinMarkets','EffFinMarkets']\n",
        "        ]\n",
        "\n",
        "mygrp=0\n",
        "while mygrp < len(col_out):\n",
        "  fig = plt.figure(figsize=[12,12]) #width by height\n",
        "  mycnt=0 #set temp variable to index the column variables\n",
        "  ax=pd.Series(range(0,50))\n",
        "  mycnt=0\n",
        "  for mycol in col_out[mygrp]:\n",
        "    #df_OI = df_OI_save\n",
        "    df_OI = df_OI[df_OI[mycol]>0] # eliminate null and zero value rows from the plot\n",
        "    plt_cols=len(col_out[mygrp])\n",
        "    df_OI_sort = df_OI.reindex(df_OI[mycol].sort_values(ascending=True).index)\n",
        "    # figure-out how to eliminate nan & 0 values from the dataframe\n",
        "    ax[mycnt]=fig.add_subplot(1,plt_cols,mycnt+1)\n",
        "    ax[mycnt].barh(df_OI_sort.country,df_OI_sort[mycol],height=.5, color='limegreen', alpha=.4)\n",
        "\n",
        "    #  Add line plots (on top of bar) for averages: OIC, non-OIC, OECD (OECD not calc yet)\n",
        "    ax[mycnt].axvline(df_OI[mycol].mean(), color='green', linewidth=2, alpha=.8, linestyle=':', label='OIC') \n",
        "    ax[mycnt].axvline(df_N[mycol].mean(), color='orange', linewidth=2, alpha=.3, linestyle='dashdot', label='Not OIC')\n",
        "    ax[mycnt].axvline(df_OE[mycol].mean(), color='lightblue', linewidth=2, linestyle='--', label='OECD')\n",
        "  \n",
        "    #  Adjust the spines & axis labels\n",
        "    ax[mycnt].set_xlabel(mycol, rotation=5, fontweight='light')\n",
        "    ax[mycnt].tick_params(bottom=False)\n",
        "    ax[mycnt].spines['top'].set_visible(False)\n",
        "    ax[mycnt].spines['bottom'].set_visible(False)\n",
        "    ax[mycnt].spines['left'].set_visible(True)\n",
        "    ax[mycnt].spines['right'].set_visible(False)\n",
        "    ax[mycnt].set(xticklabels=[])  \n",
        "    ax[mycnt].grid(False)\n",
        "    mycnt+=1\n",
        "  plt.axis('tight')\n",
        "  plt.legend(bbox_to_anchor=(1.04,.1), borderaxespad=0)\n",
        "  fig.tight_layout()\n",
        "  fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "  \n",
        "  #save the plot into a file\n",
        "  mypath = '/gdrive/MyDrive/Colab Notebooks/FinAccessData' \n",
        "  myfile = 'barplot_df_ifdindex_'+str(mygrp)+'.jpg'\n",
        "  joined_path = os.path.join(mypath, myfile)\n",
        "  #plt.savefig(joined_path,bbox_inches = 'tight', pad_inches = 0)\n",
        "\n",
        "  fig.show()\n",
        "  mygrp+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTyPq2yNSA5f"
      },
      "source": [
        "##Seaborn boxplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQWkyt4Fd9yT"
      },
      "outputs": [],
      "source": [
        "# plotting boxplots (a different approach)\n",
        "#////////////////////// df_OI & df_N are transformed\n",
        "#////// plots are slightly different for transformed data\n",
        "\n",
        "#dfa=pd.concat([df_OI,df_N])\n",
        "dfa=tmp_all\n",
        "dfa=dfa.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "my_colors = ['#BDFCC9', '#00CDCD']\n",
        "sns.set_palette(sns.color_palette(my_colors))\n",
        "\n",
        "try:\n",
        "  dfa.loc[dfa['OIC']>0.,'OIC']='Muslim'\n",
        "  dfa.loc[dfa['OIC']!='Muslim','OIC']='Not_Muslim'\n",
        "except:\n",
        "  True\n",
        "col_out=['ATMs per 100,000 adults',\n",
        "       'Account at a formal financial institution (% age 15+)',\n",
        "       'BANK DEPOSITS to GDP (%)', 'Credit card (% age 15+)',\n",
        "       'Debit card (% age 15+)', 'Deposit accounts per 1,000 adults',\n",
        "       \"Deposit money banks' assets to GDP (%)\",\n",
        "       'GNI per capita (constant 2010 US$)', 'Liquid liabilities to GDP (%)',\n",
        "       'Made or received digital payments in the past year (% age 15+)',\n",
        "       'Made payment using a mobile phone or the internet (% age 15+)',\n",
        "       'Number of ATMs per 1,000 km2',\n",
        "       'Number of commercial bank branches per 1,000 km2',\n",
        "       'Number of commercial bank branches per 100,000 adults',\n",
        "       'Number of loan accounts with commercial banks per 1,000 adults',\n",
        "       'Outstanding deposits with commercial banks',\n",
        "       'Outstanding loans from commercial banks',\n",
        "       'Private credit by deposit money banks to GDP (%)',\n",
        "       'Stock market capitalization to GDP (%)',\n",
        "       'Used a debit or credit card to make a purchase in the past year (% age 15+)'\n",
        "        ] \n",
        "\n",
        "from matplotlib.gridspec import GridSpec\n",
        "fig = plt.figure(figsize=(35,42))\n",
        "\n",
        "my_ncols = 3\n",
        "if (len(col_out)%my_ncols) == 0:\n",
        "  my_nrows = len(col_out)//my_ncols\n",
        "else:\n",
        "  my_nrows = (len(col_out)//my_ncols)+1\n",
        "\n",
        "gs = GridSpec(nrows=my_nrows, ncols=my_ncols)\n",
        "mycnt=0\n",
        "for my_col in col_out:\n",
        "  myaxcol=mycnt%my_ncols\n",
        "  myaxrow=mycnt//my_ncols\n",
        "  ax=fig.add_subplot(gs[myaxrow,myaxcol])\n",
        "\n",
        "  g1=sns.boxplot(x='inc_grp', y=my_col, showmeans=True, ax=ax, showcaps=False, showfliers=False, hue='OIC', \n",
        "                 data=dfa, order=['Low income', 'Lower middle income', 'Upper middle income','High income'])\n",
        "  #               palette='RdGn')\n",
        "  #ax = sns.swarmplot(x='inc_grp', y='AccFinInstit', data=dfa, color=\"k\", alpha=0.4)\n",
        "  #ax = sns.boxplot(x='inc_grp', y='AccFinInstit', data=dfa_OIC, )\n",
        "  g1.set_axis_on()\n",
        "  g1.grid(False)\n",
        "  g1.legend_.remove()\n",
        "  g1.tick_params(bottom=False, top=False, left=False, right=False)\n",
        "  g1.set(yticklabels=[]) \n",
        "  g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n",
        "  g1.set_xlabel(None)\n",
        "  mycnt+=1\n",
        "  #add text\n",
        "  x_offset=0.05 \n",
        "  x_loc=0\n",
        "  y_loc=myaxcol\n",
        "  text_offset_x=35\n",
        "  text_offset_y=20\n",
        "\n",
        "  annotate_params = dict(xytext=(text_offset_x, text_offset_y), textcoords='offset points', arrowprops={'arrowstyle':'->'})\n",
        "\n",
        "#save the plot into a file\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/FinAccessData' \n",
        "myfile = 'boxplot_df_iset.jpg'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "#plt.savefig(joined_path,bbox_inches = 'tight', pad_inches = 0)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "'''  ax[myaxrow,myaxcol].annotate('Median', (x_loc + 1 + x_offset, g1['medians'][x_loc].get_ydata()[0]), **annotate_params)\n",
        "  ax[myaxrow,myaxcol].annotate('25%', (x_loc + 1 + x_offset, g1['boxes'][x_loc].get_ydata()[0]), **annotate_params)\n",
        "  ax[myaxrow,myaxcol].annotate('75%', (x_loc + 1 + x_offset, g1['boxes'][x_loc].get_ydata()[2]), **annotate_params)\n",
        "  ax[myaxrow,myaxcol].annotate('5%', (x_loc + 1 + x_offset, g1['caps'][x_loc*2].get_ydata()[0]), **annotate_params)\n",
        "  ax[myaxrow,myaxcol].annotate('95%', (x_loc + 1 + x_offset, g1['caps'][(x_loc*2)+1].get_ydata()[0]), **annotate_params)\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMSLorva4n76"
      },
      "source": [
        "##Ridgeplot \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlzh2dkkzssD"
      },
      "outputs": [],
      "source": [
        "#setup dataframes to plot\n",
        "dfa=tmp_all\n",
        "dfa=dfa.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "try:\n",
        "    dfa.loc[dfa['OIC']>0.,'OIC']='Muslim'\n",
        "    dfa.loc[dfa['OIC']!='Muslim','OIC']='Not_Muslim'\n",
        "except:\n",
        "    True\n",
        "\n",
        "dfm=dfa.loc[dfa['OIC']=='Muslim']\n",
        "dfnot=dfa.loc[dfa['OIC']=='Not_Muslim']\n",
        "\n",
        "#list of columns to plot\n",
        "collst=['ATMs per 100,000 adults',\n",
        "       'Account at a formal financial institution (% age 15+)',\n",
        "       'BANK DEPOSITS to GDP (%)', 'Credit card (% age 15+)',\n",
        "       'Debit card (% age 15+)', 'Deposit accounts per 1,000 adults',\n",
        "       \"Deposit money banks' assets to GDP (%)\",\n",
        "       'GNI per capita (constant 2010 US$)', 'Liquid liabilities to GDP (%)',\n",
        "       'Made or received digital payments in the past year (% age 15+)',\n",
        "       'Made payment using a mobile phone or the internet (% age 15+)',\n",
        "       'Number of ATMs per 1,000 km2',\n",
        "       'Number of commercial bank branches per 1,000 km2',\n",
        "       'Number of commercial bank branches per 100,000 adults',\n",
        "       'Number of loan accounts with commercial banks per 1,000 adults',\n",
        "       'Outstanding deposits with commercial banks',\n",
        "       'Outstanding loans from commercial banks',\n",
        "       'Private credit by deposit money banks to GDP (%)',\n",
        "       'Stock market capitalization to GDP (%)',\n",
        "       'Used a debit or credit card to make a purchase in the past year (% age 15+)'\n",
        "        ] \n",
        "        \n",
        "from matplotlib.gridspec import GridSpec\n",
        "fig = plt.figure(figsize=(25,32))\n",
        "\n",
        "#enter number of desired columns of plots in output, then calculate the number of rows (% shows remainder; // operator is modulo function)\n",
        "my_ncols = 1\n",
        "if (len(collst)%my_ncols) == 0:\n",
        "    my_nrows = len(collst)//my_ncols\n",
        "else:\n",
        "    my_nrows = (len(collst)//my_ncols)+1\n",
        "\n",
        "gs = GridSpec(nrows=my_nrows, ncols=my_ncols, hspace=-0.2)\n",
        "\n",
        "#loop through each column in collst\n",
        "mycnt=0\n",
        "for my_col in collst:\n",
        "    #use number of columns to determine relative ax position\n",
        "    myaxcol=mycnt%my_ncols\n",
        "    myaxrow=mycnt//my_ncols\n",
        "    ax=fig.add_subplot(gs[myaxrow,myaxcol])\n",
        "    g1=sns.kdeplot(data=dfm[my_col].squeeze(), color='red', fill=True, label='Muslim', alpha=.1)\n",
        "    g1=sns.kdeplot(data=dfnot[my_col].squeeze(), color='green', fill=True, label='Not Muslim', alpha=.1)\n",
        "    g1.set_axis_on()\n",
        "    g1.grid(False)\n",
        "    #g1.legend_.remove()\n",
        "    g1.tick_params(bottom=False, top=False, left=False, right=False)\n",
        "    #g1.set(yticklabels=[my_col]) \n",
        "    g1.set_xlabel(my_col)\n",
        "    g1.axes.get_yaxis().set_visible(False)\n",
        "    rect = g1.patch\n",
        "    rect.set_alpha(0) #make background transparent\n",
        "    mycnt+=1\n",
        "    #add text\n",
        "    x_offset=0.05 \n",
        "    x_loc=0\n",
        "    y_loc=myaxcol\n",
        "    text_offset_x=35\n",
        "    text_offset_y=20\n",
        "    sns.despine(left=True,right=True,top=True)\n",
        "\n",
        "    annotate_params = dict(xytext=(text_offset_x, text_offset_y), textcoords='offset points', arrowprops={'arrowstyle':'->'})\n",
        "\n",
        "#save the plot into a file\n",
        "mypath = '/gdrive/MyDrive/Colab Notebooks/FinAccessData' \n",
        "myfile = 'ridgeplot_df_iset_'+str(mygrp)+'.jpg'\n",
        "joined_path = os.path.join(mypath, myfile)\n",
        "#plt.savefig(joined_path,bbox_inches = 'tight', pad_inches = 0)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOIim5v92Zu4"
      },
      "source": [
        "##Seaborn boxplots\n",
        "####approach #1 --not used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lwCE0UBkrR8"
      },
      "outputs": [],
      "source": [
        "####################################\n",
        "#\n",
        "#\n",
        "#  ???Should i plot transformed data  \n",
        "#  ???Will boxplot ignore null values \n",
        "#  Must add by region plots (possible to have region, incGrp, & OIC)\n",
        "#\n",
        "###################################\n",
        "\n",
        "# Use df_OIC or df_OIC_save instead of demo_all\n",
        "# need to merge with inc_grp & region data though\n",
        "\n",
        "'''def annotate_boxplot(bpdict, annotate_params=None,\n",
        "                     x_offset=0.05, x_loc=0,\n",
        "                     text_offset_x=35,\n",
        "                     text_offset_y=20):\n",
        "    \"\"\"Annotates a matplotlib boxplot with labels marking various centile levels.\n",
        "\n",
        "    Parameters:\n",
        "    - bpdict: The dict returned from the matplotlib `boxplot` function. If you're using pandas you can\n",
        "    get this dict by setting `return_type='dict'` when calling `df.boxplot()`.\n",
        "    - annotate_params: Extra parameters for the plt.annotate function. The default setting uses standard arrows\n",
        "    and offsets the text based on other parameters passed to the function\n",
        "    - x_offset: The offset from the centre of the boxplot to place the heads of the arrows, in x axis\n",
        "    units (normally just 0-n for n boxplots). Values between around -0.15 and 0.15 seem to work well\n",
        "    - x_loc: The x axis location of the boxplot to annotate. Usually just the number of the boxplot, counting\n",
        "    from the left and starting at zero.\n",
        "    text_offset_x: The x offset from the arrow head location to place the associated text, in 'figure points' units\n",
        "    text_offset_y: The y offset from the arrow head location to place the associated text, in 'figure points' units\n",
        "    \"\"\"\n",
        "    if annotate_params is None:\n",
        "        annotate_params = dict(xytext=(text_offset_x, text_offset_y), textcoords='offset points', arrowprops={'arrowstyle':'->'})\n",
        "\n",
        "    plt.annotate('Median', (x_loc + 1 + x_offset, bpdict['medians'][x_loc].get_ydata()[0]), **annotate_params)\n",
        "    plt.annotate('25%', (x_loc + 1 + x_offset, bpdict['boxes'][x_loc].get_ydata()[0]), **annotate_params)\n",
        "    plt.annotate('75%', (x_loc + 1 + x_offset, bpdict['boxes'][x_loc].get_ydata()[2]), **annotate_params)\n",
        "    plt.annotate('5%', (x_loc + 1 + x_offset, bpdict['caps'][x_loc*2].get_ydata()[0]), **annotate_params)\n",
        "    plt.annotate('95%', (x_loc + 1 + x_offset, bpdict['caps'][(x_loc*2)+1].get_ydata()[0]), **annotate_params)\n",
        "\n",
        "\n",
        "# The following section is to setup un-transformed data\n",
        "\"\"\"\n",
        "try: \n",
        "  df_demo_all['inc_grp']=df_demo_all['inc_grp'].fillna(value='other')\n",
        "except:\n",
        "  True\n",
        "\n",
        "#discard the rows with no income group\n",
        "df_demo_all=df_demo_all.loc[df_demo_all['inc_grp']!='other']\n",
        "\n",
        "unique_inc_grp = ['High income', 'Upper middle income', 'Lower middle income', 'Low income']\n",
        "\n",
        "df_dem_all = df_demo_all\n",
        "dfa=df_dem_all\n",
        "\n",
        "try:\n",
        "  dfa.loc[dfa['OIC']>0.,'OIC']='Muslim'\n",
        "  dfa.loc[dfa['OIC']!='Muslim','OIC']='Not_Muslim'\n",
        "except:\n",
        "  True\n",
        "\n",
        "dfa_OIC=dfa.loc[dfa['OIC']=='Muslim']\n",
        "\"\"\"\n",
        "\n",
        "#////////////////////// df_OI & df_N are transformed\n",
        "#////// plots are slightly different for transformed data\n",
        "\n",
        "#dfa=pd.concat([df_OI,df_N])\n",
        "dfa=tmp_all\n",
        "dfa=dfa.merge(class_codes, on='country',how='inner')\n",
        "\n",
        "try:\n",
        "  dfa.loc[dfa['OIC']>0.,'OIC']='Muslim'\n",
        "  dfa.loc[dfa['OIC']!='Muslim','OIC']='Not_Muslim'\n",
        "except:\n",
        "  True\n",
        "col_out=['GNI per capita (constant 2010 US$)','Stock market capitalization to GDP (%)',\n",
        "         'Outstanding deposits with commercial banks','Outstanding loans from commercial banks', \n",
        "         'Private credit by deposit money banks to GDP (%)'\n",
        "        ] \n",
        "\n",
        "\n",
        "#  specify number of columns in figure\n",
        "my_ncols = 2\n",
        "if (len(col_out)%my_ncols) == 0:\n",
        "  my_nrows = len(col_out)//my_ncols\n",
        "else:\n",
        "  my_nrows = (len(col_out)//my_ncols)+1\n",
        "\n",
        "fig, ax = plt.subplots(nrows=my_nrows,ncols=my_ncols, sharex=True)\n",
        "fig.set_size_inches(15,22)\n",
        "\n",
        "# hide all subplots in figure (show subplot after it's populated with data)\n",
        "my_rcnt=0\n",
        "my_ccnt=0\n",
        "while my_rcnt < my_nrows:\n",
        "  while my_ccnt < my_ncols:  \n",
        "    ax[my_rcnt, my_ccnt].set_axis_off()\n",
        "    my_ccnt+=1\n",
        "  my_ccnt=0\n",
        "  my_rcnt+=1\n",
        "\n",
        "mycnt=0\n",
        "for my_col in col_out:\n",
        "  myaxcol=mycnt%my_ncols\n",
        "  myaxrow=mycnt//my_ncols\n",
        "  g1=sns.boxplot(x='inc_grp', y=my_col, showcaps=False, showmeans=True, ax=ax[myaxrow,myaxcol], showfliers=False, \n",
        "                 palette=\"GnBu\", hue='OIC', data=dfa, order=['Low income', 'Lower middle income', 'Upper middle income','High income'])\n",
        "  #ax = sns.swarmplot(x='inc_grp', y='AccFinInstit', data=dfa, color=\"k\", alpha=0.4)\n",
        "  #ax = sns.boxplot(x='inc_grp', y='AccFinInstit', data=dfa_OIC, )\n",
        "  g1.set_axis_on()\n",
        "  g1.grid(False)\n",
        "  g1.legend_.remove()\n",
        "  g1.tick_params(bottom=False, top=False, left=False, right=False)\n",
        "  g1.set(yticklabels=[]) \n",
        "  g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n",
        "  g1.set_xlabel(None)\n",
        "\n",
        "  mycnt+=1\n",
        "  #add text\n",
        "  \"\"\"mytext=['a','b']\n",
        "  for i in range(len(x)):\n",
        "    plt.annotate(mytext[i], (x[i], y[i] + 0.2))\"\"\"\n",
        " \n",
        "  # get bpdict by setting `return_type='dict'` when calling `df.boxplot()`\n",
        "  #annotate_boxplot(bpdict)\n",
        "  \n",
        "  \"\"\"for curr_box in g1.patches:\n",
        "    height = curr_box.get_height()\n",
        "    ax[myaxrow,myaxcol].text(curr_box.get_x() + curr_box.get_width() / 2., 0.5 * height, int(height),\n",
        "                ha='center', va='center', color='black')\"\"\"\n",
        "\n",
        "handles, labels = ax[myaxrow,myaxcol].get_legend_handles_labels()\n",
        "plt.legend(handles[0:2], labels[0:2], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "sns.despine(left=True)\n",
        "plt.savefig('/gdrive/MyDrive/Colab Notebooks/box0.jpg')\n",
        "plt.show()\n",
        "\n",
        "'''  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx4t4PvM2L7C"
      },
      "source": [
        "##Plotly Split Violin\n",
        "## NOT USED because unable to sort the income group categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-2IgVOrSEMS"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "# need to create a dataframe for the ridgeline plots: \n",
        "#       Rows will be the plots to chart (e.g., ATMs, Branches,...); \n",
        "#       2 shapes will be (OIC vs. Not); \n",
        "#       x-axis will be country\n",
        "#############\n",
        "\n",
        "#Make column a categorical datatype\n",
        "''' COMMENT-out whole code block\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "#voilin showing muliple categories\n",
        "show_legend = [True,False,False,False]\n",
        "#category_orders={'inc_grp': ['High income', 'Upper middle income', 'Lower middle income', 'Low income']}, \n",
        "\n",
        "fig = go.Figure()\n",
        "curdf=df_all_o\n",
        "\n",
        "for i in range(0,len(pd.unique(curdf['inc_grp']))):\n",
        " \n",
        "  fig.add_trace(go.Violin(x=curdf['inc_grp'][(curdf['OIC'] == 1) &\n",
        "                                      (curdf['inc_grp'] == pd.unique(curdf['inc_grp'])[i])],\n",
        "                          y=curdf['AccFinInstit'][(curdf['OIC'] == 1)&\n",
        "                                      (curdf['inc_grp'] == pd.unique(curdf['inc_grp'])[i])],\n",
        "                          legendgroup='Muslim', scalegroup='Muslim', name='Muslim',\n",
        "                          side='negative',\n",
        "                          line_color='lightblue',\n",
        "                          showlegend=show_legend[i])\n",
        "               )\n",
        "  fig.add_trace(go.Violin(x=curdf['inc_grp'][(curdf['OIC'] != 1) &\n",
        "                                        (curdf['inc_grp'] == pd.unique(curdf['inc_grp'])[i])],\n",
        "                            y=curdf['AccFinInstit'][(curdf['OIC'] != 1)&\n",
        "                                               (curdf['inc_grp'] == pd.unique(curdf['inc_grp'])[i])],\n",
        "                            legendgroup='Not Muslim', scalegroup='Not Muslim', name='Not Muslim',\n",
        "                            side='positive',\n",
        "                            line_color='green',\n",
        "                            showlegend=show_legend[i])\n",
        "             )\n",
        "    \n",
        "# update characteristics shared by all traces\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"IMF Composition Metric\",\n",
        "    violingap=0, violingroupgap=0, violinmode='overlay', template='simple_white', xaxis_title='Income Group', yaxis_title='Access to Financial Institutions')\n",
        "\n",
        "fig.show()\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_-GJPW6wR1b"
      },
      "source": [
        "##Sparkline type charts\n",
        "#####Currently not working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYghdi-9pXBb"
      },
      "outputs": [],
      "source": [
        "#sparkline type report\n",
        "'''\n",
        "data = np.cumsum(np.random.rand(1000)-0.5)\n",
        "data = data - np.mean(data)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(411) # nrows, ncols, plot_number, top sparkline\n",
        "ax1.plot(data, 'b-')\n",
        "ax1.axhline(c='grey', alpha=0.5)\n",
        "\n",
        "ax2 = fig.add_subplot(412, sharex=ax1) \n",
        "ax2.plot(data, 'g-')\n",
        "ax2.axhline(c='grey', alpha=0.5)\n",
        "\n",
        "ax3 = fig.add_subplot(413, sharex=ax1)\n",
        "ax3.plot(data, 'y-')\n",
        "ax3.axhline(c='grey', alpha=0.5)\n",
        "\n",
        "ax4 = fig.add_subplot(414, sharex=ax1) # bottom sparkline\n",
        "ax4.plot(data, 'r-')\n",
        "ax4.axhline(c='grey', alpha=0.5)\n",
        "\n",
        "\n",
        "for axes in [ax1, ax2, ax3, ax4]: # remove all borders\n",
        "    plt.setp(axes.get_xticklabels(), visible=False)\n",
        "    plt.setp(axes.get_yticklabels(), visible=False)\n",
        "    plt.setp(axes.get_xticklines(), visible=False)\n",
        "    plt.setp(axes.get_yticklines(), visible=False)\n",
        "    plt.setp(axes.spines.values(), visible=False)\n",
        "\n",
        "\n",
        "# bottom sparkline\n",
        "plt.setp(ax4.get_xticklabels(), visible=True)\n",
        "plt.setp(ax4.get_xticklines(), visible=True)\n",
        "ax4.xaxis.tick_bottom() # but onlyt the lower x ticks not x ticks at the top\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MxQWcoWXs76"
      },
      "source": [
        "#not used 'potential routines'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjYEqt-ir_sF"
      },
      "outputs": [],
      "source": [
        "\n",
        "''' \n",
        "#This works! Saving df info in a loop\n",
        "new_df = []\n",
        "for curr_df in my_df_list:\n",
        "    new_df.append(curr_df)\n",
        "#outside of loop ...concat\n",
        "large_df = pd.concat(new_df, ignore_index=True)\n",
        "\n",
        "#sample code saving df values in a loop (untested)\n",
        "df = pd.DataFrame(columns=[\"A\", \"B\"])\n",
        "for i in len(df):\n",
        "    this_column = df.columns[i]\n",
        "    df[this_column] = [i, i+1]\n",
        "\n",
        "#sample code to save data to a file\n",
        "df.to_csv(file_name, encoding='utf-8', index=False)                                \n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "imputer = KNNImputer(n_neighbors= 5)\n",
        "impute_with_3 = imputer.fit_transform(OIC)\n",
        "'''\n",
        "\n",
        "'''collst=df_OIC_l.columns.to_list()\n",
        "lastcol=len(collst)-1\n",
        "mydfcnt=0\n",
        "for currdf in dfs_vals:\n",
        "  search_values = ['_ot','_it']\n",
        "  if dfs_keys[mydfcnt].str.contains('|'.join(search_values))]:\n",
        "    break\n",
        "  mycnt=0\n",
        "  collst=currdf.columns.to_list() # since cols are incorrectly different build new list\n",
        "  lastcol=len(collst)-1\n",
        "  for currcol in collst:\n",
        "    if is_numeric_dtype(currdf[currcol]):\n",
        "      currdf[currcol].fillna(currdf[currcol].median()).reset_index(drop=True)\n",
        "    if mycnt ==lastcol:\n",
        "      name = dfs_keys[mydfcnt]\n",
        "      print('name',name)\n",
        "      currdf.to_csv('%s.csv' % name, encoding='utf-8', index=False)  \n",
        "    mycnt+=1\n",
        "  mydfcnt+=1\n",
        "\n",
        "df_OIC_i = pd.concat([df_OIC_l, df_OIC_lm, df_OIC_um, df_OIC_h], join='inner')\n",
        "df_not_i = pd.concat([df_not_l, df_not_lm, df_not_um, df_not_h], join='inner')\n",
        "\n",
        "pdList = [df_not_i, df_OIC_i]\n",
        "df_all_i= pd.concat(pdList, join='inner')\n",
        "\n",
        "'''\n",
        "\n",
        "'''print('The single Null value imputation method – unconditional mean imputation has been used to \\n', \n",
        "      'determine the missing values in terms of the OIC classification scheme.\\n',\n",
        "      'Natural log data transformation or cube-root transformations used to reduce skewness within data.')\n",
        "'''\n",
        "'''\n",
        "sample code:\n",
        "# to slice a dataframe based on values in a string\n",
        "search_values = ['m','ia']\n",
        "df0=df0[df0.index.str.contains('|'.join(search_values ))]\n",
        "'''\n",
        "\n",
        "\n",
        "'''for dfname, dikt in zip(dfs.keys(), dicts):\n",
        "    dfs[dfname] = dfs[dfname].from_dict(dikt, orient='columns', dtype=None)\n",
        "    dfs['df_OIC_l']=df_all_it.pivot_table(index='country', columns='desc', values='value')\n",
        "'''\n",
        "\n",
        "'''////outdated\n",
        "#overall median\n",
        "collst=df_OIC_h.columns.to_list()\n",
        "for currcol in collst:\n",
        "  if (currcol not in ['OIC','OECD'])&(is_numeric_dtype(currdf[currcol])):\n",
        "    tmp_all[currcol].fillna(tmp_all[currcol].median())\n",
        "'''\n",
        "\n",
        "'''\n",
        "# sample code to slice a dataframe based on values in a string\n",
        "search_values = ['m','ia']\n",
        "df0=df0[df0.index.str.contains('|'.join(search_values ))]\n",
        "'''\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "old_main_FinAcc.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}